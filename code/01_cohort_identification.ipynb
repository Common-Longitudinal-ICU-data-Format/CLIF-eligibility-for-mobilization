{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eligibility for mobilization: Cohort ID and Discretizing script\n",
    "\n",
    "Author: Kaveri Chhikara\n",
    "v1: October 30, 2024\n",
    "v2: February 10, 2025\n",
    "v3: March 12, 2025\n",
    "v4: March 20, 2025\n",
    "\n",
    "This script identifies the cohort using CLIF 2.0 tables and discretizes the dataset at an hourly level\n",
    "\n",
    " \n",
    "                        🚨Code will break if the following requirements are not satisfied🚨  \n",
    "#### Requirements:\n",
    "* Required table filenames should be `clif_patient`, `clif_hospitalization`, `clif_adt`, `clif_vitals`, `clif_labs`, `clif_medication_admin_continuous`, `clif_respiratory_support`\n",
    "* Within each table, the following variables and categories are required.\n",
    "\n",
    "| Table Name | Required Variables | Required Categories |\n",
    "| --- | --- | --- |\n",
    "| **patient** | `patient_id`, `race_category`, `ethnicity_category`, `sex_category`, `death_dttm` | - |\n",
    "| **hospitalization** | `patient_id`, `hospitalization_id`, `admission_dttm`, `discharge_dttm`, `age_at_admission` | - |\n",
    "| **adt** |  `hospitalization_id`, `hospital_id`,`in_dttm`, `out_dttm`, `location_category` | - |\n",
    "| **vitals** | `hospitalization_id`, `recorded_dttm`, `vital_category`, `vital_value` | heart_rate, resp_rate, sbp, dbp, map, spo2, weight_kg, height_cm |\n",
    "| **labs** | `hospitalization_id`, `lab_result_dttm`, `lab_category`, `lab_value` | lactate |\n",
    "| **medication_admin_continuous** | `hospitalization_id`, `admin_dttm`, `med_name`, `med_category`, `med_dose`, `med_dose_unit` | norepinephrine, epinephrine, phenylephrine, vasopressin, dopamine, angiotensin(optional), nicardipine, nitroprusside, clevidipine, cisatracurium, vecuronium, rocuronium |\n",
    "| **respiratory_support** | `hospitalization_id`, `recorded_dttm`, `device_category`, `mode_category`, `tracheostomy`, `fio2_set`, `lpm_set`, `resp_rate_set`, `peep_set`, `resp_rate_obs`, `tidal_volume_set`, `pressure_control_set`, `pressure_support_set`, `peak_inspiratory_pressure_set`, `tidal_volume_obs` | - |\n",
    "| **crrt_therapy** | `hospitalization_id`, `recorded_dttm`, `crrt_mode_name`, `crrt_mode_category`, `dialysis_machine_name`, `blood_flow_rate`, `dialysate_flow_rate`, `ultrafiltration_out` | - |\n",
    "\n",
    "\n",
    "Updates 2/10:\n",
    "* Get discharge_dttm and death_dttm. Everyone in the cohort must have one of these. If not dead, assume discharged alive.\n",
    "* Include all paralytics in the mCIDE. Instead of excluding anyone who ever received a paralytics, create flags for paralytics. While creating flags for eligibility, exclude hours when the patient was on a paralytic.\n",
    "* Update exclusion criteria - exclude all patients intubated for < 4 hrs instead of 2 hrs to be consistent with the cool-off period.\n",
    "* Add code to stitch encounters when there are multiple hospitals at a site\n",
    "* Extend the analysis to competing risk. Events - 1 - eligible, 2- died, 3 discharged-alive\n",
    "\n",
    "Updates 3/12 \n",
    "* Updated code to handle dttm conversion from UTC to local time from the config file. No updates required in analysis script after the analysis df are imported.\n",
    "\n",
    "Updates 3/20\n",
    "* Updated eligibility flags to only become eligible during business hours. This gives us real time to eligibility from intubation. Use the entire hospitalization as the input for survival analysis.\n",
    "* Updated the `.qmd` script for competing risk analysis. Compare with UMN's results. \n",
    "\n",
    "\n",
    "Updates 4/15:\n",
    "* Time out lactate carry forward value after 24 hours- found a BUG 🐛- lactate values were filtered down to the first three days. Corrected. Eligible if lactate is less than  eq to 4 or NA for TEAM. \n",
    "* If no NE delivered, then team_ne flag is 1. There should be no NA values for the pressors. \n",
    "* Added SOFA calculation based on the 1997 definition. \n",
    "\n",
    "Updates 4/17:\n",
    "* Update criteria flags to be 1 (eligible) for all criteria to account for NAs. Cases where NAs are coded as 0:\n",
    "    * Red criteria - NE > 0.3 \n",
    "    * Yellow criteria - Blood pressure greater than lower limit of target range (MAP 65+) while receiving moderate level of support (medium-define as 0.1–0.3 μg/kg/min of Norepi equivalents). Only considered non-missing values of MAP and NE\n",
    "    * Generally if there is a lower threshold specified for any componenet, then missing values are not coded as eligible. If no lower threshold specified, then NAs coded as 1. \n",
    "\n",
    "Updates 4/23:\n",
    "* Updated the code to calculate SOFA score for the identified according to the logic given in sofa_score.py\n",
    "* Updated the waterfall logic to align more closely with Nick's code in R. \n",
    "\n",
    "\n",
    "Updates 4/28:\n",
    "* The thresholds for vasopressors were dropping 0, which implied that the meds were stopped. Updated the outlier threshold config file to have 0 as the lower threshold.\n",
    "* Updated red meds flags and paralytic med flags to be 1 only when the meds are administered and med dose is > 0.0\n",
    "* Updated code to consider the last recorded value for the hour for all pressors, not the max.\n",
    "* Added crrt flag to SOFA calculation, if the encounter received CRRT within start_dttm - 72 hours, end dttm, the crrt flag is 1, and SOFA renal score is 4.\n",
    "\n",
    "Updates 5/1:\n",
    "* Lactate value carry forward time out at 24 hours.  \n",
    "* Updated the final outcome to dead when discharge category is 'Hospice' AND 'Expired'\n",
    "* TEAM criteria was using resp rate obs from the respiratory support table, while the Patel criteria uses the vitals table. Updated the code to use the vitals table for resp rate obs.\n",
    "* Updated the code to use the last recorded value for the hour for all pressors, not the max.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install pandas numpy duckdb seaborn matplotlib plotly\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "import pyCLIF\n",
    "import sofa_score\n",
    "import waterfall\n",
    "\n",
    "## import outlier json\n",
    "with open('../config/outlier_config.json', 'r') as f:\n",
    "    outlier_cfg = json.load(f)\n",
    "\n",
    "graphs_folder = '../output/final/graphs'\n",
    "if not os.path.exists(graphs_folder):\n",
    "    os.makedirs(graphs_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import pyCLIF\n",
    "importlib.reload(pyCLIF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required columns and categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rst_required_columns = [\n",
    "    'hospitalization_id',\n",
    "    'recorded_dttm',\n",
    "    'device_name',\n",
    "    'device_category',\n",
    "    'mode_name', \n",
    "    'mode_category',\n",
    "    'tracheostomy',\n",
    "    'fio2_set',\n",
    "    'lpm_set',\n",
    "    'resp_rate_set',\n",
    "    'peep_set',\n",
    "    'resp_rate_obs',\n",
    "    'tidal_volume_set', \n",
    "    'pressure_control_set',\n",
    "    'pressure_support_set',\n",
    "    'peak_inspiratory_pressure_set'\n",
    "\n",
    "]\n",
    "\n",
    "vitals_required_columns = [\n",
    "    'hospitalization_id',\n",
    "    'recorded_dttm',\n",
    "    'vital_category',\n",
    "    'vital_value'\n",
    "]\n",
    "vitals_of_interest = ['heart_rate', 'respiratory_rate', 'sbp', 'dbp', 'map', 'spo2', 'weight_kg', 'height_cm']\n",
    "\n",
    "labs_required_columns = [\n",
    "    'hospitalization_id',\n",
    "    'lab_result_dttm',\n",
    "    'lab_category',\n",
    "    'lab_value',\n",
    "    'lab_value_numeric'\n",
    "]\n",
    "labs_of_interest = ['lactate']\n",
    "\n",
    "meds_required_columns = [\n",
    "    'hospitalization_id',\n",
    "    'admin_dttm',\n",
    "    'med_name',\n",
    "    'med_category',\n",
    "    'med_dose',\n",
    "    'med_dose_unit'\n",
    "]\n",
    "meds_of_interest = [\n",
    "    'norepinephrine', 'epinephrine', 'phenylephrine', 'vasopressin',\n",
    "    'dopamine', 'angiotensin', 'nicardipine', 'nitroprusside',\n",
    "    'clevidipine', 'cisatracurium', 'vecuronium', 'rocuronium '\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient = pyCLIF.load_data('clif_patient')\n",
    "hospitalization = pyCLIF.load_data('clif_hospitalization')\n",
    "adt = pyCLIF.load_data('clif_adt')\n",
    "\n",
    "# ensure id variable is of dtype character\n",
    "hospitalization['hospitalization_id']= hospitalization['hospitalization_id'].astype(str)\n",
    "patient['patient_id']= patient['patient_id'].astype(str)\n",
    "adt['hospitalization_id']= adt['hospitalization_id'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duplicate check\n",
    "\n",
    "If duplicates exist, only the first row is preserved after arranging the data by time. Please check your CLIF tables if there are duplicates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicates\n",
    "# patient table should be unique by patient id\n",
    "patient = pyCLIF.remove_duplicates(patient, ['patient_id'], 'patient')\n",
    "# hospitalization table should be unique by hospitalization id\n",
    "hospitalization = pyCLIF.remove_duplicates(hospitalization, ['hospitalization_id'], 'hospitalization')\n",
    "# adt table should be unique by hospitalization id and in dttm\n",
    "adt = pyCLIF.remove_duplicates(adt, ['hospitalization_id', 'hospital_id', 'in_dttm'], 'adt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total Number of unique encounters in the hospitalization table: {pyCLIF.count_unique_encounters(hospitalization, 'hospitalization_id')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adt.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize all _dttm variables to the same format\n",
    "\n",
    "patient = pyCLIF.convert_datetime_columns_to_site_tz(patient,  pyCLIF.helper['timezone'])\n",
    "hospitalization = pyCLIF.convert_datetime_columns_to_site_tz(hospitalization, pyCLIF.helper['timezone'])\n",
    "adt = pyCLIF.convert_datetime_columns_to_site_tz(adt,  pyCLIF.helper['timezone'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adt.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cohort Identification\n",
    "\n",
    "**Inclusion Criteria:**\n",
    "\n",
    "* Adult admissions between 2020-03-01 and 2022-03-31\n",
    "* Encounters receiving invasive mechanical ventilation during this period\n",
    "\n",
    "**Exclusion criteria:**\n",
    "\n",
    "1. Encounters that were on vent for less than 4 hours in the first 72 hours of first intubation\n",
    "2. Encounters that were on trach in the first 72 hours of first intubation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up a dictionary to keep track of STROBE counts\n",
    "strobe_counts = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (A) Date and Age Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP A: Basic Data Cleaning + Date/Age Filter\n",
    "#   - Filter hospitalization for date range & adult patients\n",
    "#   - Then reduce ADT to those hospitalization_ids\n",
    "\n",
    "\n",
    "print(\"\\n=== STEP A: Filter by date range & age ===\\n\")\n",
    "date_mask = (hospitalization['admission_dttm'] >= '2020-03-01') & \\\n",
    "            (hospitalization['admission_dttm'] <= '2022-03-31')\n",
    "age_mask = (hospitalization['age_at_admission'] >= 18)\n",
    "\n",
    "hospitalization_cohort = hospitalization[date_mask & age_mask].copy()\n",
    "\n",
    "strobe_counts['A_after_date_age_filter'] = hospitalization_cohort['hospitalization_id'].nunique()\n",
    "print(f\"Number of unique hospitalizations after date & age filter: {strobe_counts['A_after_date_age_filter']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get total unique hospitalizations without time filter, only age filter\n",
    "age_mask = (hospitalization['age_at_admission'] >= 18)\n",
    "total_adult_hospitalizations = hospitalization[age_mask]['hospitalization_id'].nunique()\n",
    "print(f\"\\nTotal number of unique adult hospitalizations (no date filter): {total_adult_hospitalizations}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter ADT for ICU locations\n",
    "icu_locations = ['icu', 'ICU']\n",
    "icu_mask = adt['location_category'].isin(icu_locations)\n",
    "icu_adt = adt[icu_mask].copy()\n",
    "\n",
    "# Get unique hospitalization IDs that had ICU stays\n",
    "icu_hosp_ids = icu_adt['hospitalization_id'].unique()\n",
    "\n",
    "# Filter hospitalizations for adult patients who were in ICU\n",
    "age_mask = (hospitalization['age_at_admission'] >= 18)\n",
    "icu_hospitalization = hospitalization[\n",
    "    (hospitalization['hospitalization_id'].isin(icu_hosp_ids)) & \n",
    "    age_mask\n",
    "].copy()\n",
    "\n",
    "# strobe_counts['A_after_date_age_filter'] = icu_hospitalization['hospitalization_id'].nunique()\n",
    "print(f\"Number of unique adult hospitalizations with ICU stays: {icu_hospitalization['hospitalization_id'].nunique()}\")\n",
    "\n",
    "# Update hospitalization_cohort to be ICU cohort\n",
    "# hospitalization_cohort = icu_hospitalization.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (B) Stitch hospitalizations\n",
    "\n",
    "Combine multiple `hospitalization_ids` into a single `encounter_block` for patients who transfer between hospital campuses or return soon after discharge. Hospitalizations that have a gap of **6 hours or less** between the discharge dttm and admission dttm are put in one encounter block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter ADT to only those in the cohort set\n",
    "cohort_ids = hospitalization_cohort['hospitalization_id'].unique().tolist()\n",
    "adt_cohort = adt[adt['hospitalization_id'].isin(cohort_ids)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in admission and discharge dates\n",
    "print(\"\\nMissing values in admission_dttm:\", hospitalization_cohort['admission_dttm'].isna().sum())\n",
    "print(\"Missing values in discharge_dttm:\", hospitalization_cohort['discharge_dttm'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP B: Stitch Encounters => 'encounter_block'\n",
    "# Use stitch_encounters from pyCLIF with time_interval=6\n",
    "\n",
    "print(\"\\n=== STEP B: Stitch encounters ===\\n\")\n",
    "stitched_cohort = pyCLIF.stitch_encounters(hospitalization_cohort, adt_cohort, time_interval=6)\n",
    "# stitched_cohort now has: 'patient_id','hospitalization_id','encounter_block' and other ADT variables. This will have duplicate rows because of location category\n",
    "# We only want 1 row per unique encounter_block for the next steps.\n",
    "stitched_unique = stitched_cohort[['patient_id', 'encounter_block']].drop_duplicates()\n",
    "\n",
    "strobe_counts['B_before_stitching'] = stitched_cohort['hospitalization_id'].nunique()\n",
    "strobe_counts['B_after_stitching'] = stitched_unique['encounter_block'].nunique()\n",
    "strobe_counts['B_stitched_hosp_ids'] = strobe_counts['B_before_stitching']-strobe_counts['B_after_stitching']\n",
    "print(f\"Number of unique hospitalizations before stitching: {stitched_cohort['hospitalization_id'].nunique()}\")\n",
    "print(f\"Number of unique encounter blocks after stitching: {strobe_counts['B_after_stitching']}\")\n",
    "print(f\"Number of linked hospitalization ids: {strobe_counts['B_before_stitching']-strobe_counts['B_after_stitching']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping of patient id, hospitalization id and encounter blocks\n",
    "all_ids = stitched_cohort[['patient_id', 'hospitalization_id', 'encounter_block']].drop_duplicates()\n",
    "print(\"\\nUnique values in each column:\")\n",
    "for col in all_ids.columns:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(all_ids[col].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (C) Identify ventilator usage\n",
    "\n",
    "Filter down to encounters that received invasive mechanical ventilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP C: Identify Ventilator Usage\n",
    "# Load respiratory support only for the relevant “hospitalization_id” set\n",
    "# These hospitalizations map to an encounter_block for final grouping.\n",
    "\n",
    "print(\"\\n=== STEP C: Load & process respiratory support => Identify IMV usage ===\\n\")\n",
    "\n",
    "# 1) Load respiratory support\n",
    "resp_support_raw = pyCLIF.load_data(\n",
    "    'clif_respiratory_support',\n",
    "    columns=rst_required_columns,\n",
    "    filters={'hospitalization_id': all_ids['hospitalization_id'].unique().tolist()}\n",
    ")\n",
    "\n",
    "resp_support = resp_support_raw.copy()\n",
    "resp_support = pyCLIF.convert_datetime_columns_to_site_tz(resp_support, pyCLIF.helper['timezone'])\n",
    "resp_support['device_category'] = resp_support['device_category'].str.lower()\n",
    "resp_support['mode_category'] = resp_support['mode_category'].str.lower()\n",
    "resp_support['fio2_set'] = pd.to_numeric(resp_support['fio2_set'], errors='coerce')\n",
    "resp_support['lpm_set'] = pd.to_numeric(resp_support['lpm_set'], errors='coerce')\n",
    "resp_support['resp_rate_set'] = pd.to_numeric(resp_support['resp_rate_set'], errors='coerce')\n",
    "resp_support['peep_set'] = pd.to_numeric(resp_support['peep_set'], errors='coerce')\n",
    "resp_support['resp_rate_obs'] = pd.to_numeric(resp_support['resp_rate_obs'], errors='coerce')\n",
    "\n",
    "# del resp_support_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Respiratory Support Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list = []\n",
    "group_cols = 'device_category'  # or a list like ['device_category','mode_category']\n",
    "numeric_cols = ['fio2_set','peep_set','lpm_set', 'resp_rate_set', 'resp_rate_obs']\n",
    "\n",
    "for col in numeric_cols:\n",
    "    tmp = pyCLIF.create_summary_table(\n",
    "        df=resp_support,\n",
    "        numeric_col=col,\n",
    "        group_by_cols=group_cols\n",
    "    )\n",
    "    # tmp might have columns:\n",
    "    #   ['device_category','N','missing','min','q25','median','q75','mean','max']\n",
    "    # Insert a \"variable\" column next to the group-by columns:\n",
    "    tmp['variable'] = col\n",
    "    # We want \"device_category\" (the group col), then \"variable\", then the rest\n",
    "    if isinstance(group_cols, str):\n",
    "        group_cols_list = [group_cols]  # unify into list\n",
    "    else:\n",
    "        group_cols_list = group_cols  # already a list\n",
    "    # Reorder so that group-by columns come first, then 'variable', then the rest\n",
    "    front_cols = group_cols_list + ['variable']\n",
    "    # Build the list of remaining columns\n",
    "    rest_cols = [c for c in tmp.columns if c not in front_cols]\n",
    "    new_cols = front_cols + rest_cols\n",
    "    tmp = tmp[new_cols]\n",
    "    results_list.append(tmp)\n",
    "\n",
    "# Finally, concatenate all results\n",
    "final_summary_resp_support = pd.concat(results_list, ignore_index=True)\n",
    "final_summary_resp_support.to_csv('../output/final/summary_respiratory_support_by_device.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list = []\n",
    "group_cols = ['device_category','mode_category']\n",
    "numeric_cols = ['fio2_set','peep_set','lpm_set', 'resp_rate_set', 'resp_rate_obs']\n",
    "\n",
    "for col in numeric_cols:\n",
    "    tmp = pyCLIF.create_summary_table(\n",
    "        df=resp_support,\n",
    "        numeric_col=col,\n",
    "        group_by_cols=group_cols\n",
    "    )\n",
    "    # tmp might have columns:\n",
    "    #   ['device_category','N','missing','min','q25','median','q75','mean','max']\n",
    "    # Insert a \"variable\" column next to the group-by columns:\n",
    "    tmp['variable'] = col\n",
    "    # We want \"device_category\" (the group col), then \"variable\", then the rest\n",
    "    if isinstance(group_cols, str):\n",
    "        group_cols_list = [group_cols]  # unify into list\n",
    "    else:\n",
    "        group_cols_list = group_cols  # already a list\n",
    "    # Reorder so that group-by columns come first, then 'variable', then the rest\n",
    "    front_cols = group_cols_list + ['variable']\n",
    "    # Build the list of remaining columns\n",
    "    rest_cols = [c for c in tmp.columns if c not in front_cols]\n",
    "    new_cols = front_cols + rest_cols\n",
    "    tmp = tmp[new_cols]\n",
    "    results_list.append(tmp)\n",
    "\n",
    "# Finally, concatenate all results\n",
    "final_summary_resp_support = pd.concat(results_list, ignore_index=True)\n",
    "final_summary_resp_support.to_csv('../output/final/summary_respiratory_support_by_device_mode.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Respiratory Support Waterfall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import importlib\n",
    "# importlib.reload(waterfall)\n",
    "# import waterfall\n",
    "# processed_resp_support = waterfall.process_resp_support_waterfall( resp_support_raw, \n",
    "#                                                         id_col = \"hospitalization_id\",\n",
    "#                                                         verbose = True)\n",
    "\n",
    "# processed_resp_support = pyCLIF.convert_datetime_columns_to_site_tz(processed_resp_support, pyCLIF.helper['timezone'])\n",
    "# processed_resp_support.to_parquet('../output/intermediate/processed_resp_support.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_resp_support.to_parquet('../output/intermediate/processed_resp_support.parquet', index=False)\n",
    "processed_resp_support = pd.read_parquet('../output/intermediate/processed_resp_support.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Merge to get encounter_block for the cohort identified so far\n",
    "resp_stitched = processed_resp_support.merge(\n",
    "    all_ids[['hospitalization_id','encounter_block']],\n",
    "    on='hospitalization_id', how='right'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Apply outlier thresholds ===\\n\")\n",
    "\n",
    "# (Optional) If FiO2 is >1 on average => scale by /100\n",
    "fio2_mean = resp_stitched['fio2_set'].mean(skipna=True)\n",
    "# If the mean is greater than 1, divide 'fio2_set' by 100\n",
    "if fio2_mean and fio2_mean > 1.0:\n",
    "    # Only divide values greater than 1 to avoid re-dividing already correct values\n",
    "    resp_stitched.loc[resp_stitched['fio2_set'] > 1, 'fio2_set'] = \\\n",
    "        resp_stitched.loc[resp_stitched['fio2_set'] > 1, 'fio2_set'] / 100\n",
    "    print(\"Updated fio2_set to be between 0.21 and 1\")\n",
    "else:\n",
    "    print(\"FIO2_SET mean=\", fio2_mean, \"is within the required range\")\n",
    "\n",
    "pyCLIF.apply_outlier_thresholds(resp_stitched, 'fio2_set', *outlier_cfg['fio2_set'])\n",
    "pyCLIF.apply_outlier_thresholds(resp_stitched, 'peep_set', *outlier_cfg['peep_set'])\n",
    "pyCLIF.apply_outlier_thresholds(resp_stitched, 'lpm_set',  *outlier_cfg['lpm_set'])\n",
    "pyCLIF.apply_outlier_thresholds(resp_stitched, 'resp_rate_set', *outlier_cfg['resp_rate_set'])\n",
    "pyCLIF.apply_outlier_thresholds(resp_stitched, 'resp_rate_obs', *outlier_cfg['resp_rate_obs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Identify IMV\n",
    "imv_mask = resp_stitched['device_category'].str.contains(\"imv\", case=False, na=False)\n",
    "resp_stitched_imv = resp_stitched[imv_mask].copy()\n",
    "# this creates a on vent field for everytime the patient is on a vent\n",
    "# Create on_vent column for IMV records\n",
    "resp_stitched_imv['on_vent'] = 1\n",
    "\n",
    "# Left join back to full resp_stitched to include non-vent records\n",
    "resp_stitched = resp_stitched.merge(\n",
    "    resp_stitched_imv[['hospitalization_id', 'recorded_dttm', 'on_vent']], \n",
    "    on=['hospitalization_id', 'recorded_dttm'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Fill NaN values with 0 for times when not on vent\n",
    "resp_stitched['on_vent'] = resp_stitched['on_vent'].fillna(0)\n",
    "\n",
    "\n",
    "strobe_counts['C_imv_hospitalizations'] = resp_stitched_imv['hospitalization_id'].nunique()\n",
    "strobe_counts['C_imv_encounter_blocks'] = resp_stitched_imv['encounter_block'].nunique()\n",
    "\n",
    "print(f\"Total IMV respiratory support hospitalizations: {strobe_counts['C_imv_hospitalizations']}\")\n",
    "print(f\"Total IMV respiratory support encounter blocks: {strobe_counts['C_imv_encounter_blocks']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ids =  all_ids[all_ids['encounter_block'].isin(resp_stitched_imv['encounter_block'].unique())]\n",
    "all_ids = all_ids[all_ids['hospitalization_id'].isin(resp_stitched_imv['hospitalization_id'].unique())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in all_ids.columns:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(all_ids[col].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (D) Vent start and end times \n",
    "\n",
    "Calculate vent start times for the first episode of invasive mechanical intubation.   \n",
    "Limitation: the vent end time might not be associated with the same intubation episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP E: Determine Vent Start/End for Each Hospitalization and Encounter block\n",
    "\n",
    "print(\"\\n=== STEP D: Determine ventilation times (start/end) at d encounter block level ===\\n\")\n",
    "\n",
    "# at the hospitalization id level\n",
    "vent_start_end = resp_stitched_imv.groupby('hospitalization_id').agg(\n",
    "    vent_start_time=('recorded_dttm','min'),\n",
    "    vent_end_time=('recorded_dttm','max')\n",
    ").reset_index()\n",
    "\n",
    "# Exclude edge case: if start_time == end_time \n",
    "# these would otherwise have been excluded when we remove encounters on vent for less than 4 hours\n",
    "check_same_vent_start_end = vent_start_end[vent_start_end['vent_start_time'] == vent_start_end['vent_end_time']].copy()\n",
    "vent_start_end= vent_start_end[vent_start_end['vent_start_time'] != vent_start_end['vent_end_time']].copy()\n",
    "\n",
    "strobe_counts['D_hospitalizations_with_valid_vent'] = vent_start_end['hospitalization_id'].nunique()\n",
    "strobe_counts['D_hospitalizations_with_same_vent_start_end'] = check_same_vent_start_end['hospitalization_id'].nunique()\n",
    "print(f\"Unique hospitalizations with valid IMV start/end: {strobe_counts['D_hospitalizations_with_valid_vent']}\")\n",
    "\n",
    "# at the block level\n",
    "block_vent_times = resp_stitched_imv.groupby('encounter_block', dropna=True).agg(\n",
    "    block_vent_start_dttm=('recorded_dttm','min'),\n",
    "    block_vent_end_dttm=('recorded_dttm','max')\n",
    ").reset_index()\n",
    "\n",
    "# If start==end, no real vent- there was just ONE vent entry, this exclusion can count under \n",
    "block_same_vent = block_vent_times[block_vent_times['block_vent_start_dttm']==block_vent_times['block_vent_end_dttm']].copy()\n",
    "block_vent_times = block_vent_times[block_vent_times['block_vent_start_dttm']!=block_vent_times['block_vent_end_dttm']].copy()\n",
    "\n",
    "strobe_counts['D_blocks_with_valid_vent'] = block_vent_times['encounter_block'].nunique()\n",
    "strobe_counts['D_blocks_with_same_vent_start_end'] = block_same_vent['encounter_block'].nunique()\n",
    "print(f\"Unique encounter blocks with valid IMV start/end: {strobe_counts['D_blocks_with_valid_vent']}\")\n",
    "\n",
    "valid_blocks_vent = block_vent_times['encounter_block'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strobe_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter all_ids to only keep rows where encounter_block is in valid_blocks_vent\n",
    "all_ids = all_ids[all_ids['encounter_block'].isin(valid_blocks_vent)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in all_ids.columns:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(all_ids[col].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (E) Hourly Sequence \n",
    "\n",
    "This section achieves the following steps:  \n",
    "* Identifies the first and last recorded times for vitals for each encounter block\n",
    "* These times are used to generate an hourly sequence of patients hospitalization journey\n",
    "* Combines with hourly vent usage data from the respiratory support table\n",
    "* Excludes encounters on vent for less than 4 hours in the first 72 hours\n",
    "* Creates a final dataframe with the identified cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP E: Generate Hourly Sequence & Exclude encounter blocks with <4 Vent Hours\n",
    "#  Create an hourly timeline from vent_start to last vital or outcome time for each encounter block\n",
    "# We stop operating at hospitalization id level \n",
    "\n",
    "print(\"\\n=== STEP E: Hourly sequence generation & < 4 hour vent exclusion BLOCK level===\\n\")\n",
    "\n",
    "# 1) define the 'end_time' for the sequence from vitals or outcome.\n",
    "vitals_cohort = pyCLIF.load_data('clif_vitals',\n",
    "    columns=vitals_required_columns,\n",
    "    filters={'hospitalization_id': all_ids['hospitalization_id'].unique().tolist(), \n",
    "             'vital_category': vitals_of_interest}\n",
    ")\n",
    "vitals_cohort = pyCLIF.convert_datetime_columns_to_site_tz(vitals_cohort, pyCLIF.helper['timezone'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace outliers with NAs in the vitals table \n",
    "# Extract min/max values from config for each vital\n",
    "min_hr, max_hr = outlier_cfg['heart_rate']\n",
    "min_rr, max_rr = outlier_cfg['respiratory_rate'] \n",
    "min_sbp, max_sbp = outlier_cfg['sbp']\n",
    "min_dbp, max_dbp = outlier_cfg['dbp']\n",
    "min_map, max_map = outlier_cfg['map']\n",
    "min_spo2, max_spo2 = outlier_cfg['spo2']\n",
    "min_weight, max_weight = outlier_cfg['weight_kg']\n",
    "min_height, max_height = outlier_cfg['height_cm']\n",
    "\n",
    "# For each vital category, set out-of-range values to NaN\n",
    "is_hr = vitals_cohort['vital_category'] == 'heart_rate'\n",
    "vitals_cohort.loc[is_hr & (vitals_cohort['vital_value'] < min_hr), 'vital_value'] = np.nan\n",
    "vitals_cohort.loc[is_hr & (vitals_cohort['vital_value'] > max_hr), 'vital_value'] = np.nan\n",
    "\n",
    "is_rr = vitals_cohort['vital_category'] == 'respiratory_rate'\n",
    "vitals_cohort.loc[is_rr & (vitals_cohort['vital_value'] < min_rr), 'vital_value'] = np.nan\n",
    "vitals_cohort.loc[is_rr & (vitals_cohort['vital_value'] > max_rr), 'vital_value'] = np.nan\n",
    "\n",
    "is_sbp = vitals_cohort['vital_category'] == 'sbp'\n",
    "vitals_cohort.loc[is_sbp & (vitals_cohort['vital_value'] < min_sbp), 'vital_value'] = np.nan\n",
    "vitals_cohort.loc[is_sbp & (vitals_cohort['vital_value'] > max_sbp), 'vital_value'] = np.nan\n",
    "\n",
    "is_dbp = vitals_cohort['vital_category'] == 'dbp'\n",
    "vitals_cohort.loc[is_dbp & (vitals_cohort['vital_value'] < min_dbp), 'vital_value'] = np.nan\n",
    "vitals_cohort.loc[is_dbp & (vitals_cohort['vital_value'] > max_dbp), 'vital_value'] = np.nan\n",
    "\n",
    "is_map = vitals_cohort['vital_category'] == 'map'\n",
    "vitals_cohort.loc[is_map & (vitals_cohort['vital_value'] < min_map), 'vital_value'] = np.nan\n",
    "vitals_cohort.loc[is_map & (vitals_cohort['vital_value'] > max_map), 'vital_value'] = np.nan\n",
    "\n",
    "is_spo2 = vitals_cohort['vital_category'] == 'spo2'\n",
    "vitals_cohort.loc[is_spo2 & (vitals_cohort['vital_value'] < min_spo2), 'vital_value'] = np.nan\n",
    "vitals_cohort.loc[is_spo2 & (vitals_cohort['vital_value'] > max_spo2), 'vital_value'] = np.nan\n",
    "\n",
    "is_weight = vitals_cohort['vital_category'] == 'weight_kg'\n",
    "vitals_cohort.loc[is_weight & (vitals_cohort['vital_value'] < min_weight), 'vital_value'] = np.nan\n",
    "vitals_cohort.loc[is_weight & (vitals_cohort['vital_value'] > max_weight), 'vital_value'] = np.nan\n",
    "\n",
    "is_height = vitals_cohort['vital_category'] == 'height_cm'\n",
    "vitals_cohort.loc[is_height & (vitals_cohort['vital_value'] < min_height), 'vital_value'] = np.nan\n",
    "vitals_cohort.loc[is_height & (vitals_cohort['vital_value'] > max_height), 'vital_value'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_vitals = pyCLIF.create_summary_table(\n",
    "        df=vitals_cohort,\n",
    "        numeric_col='vital_value',\n",
    "        group_by_cols='vital_category'\n",
    "    )\n",
    "summary_vitals.to_csv('../output/final/summary_vitals_by_category.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge to get encounter_block on each vital\n",
    "vitals_stitched = vitals_cohort.merge(all_ids, on='hospitalization_id', how='left')\n",
    "# Group by block => find earliest & latest vital for that block\n",
    "vital_bounds_block = vitals_stitched.groupby('encounter_block', dropna=True)['recorded_dttm'].agg(['min','max']).reset_index()\n",
    "vital_bounds_block.columns = ['encounter_block','block_first_vital_dttm','block_last_vital_dttm']\n",
    "\n",
    "# 2) Merge block_vent_times with vital_bounds_block\n",
    "final_blocks = block_vent_times.merge(vital_bounds_block, on='encounter_block', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) If block_last_vital_dttm < vent_start_time => weird edge case. Ideally shouldn't happen. \n",
    "# If such bad blocks exist, check your CLIF tables bro\n",
    "bad_block = final_blocks[final_blocks['block_last_vital_dttm'] < final_blocks['block_vent_start_dttm']]\n",
    "if len(bad_block) > 0:\n",
    "    print(\"Warning: Some blocks have last vital < vent start:\\n\", bad_block)\n",
    "else:\n",
    "    print(\"There are no bad blocks! Good job CLIF-ing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Generate the hourly sequence at block level\n",
    "def generate_hourly_sequence_block(row):\n",
    "    blk  = row['encounter_block'].iloc[0]\n",
    "    start_time = row['block_vent_start_dttm'].iloc[0]\n",
    "    end_time   = row['block_last_vital_dttm'].iloc[0]\n",
    "    hourly_timestamps = pd.date_range(start=start_time, end=end_time, freq='h')\n",
    "    return pd.DataFrame({\n",
    "        'encounter_block': blk,\n",
    "        'recorded_dttm': hourly_timestamps\n",
    "    })\n",
    "\n",
    "hourly_seq_block = final_blocks.groupby('encounter_block', as_index=False).apply(generate_hourly_sequence_block)\n",
    "hourly_seq_block = hourly_seq_block.reset_index(drop=True)\n",
    "\n",
    "# hourly_seq_block['recorded_dttm'] = hourly_seq_block['recorded_dttm'].dt.tz_convert('UTC')\n",
    "hourly_seq_block['recorded_date'] = hourly_seq_block['recorded_dttm'].dt.date\n",
    "hourly_seq_block['recorded_hour'] = hourly_seq_block['recorded_dttm'].dt.hour\n",
    "hourly_seq_block = hourly_seq_block.drop_duplicates(subset=['encounter_block', 'recorded_date', 'recorded_hour'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Add time_from_vent & 4-hr “cool-off”\n",
    "hourly_seq_block['time_from_vent'] = hourly_seq_block.groupby('encounter_block').cumcount()\n",
    "hourly_seq_block['time_from_vent_adjusted'] = np.where(\n",
    "    hourly_seq_block['time_from_vent'] < 4, -1, hourly_seq_block['time_from_vent'] - 4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add time_biz_from_vent (counts only business hours 8-16)\n",
    "biz_hours_mask = (hourly_seq_block['recorded_hour'] >= 8) & (hourly_seq_block['recorded_hour'] <= 16)\n",
    "hourly_seq_block['time_biz_from_vent'] = 0  # Initialize with 0\n",
    "\n",
    "# Create a temporary DataFrame with only business hours\n",
    "biz_hours_df = hourly_seq_block[biz_hours_mask].copy()\n",
    "biz_hours_df = biz_hours_df.sort_values(['encounter_block', 'recorded_dttm'])\n",
    "\n",
    "# Calculate cumulative count for business hours\n",
    "biz_hours_df['time_biz_from_vent'] = biz_hours_df.groupby('encounter_block').cumcount()\n",
    "\n",
    "# Update the original DataFrame with business hours counts\n",
    "hourly_seq_block.loc[biz_hours_mask, 'time_biz_from_vent'] = biz_hours_df['time_biz_from_vent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Combine with actual vent usage by hour\n",
    "resp_stitched_final = resp_stitched[resp_stitched['encounter_block'].isin(all_ids['encounter_block'])]\n",
    "resp_stitched_final['recorded_date'] = resp_stitched_final['recorded_dttm'].dt.date\n",
    "resp_stitched_final['recorded_hour'] = resp_stitched_final['recorded_dttm'].dt.hour\n",
    "\n",
    "\n",
    "hourly_vent_block = resp_stitched_final.groupby(['encounter_block','recorded_date','recorded_hour']).agg(\n",
    "    min_fio2_set=('fio2_set','min'),\n",
    "    max_fio2_set=('fio2_set','max'),\n",
    "    min_peep_set=('peep_set','min'),\n",
    "    max_peep_set=('peep_set','max'),\n",
    "    min_lpm_set=('lpm_set', 'min'),\n",
    "    max_lpm_set=('lpm_set', 'max'),\n",
    "    min_resp_rate_obs=('resp_rate_obs', 'min'),\n",
    "    max_resp_rate_obs=('resp_rate_obs', 'max'),\n",
    "    hourly_trach=('tracheostomy', lambda x: 1 if x.max()==1 else 0), # 1 if the any value within that hour is 1\n",
    "    hourly_on_vent=('on_vent','max'),\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check- Find encounter_blocks that are in hourly_seq_block but not in hourly_vent_block and vice versa\n",
    "# This is possible when the patient is put on IMV in the ED, and dies shortly after. \n",
    "# Still might be worth exploring the trajectory for these patients \n",
    "seq_blocks = set(hourly_seq_block['encounter_block'].unique())\n",
    "vent_blocks = set(hourly_vent_block['encounter_block'].unique())\n",
    "\n",
    "blocks_in_seq_not_vent = seq_blocks - vent_blocks\n",
    "blocks_in_vent_not_seq = vent_blocks - seq_blocks\n",
    "\n",
    "print(\"Blocks in hourly_seq_block but not in hourly_vent_block:\", len(blocks_in_seq_not_vent))\n",
    "if len(blocks_in_seq_not_vent) > 0:\n",
    "    print(sorted(list(blocks_in_seq_not_vent)))\n",
    "\n",
    "print(\"\\nBlocks in hourly_vent_block but not in hourly_seq_block:\", len(blocks_in_vent_not_seq))\n",
    "if len(blocks_in_vent_not_seq) > 0:\n",
    "    print(sorted(list(blocks_in_vent_not_seq)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_block = pd.merge(\n",
    "    hourly_seq_block,\n",
    "    hourly_vent_block,\n",
    "    on=['encounter_block','recorded_date','recorded_hour'],\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of final_df_block:\", final_df_block.shape)\n",
    "print(\"\\nUnique counts:\")\n",
    "print(f\"Encounter blocks: {final_df_block['encounter_block'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Count how many vent hours per block in the first 72 hours after first intubation,\n",
    "#  Exclude <4 hours on vent in first 72 hours at block level- They cannot meaningfully be studied for early mobilization if they’re barely intubated.. including them could bias results\n",
    "first_72_hours = final_df_block[(final_df_block['time_from_vent'] >= 0) & (final_df_block['time_from_vent'] < 72)]\n",
    "vent_hours_per_block = first_72_hours.groupby('encounter_block')['hourly_on_vent'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks_under_4 = vent_hours_per_block[vent_hours_per_block < 4].index\n",
    "blocks_under_4_df = final_df_block[final_df_block['encounter_block'].isin(blocks_under_4)]\n",
    "final_df_block = final_df_block[~final_df_block['encounter_block'].isin(blocks_under_4)]\n",
    "\n",
    "strobe_counts['G_blocks_with_vent_4_or_more'] = final_df_block['encounter_block'].nunique()\n",
    "strobe_counts['G_blocks_with_vent_less_than_4'] = len(blocks_under_4)\n",
    "print(f\"Unique encounter blocks with valid IMV start/end: {strobe_counts['G_blocks_with_vent_4_or_more']}\")\n",
    "print(f\"Excluded {len(blocks_under_4)} encounter blocks with <4 vent hours in first 72 hours of intubation.\\n\")\n",
    "\n",
    "# 8) Exclude blocks with early trach in first 72\n",
    "trach_flag_block = first_72_hours.groupby('encounter_block')['hourly_trach'].max()\n",
    "blocks_with_trach = trach_flag_block[trach_flag_block==1].index\n",
    "\n",
    "final_df_block = final_df_block[~final_df_block['encounter_block'].isin(blocks_with_trach)]\n",
    "print(f\"Excluded {len(blocks_with_trach)} encounter blocks with trach in first 72 hours of intubation.\\n\")\n",
    "\n",
    "strobe_counts['G_final_blocks_without_trach'] = final_df_block['encounter_block'].nunique()\n",
    "strobe_counts['G_final_blocks_with_trach'] = len(blocks_with_trach)\n",
    "print(f\"Final cohort size (unique blocks) after all exclusions: {strobe_counts['G_final_blocks_without_trach']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strobe_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.merge(\n",
    "    final_df_block,\n",
    "    all_ids,\n",
    "    on='encounter_block',\n",
    "    how='left'\n",
    ").reindex(columns=[\n",
    "    'patient_id', 'hospitalization_id', 'encounter_block', \n",
    "    'recorded_dttm', 'recorded_date', 'recorded_hour',\n",
    "    'time_from_vent', 'time_from_vent_adjusted', 'time_biz_from_vent',\n",
    "    'min_fio2_set', 'max_fio2_set', 'min_peep_set', 'max_peep_set',\n",
    "    'min_lpm_set', 'max_lpm_set', 'min_resp_rate_obs', 'max_resp_rate_obs',\n",
    "    'hourly_trach', 'hourly_on_vent'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "key_cols = ['encounter_block', 'recorded_date', 'recorded_hour']\n",
    "duplicates = final_df.duplicated(subset=key_cols).sum()\n",
    "print(f\"Number of duplicate rows: {duplicates}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ids = all_ids[all_ids['encounter_block'].isin(final_df['encounter_block'])]\n",
    "all_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in all_ids.columns:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(all_ids[col].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (E) Add final outcome dttm\n",
    "\n",
    "Calculate final outcome dttm for each encounter block using last vital recorded dttm and discharge disposition.   \n",
    "\n",
    "To get the `final_outcome_dttm`, we use the `block_last_vital_dttm`. Added a `is_dead` flag when `discharge_category` == `Expired` or `Hospice`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Merge `all_ids` (patient_id, hospitalization_id, encounter_block)\n",
    "#    with final blocks DataFrame (which has block-level columns -  \tblock_vent_start_dttm,\tblock_vent_end_dttm block_first_vital_dttm, block_last_vital_dttm, ).\n",
    "all_ids_w_outcome = pd.merge(\n",
    "    all_ids,\n",
    "    final_blocks,           \n",
    "    on='encounter_block',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# 2) Merge with hospitalization table to get discharge_dttm optional discharge_disposition\n",
    "all_ids_w_outcome = pd.merge(\n",
    "    all_ids_w_outcome,\n",
    "    hospitalization[['hospitalization_id', 'discharge_dttm', 'discharge_category']],\n",
    "    on='hospitalization_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# 3) Merge with patient table to get death_dttm\n",
    "all_ids_w_outcome = pd.merge(\n",
    "    all_ids_w_outcome,\n",
    "    patient[['patient_id','death_dttm']],\n",
    "    on='patient_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# New logic: Use block_last_vital_dttm as the final_outcome_dttm\n",
    "all_ids_w_outcome['final_outcome_dttm'] = all_ids_w_outcome['block_last_vital_dttm']\n",
    "\n",
    "# Add is_dead flag based on discharge_category\n",
    "all_ids_w_outcome['is_dead'] = (all_ids_w_outcome['discharge_category'].str.lower().isin(['expired', 'hospice'])).astype(int)\n",
    "\n",
    "# Handle case where death_dttm is less than discharge_dttm\n",
    "mask_death_before_discharge = all_ids_w_outcome['death_dttm'] < all_ids_w_outcome['discharge_dttm']\n",
    "all_ids_w_outcome.loc[mask_death_before_discharge, 'final_outcome_dttm'] = all_ids_w_outcome['death_dttm']\n",
    "all_ids_w_outcome.loc[mask_death_before_discharge, 'is_dead'] = 1\n",
    "\n",
    "all_ids_w_outcome.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SANITY CHECK- check blocks where death_dttm is before block_last_vital_dttm\n",
    "## For this project, we used bloack_last_vital_dttm as the final_outcome_dttm to circumvent possible issues \n",
    "mask_death_before_vitals = (all_ids_w_outcome['death_dttm'].notna()) & (all_ids_w_outcome['death_dttm'] < all_ids_w_outcome['block_last_vital_dttm'])\n",
    "print(\"Number of blocks where death_dttm is before block_last_vital_dttm:\", mask_death_before_vitals.sum())\n",
    "print(\"\\nExample cases:\")\n",
    "death_before_vitals_df = all_ids_w_outcome[mask_death_before_vitals][['patient_id', 'hospitalization_id', 'encounter_block', 'death_dttm', 'block_last_vital_dttm', 'final_outcome_dttm']]\n",
    "\n",
    "# Calculate the difference in hours between death_dttm and block_last_vital_dttm\n",
    "death_before_vitals_df['diff_hour'] = (death_before_vitals_df['death_dttm'] - death_before_vitals_df['block_last_vital_dttm']).dt.total_seconds() / 3600\n",
    "\n",
    "death_before_vitals_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in all_ids_w_outcome.columns[:3]:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(all_ids_w_outcome[col].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hourly Vitals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get height , weight to calculate bmi\n",
    "# Filter vitals to include only height and weight\n",
    "vitals_bmi = vitals_stitched[\n",
    "    (vitals_stitched['vital_category'].isin(['weight_kg', 'height_cm'])) &\n",
    "    (vitals_stitched['encounter_block'].isin(all_ids_w_outcome['encounter_block']))\n",
    "].copy()\n",
    "\n",
    "# Remove outliers\n",
    "# Extract the min/max from the config\n",
    "min_height, max_height = outlier_cfg['height_cm']\n",
    "min_weight, max_weight = outlier_cfg['weight_kg']\n",
    "\n",
    "# For height rows: set out-of-range to NaN\n",
    "is_height = vitals_bmi['vital_category'] == 'height_cm'\n",
    "height_mask_low  = is_height & (vitals_bmi['vital_value'] < min_height)\n",
    "height_mask_high = is_height & (vitals_bmi['vital_value'] > max_height)\n",
    "vitals_bmi.loc[height_mask_low | height_mask_high, 'vital_value'] = np.nan\n",
    "\n",
    "# For weight rows: set out-of-range to NaN\n",
    "is_weight = vitals_bmi['vital_category'] == 'weight_kg'\n",
    "weight_mask_low  = is_weight & (vitals_bmi['vital_value'] < min_weight)\n",
    "weight_mask_high = is_weight & (vitals_bmi['vital_value'] > max_weight)\n",
    "vitals_bmi.loc[weight_mask_low | weight_mask_high, 'vital_value'] = np.nan\n",
    "\n",
    "# Merge with vent_start_end to get ventilation start time\n",
    "vitals_bmi = vitals_bmi.merge(\n",
    "    block_vent_times[['encounter_block','block_vent_start_dttm']],\n",
    "    on='encounter_block',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Calculate time difference between recorded_dttm and vent_start_time\n",
    "vitals_bmi['time_diff'] = (vitals_bmi['recorded_dttm'] - vitals_bmi['block_vent_start_dttm']).dt.total_seconds() / 3600  # in hours\n",
    "\n",
    "# Define whether measurement is before or after vent_start_time\n",
    "vitals_bmi['before_vent_start'] = (vitals_bmi['time_diff'] <= 0).astype(int)\n",
    "\n",
    "# Calculate absolute time difference\n",
    "vitals_bmi['abs_time_diff'] = vitals_bmi['time_diff'].abs()\n",
    "\n",
    "# Sort data to prioritize measurements before vent start and closest in time\n",
    "vitals_bmi = vitals_bmi.sort_values(['encounter_block', 'vital_category', 'before_vent_start', 'abs_time_diff'], \n",
    "                                    ascending=[True, True, False, True])\n",
    "\n",
    "# Drop duplicates to keep the closest measurement for each vital_category per encounter block\n",
    "vitals_bmi = vitals_bmi.drop_duplicates(subset=['encounter_block', 'vital_category'], keep='first')\n",
    "\n",
    "# Pivot to get height and weight per encounter block\n",
    "vitals_bmi_pivot = vitals_bmi.pivot(index='encounter_block', \n",
    "                                    columns='vital_category', \n",
    "                                    values='vital_value'\n",
    "                                    ).reset_index()\n",
    "\n",
    "# Calculate BMI\n",
    "vitals_bmi_pivot['bmi'] = vitals_bmi_pivot['weight_kg'] / ((vitals_bmi_pivot['height_cm'] / 100) ** 2)\n",
    "\n",
    "print(f\"Number of unique encounter blocks with BMI data: {vitals_bmi_pivot['encounter_block'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract 'recorded_date' and 'recorded_hour' from recorded_dttm\n",
    "vitals_stitched['recorded_date'] = vitals_stitched['recorded_dttm'].dt.date\n",
    "vitals_stitched['recorded_hour'] = vitals_stitched['recorded_dttm'].dt.hour\n",
    "print(f\"Number of unique encounter blocks BEFORE: {vitals_stitched['encounter_block'].nunique()}\")\n",
    "vitals_stitched = vitals_stitched[vitals_stitched['encounter_block'].isin(all_ids_w_outcome['encounter_block'])]\n",
    "print(f\"Number of unique encounter blocks AFTER: {vitals_stitched['encounter_block'].nunique()}\")\n",
    "strobe_counts['final_blocks_with_vitals'] = vitals_stitched['encounter_block'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MAP if it doesn't exist\n",
    "# Check if 'map' exists\n",
    "if 'map' not in vitals_stitched['vital_category'].unique():\n",
    "    print(\"map is not present, so we'll calculate it...\")\n",
    "    # 1) Filter for sbp & dbp\n",
    "    sbp_dbp = vitals_stitched[vitals_stitched['vital_category'].isin(['sbp','dbp'])].copy()\n",
    "    \n",
    "    # 2) Pivot at the encounter_block + recorded_dttm level\n",
    "    sbp_dbp_pivot = sbp_dbp.pivot_table(\n",
    "        index=['encounter_block','recorded_dttm'],\n",
    "        columns='vital_category',\n",
    "        values='vital_value'\n",
    "    ).reset_index()\n",
    "    \n",
    "    # 3) Drop any row missing sbp or dbp\n",
    "    sbp_dbp_pivot = sbp_dbp_pivot.dropna(subset=['sbp','dbp'])\n",
    "    \n",
    "    # 4) Calculate MAP\n",
    "    sbp_dbp_pivot['map'] = (sbp_dbp_pivot['sbp'] + 2*sbp_dbp_pivot['dbp']) / 3\n",
    "    \n",
    "    # 5) Build a DataFrame for map\n",
    "    map_vitals = sbp_dbp_pivot[['encounter_block','recorded_dttm','map']].copy()\n",
    "    map_vitals['vital_category'] = 'map'\n",
    "    map_vitals['vital_value'] = map_vitals['map']\n",
    "    \n",
    "    # Also add recorded_date/hour\n",
    "    map_vitals['recorded_date'] = map_vitals['recorded_dttm'].dt.date\n",
    "    map_vitals['recorded_hour'] = map_vitals['recorded_dttm'].dt.hour\n",
    "    \n",
    "    # Keep only the needed columns\n",
    "    map_vitals = map_vitals[[\n",
    "        'encounter_block','recorded_dttm','recorded_date','recorded_hour','vital_category','vital_value'\n",
    "    ]]\n",
    "    \n",
    "    # 6) Append 'map' to the main vitals_stitched DataFrame\n",
    "    vitals_stitched = pd.concat([vitals_stitched, map_vitals], ignore_index=True)\n",
    "    print(\"...map was calculated and appended to vitals_stitched.\")\n",
    "else:\n",
    "    print(\"Map exists in your CLIF database\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute min/max vitals  at the BLOCK level\n",
    "# group by encounter_block + recorded_date + recorded_hour + vital_category\n",
    "vitals_min_max = vitals_stitched.groupby(\n",
    "    ['encounter_block','recorded_date','recorded_hour','vital_category']\n",
    ").agg(\n",
    "    min_val=('vital_value','min'),\n",
    "    max_val=('vital_value','max')\n",
    ").reset_index()\n",
    "\n",
    "# 3) Pivot so each row is unique by (encounter_block, recorded_date, recorded_hour),\n",
    "#    with columns like min_sbp, max_sbp, min_map, max_map, etc.\n",
    "vitals_pivot = vitals_min_max.pivot_table(\n",
    "    index=['encounter_block','recorded_date','recorded_hour'],\n",
    "    columns='vital_category',\n",
    "    values=['min_val','max_val']\n",
    ").reset_index()\n",
    "\n",
    "# Flatten the multi-level columns\n",
    "vitals_pivot.columns = [\n",
    "    '_'.join(col).rstrip('_') if isinstance(col, tuple) else col \n",
    "    for col in vitals_pivot.columns\n",
    "]\n",
    "\n",
    "#  Rename columns for clarity\n",
    "rename_dict = {}\n",
    "for c in vitals_pivot.columns:\n",
    "    if c.startswith('min_val_'):\n",
    "        rename_dict[c] = c.replace('min_val_','min_')\n",
    "    elif c.startswith('max_val_'):\n",
    "        rename_dict[c] = c.replace('max_val_','max_')\n",
    "\n",
    "vitals_pivot = vitals_pivot.rename(columns=rename_dict)\n",
    "\n",
    "# The resulting columns might look like:\n",
    "# ['encounter_block','recorded_date','recorded_hour',\n",
    "#  'min_sbp','max_sbp','min_map','max_map','min_resp_rate','max_resp_rate', etc.]\n",
    "\n",
    "print(\"Finished creating block-level min/max vitals pivot:\")\n",
    "vitals_pivot.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## confirm duplicates don't exist\n",
    "checkpoint_vitals = pyCLIF.remove_duplicates(final_df, [\n",
    "    'encounter_block','recorded_date', 'recorded_hour'\n",
    "], 'final_df')\n",
    "del checkpoint_vitals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge vitals with final_df\n",
    "final_df = pd.merge(final_df, vitals_pivot, on=['encounter_block', 'recorded_date', 'recorded_hour'], \n",
    "                   how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hourly Meds\n",
    "\n",
    "* Handle med dose unit conversion for all vasoactives\n",
    "* Calculate NE equivalent levels using \"norepinephrine\", \"epinephrine\", \"phenylephrine\", \"vasopressin\", \"dopamine\",  \"angiotensin\"\n",
    "* Create flags for \"nicardipine\", \"nitroprusside\", \"clevidipine\" for the red criteria under consensus criteria\n",
    "* Identify encounters on paralytics - cisatracurium, vecuronium, rocuronium- and create flags for each of these paralytic meds. These patients will not be considered eligible for mobilization during the hour they were receiving paralytic medication. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import clif continuous meds for the cohort on vent during the required time period\n",
    "meds_filters = {\n",
    "    'hospitalization_id': all_ids['hospitalization_id'].unique().tolist(),\n",
    "    'med_category': meds_of_interest\n",
    "}\n",
    "meds = pyCLIF.load_data('clif_medication_admin_continuous', columns=meds_required_columns, filters=meds_filters)\n",
    "meds = meds.merge(all_ids, on='hospitalization_id', how='left')\n",
    "print(\"Unique encounters in meds\", pyCLIF.count_unique_encounters(meds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure correct format\n",
    "meds['hospitalization_id']= meds['hospitalization_id'].astype(str)\n",
    "meds['med_dose_unit'] = meds['med_dose_unit'].str.lower()\n",
    "meds = pyCLIF.convert_datetime_columns_to_site_tz(meds,  pyCLIF.helper['timezone'])\n",
    "meds['med_dose'] = pd.to_numeric(meds['med_dose'], errors='coerce')\n",
    "# Create 'date' and 'hour_of_day' columns\n",
    "meds['recorded_date'] = meds['admin_dttm'].dt.date\n",
    "meds['recorded_hour'] = meds['admin_dttm'].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary table for each med_category\n",
    "summary_meds= meds.groupby('med_category').agg(\n",
    "    total_N=('med_category', 'size'),\n",
    "    min=('med_dose', 'min'),\n",
    "    max=('med_dose', 'max'),\n",
    "    first_quantile=('med_dose', lambda x: x.quantile(0.25)),\n",
    "    second_quantile=('med_dose', lambda x: x.quantile(0.5)),\n",
    "    third_quantile=('med_dose', lambda x: x.quantile(0.75)),\n",
    "    missing_values=('med_dose', lambda x: x.isna().sum())\n",
    ").reset_index()\n",
    "\n",
    "summary_meds.to_csv('../output/final/summary_meds_by_category.csv', index=False)\n",
    "## check the distrbituon of required continuous meds\n",
    "summary_meds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary table for each med_category and med_dose_unit combination\n",
    "summary_meds_cat_dose= meds.groupby(['med_category', 'med_dose_unit']).agg(\n",
    "    total_N=('med_category', 'size'),\n",
    "    min=('med_dose', 'min'),\n",
    "    max=('med_dose', 'max'),\n",
    "    first_quantile=('med_dose', lambda x: x.quantile(0.25)),\n",
    "    second_quantile=('med_dose', lambda x: x.quantile(0.5)),\n",
    "    third_quantile=('med_dose', lambda x: x.quantile(0.75)),\n",
    "    missing_values=('med_dose', lambda x: x.isna().sum())\n",
    ").reset_index()\n",
    "summary_meds_cat_dose.to_csv('../output/final/summary_meds_by_category_dose_units.csv', index=False)\n",
    "## check the distrbituon of required continuous meds\n",
    "summary_meds_cat_dose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by med_category and med_dose_unit\n",
    "grouped_data = meds.groupby(['med_category', 'med_dose_unit'])\n",
    "\n",
    "# Dynamically determine the number of required subplots\n",
    "n_plots = len(grouped_data.groups.keys())\n",
    "n_cols = 4\n",
    "n_rows = (n_plots + n_cols - 1) // n_cols  # Round up to determine rows\n",
    "\n",
    "fig, axs = plt.subplots(n_rows, n_cols, figsize=(20, n_rows * 5))\n",
    "\n",
    "# Flatten the axs array for easier indexing\n",
    "axs = axs.flatten()\n",
    "\n",
    "# Loop through each group and plot the histogram\n",
    "for i, ((med_category, med_dose_unit), group) in enumerate(grouped_data):\n",
    "    ax = axs[i]\n",
    "    ax.hist(group['med_dose'], bins=20, alpha=0.7, label=f\"N = {len(group)}\")\n",
    "    ax.set_title(f\"{med_category} - {med_dose_unit}\")\n",
    "    ax.set_xlabel('Med Dose')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "# Hide any unused axes\n",
    "for j in range(i + 1, len(axs)):\n",
    "    axs[j].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../output/final/graphs/meds_histograms.png')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SANITY CHECKS- Check the med_dose_unit for each med_category in the meds table\n",
    "med_dose_unit_check = meds.groupby(['med_category', 'med_dose_unit']).size().reset_index(name='count')\n",
    "\n",
    "# Apply the function to the DataFrame\n",
    "med_dose_unit_check['unit_validity'] = med_dose_unit_check.apply(pyCLIF.check_dose_unit, axis=1)\n",
    "\n",
    "# # Optional: Filter for invalid units\n",
    "invalid_units = med_dose_unit_check[med_dose_unit_check['unit_validity'] == 'Not an acceptable unit']\n",
    "print(\"Invalid units. These will be dropped:\\n\")\n",
    "print(invalid_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Norepinephrine equivalent calculation\n",
    "# Goradia S, Sardaneh AA, Narayan SW, Penm J, Patanwala AE. Vasopressor dose equivalence: \n",
    "# A scoping review and suggested formula. J Crit Care. 2021 Feb;61:233-240. doi: 10.1016/j.jcrc.2020.11.002. Epub 2020 Nov 14. PMID: 33220576.\n",
    "\n",
    "# Filter meds to include only rows with '/hr' or '/min' in 'med_dose_unit'\n",
    "meds_filtered = meds[~meds['med_dose'].isnull()].copy()\n",
    "meds_filtered = meds_filtered[meds_filtered['med_dose_unit'].apply(pyCLIF.has_per_hour_or_min)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meds_list = [\n",
    "    \"norepinephrine\", \"epinephrine\", \"phenylephrine\", \n",
    "    \"vasopressin\", \"dopamine\",  \n",
    "    \"angiotensin\"\n",
    "]\n",
    "\n",
    "# **2. Convert Medication Doses to Required Units**\n",
    "ne_df = meds_filtered[meds_filtered['med_category'].isin(meds_list)].copy()\n",
    "# Merge weight_kg into meds_filtered (assuming 'vitals_bmi_pivot' is available)\n",
    "ne_df = ne_df.merge(vitals_bmi_pivot[['encounter_block', 'weight_kg']], on='encounter_block', how='left')\n",
    "ne_df[\"med_dose_converted\"] = ne_df.apply(pyCLIF.convert_dose, axis=1)\n",
    "\n",
    "# Filter doses within acceptable ranges\n",
    "ne_df = ne_df[ne_df.apply(pyCLIF.is_dose_within_range, axis=1, args=(outlier_cfg,))].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **4. Flag Medications Not in the Dataset**\n",
    "\n",
    "for med in meds_list:\n",
    "    if med not in ne_df['med_category'].unique():\n",
    "        print(f\"❌ {med} is not in the dataset.\")\n",
    "    print(f\"✅ {med} is in the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot and Aggregate the Data**\n",
    "# Group and aggregate doses\n",
    "group_cols = ['encounter_block', 'recorded_date', 'recorded_hour', 'med_category']\n",
    "dose_agg = ne_df.groupby(group_cols)['med_dose_converted'].agg(['min', 'max', 'first', 'last']).reset_index()\n",
    "\n",
    "# Pivot to have medications as columns\n",
    "dose_pivot_min   = dose_agg.pivot_table(index=['encounter_block', 'recorded_date', 'recorded_hour'], columns='med_category', values='min').reset_index()\n",
    "dose_pivot_max   = dose_agg.pivot_table(index=['encounter_block', 'recorded_date', 'recorded_hour'], columns='med_category', values='max').reset_index()\n",
    "dose_pivot_first = dose_agg.pivot_table(index=['encounter_block', 'recorded_date', 'recorded_hour'], columns='med_category', values='first').reset_index()\n",
    "dose_pivot_last  = dose_agg.pivot_table(index=['encounter_block', 'recorded_date', 'recorded_hour'], columns='med_category', values='last').reset_index()\n",
    "\n",
    "# Rename columns to indicate min and max\n",
    "dose_pivot_min.columns   = ['encounter_block', 'recorded_date', 'recorded_hour'] + ['min_'   + col for col in dose_pivot_min.columns if col not in ['encounter_block', 'recorded_date', 'recorded_hour']]\n",
    "dose_pivot_max.columns   = ['encounter_block', 'recorded_date', 'recorded_hour'] + ['max_'   + col for col in dose_pivot_max.columns if col not in ['encounter_block', 'recorded_date', 'recorded_hour']]\n",
    "dose_pivot_first.columns = ['encounter_block', 'recorded_date', 'recorded_hour'] + ['first_' + col for col in dose_pivot_first.columns if col not in ['encounter_block', 'recorded_date', 'recorded_hour']]\n",
    "dose_pivot_last.columns  = ['encounter_block', 'recorded_date', 'recorded_hour'] + ['last_'  + col for col in dose_pivot_last.columns if col not in ['encounter_block', 'recorded_date', 'recorded_hour']]\n",
    "\n",
    "# Merge min and max DataFrames\n",
    "dose_pivot = pyCLIF.merge_multiple_dfs(dose_pivot_min, dose_pivot_max, dose_pivot_first, dose_pivot_last,\n",
    "                              on=['encounter_block', 'recorded_date', 'recorded_hour'],\n",
    "                              how='outer')\n",
    "\n",
    "# **6. Calculate Norepinephrine Equivalents**\n",
    "\n",
    "# Replace NaN with 0 for calculations\n",
    "dose_pivot.fillna(0, inplace=True)\n",
    "\n",
    "# Calculate NE min\n",
    "dose_pivot['ne_calc_min'] = (\n",
    "    dose_pivot.get('min_norepinephrine', 0) +\n",
    "    dose_pivot.get('min_epinephrine', 0) +\n",
    "    dose_pivot.get('min_phenylephrine', 0) / 10 +\n",
    "    dose_pivot.get('min_dopamine', 0) / 100 +\n",
    "    dose_pivot.get('min_metaraminol', 0) / 8 +\n",
    "    dose_pivot.get('min_vasopressin', 0) * 2.5 +\n",
    "    dose_pivot.get('min_angiotensin', 0) * 10\n",
    ")\n",
    "\n",
    "# Calculate NE max\n",
    "dose_pivot['ne_calc_max'] = (\n",
    "    dose_pivot.get('max_norepinephrine', 0) +\n",
    "    dose_pivot.get('max_epinephrine', 0) +\n",
    "    dose_pivot.get('max_phenylephrine', 0) / 10 +\n",
    "    dose_pivot.get('max_dopamine', 0) / 100 +\n",
    "    dose_pivot.get('max_metaraminol', 0) / 8 +\n",
    "    dose_pivot.get('max_vasopressin', 0) * 2.5 +\n",
    "    dose_pivot.get('max_angiotensin', 0) * 10\n",
    ")\n",
    "\n",
    "# Calculate NE first\n",
    "dose_pivot['ne_calc_first'] = (\n",
    "    dose_pivot.get('first_norepinephrine', 0) +\n",
    "    dose_pivot.get('first_epinephrine', 0) +\n",
    "    dose_pivot.get('first_phenylephrine', 0) / 10 +\n",
    "    dose_pivot.get('first_dopamine', 0) / 100 +\n",
    "    dose_pivot.get('first_metaraminol', 0) / 8 +\n",
    "    dose_pivot.get('first_vasopressin', 0) * 2.5 +\n",
    "    dose_pivot.get('first_angiotensin', 0) * 10\n",
    ")\n",
    "\n",
    "# Calculate NE last\n",
    "dose_pivot['ne_calc_last'] = (\n",
    "    dose_pivot.get('last_norepinephrine', 0) +\n",
    "    dose_pivot.get('last_epinephrine', 0) +\n",
    "    dose_pivot.get('last_phenylephrine', 0) / 10 +\n",
    "    dose_pivot.get('last_dopamine', 0) / 100 +\n",
    "    dose_pivot.get('last_metaraminol', 0) / 8 +\n",
    "    dose_pivot.get('last_vasopressin', 0) * 2.5 +\n",
    "    dose_pivot.get('last_angiotensin', 0) * 10\n",
    ")\n",
    "\n",
    "# **7. Prepare the Final Dataset**\n",
    "# Keep only the required columns\n",
    "ne_calc_df = dose_pivot[['encounter_block', 'recorded_date', \n",
    "                         'recorded_hour', \n",
    "                         'ne_calc_min', 'ne_calc_max', \n",
    "                         'ne_calc_first', 'ne_calc_last']].drop_duplicates(subset=['encounter_block', 'recorded_date', 'recorded_hour'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strobe_counts['final_blocks_with_norepi_eq'] = ne_calc_df['encounter_block'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encounter_blocks_list = ne_df['encounter_block'].unique().tolist()\n",
    "import importlib\n",
    "import pyCLIF\n",
    "importlib.reload(pyCLIF)\n",
    "hourly_ne = pyCLIF.build_meds_hourly_scaffold(\n",
    "    ne_df,\n",
    "    id_col=\"encounter_block\",      # column to group by\n",
    "    ids=encounter_blocks_list,     # Iterable of id_col to keep\n",
    "    timestamp_col=\"admin_dttm\",    # change if your column is named differently\n",
    "    site_tz=\"US/Central\"           # change to the zone you need\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_meds = pyCLIF.remove_duplicates(hourly_ne, [\n",
    "    'encounter_block','recorded_date', 'recorded_hour'\n",
    "], 'final_df')\n",
    "del checkpoint_meds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the DataFrame is sorted by 'hospitalization_id' and 'time_from_vent'\n",
    "ne_calc_df = ne_calc_df.sort_values(by=['encounter_block', 'recorded_date', 'recorded_hour'])\n",
    "# Merge the norepinephrine equivalent DataFrame with the hourly norepinephrine DataFrame\n",
    "hourly_ne_merged = pd.merge(\n",
    "    hourly_ne,\n",
    "    ne_calc_df,\n",
    "    on=['encounter_block', 'recorded_date', 'recorded_hour'],\n",
    "    how='left'\n",
    ")\n",
    "# Fill forward the specified columns\n",
    "cols_to_fill = ['ne_calc_min', 'ne_calc_max', 'ne_calc_first', 'ne_calc_last']\n",
    "hourly_ne_merged[cols_to_fill] = hourly_ne_merged[cols_to_fill].fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_last_ne_6h(group: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For one encounter_block add/overwrite the column\n",
    "    `last_ne_dose_last_6_hours` with the `ne_calc_last` value that\n",
    "    occurred **exactly six hours earlier**.  If that row does not\n",
    "    exist (e.g. the first <6 hours of the stay) the value is 0.\n",
    "    \"\"\"\n",
    "    group['last_ne_dose_last_6_hours'] = (\n",
    "        group['ne_calc_last']\n",
    "        .shift(6)           # value 6 rows (hours) ago\n",
    "        .fillna(0)          # treat “no record” as 0\n",
    "    )\n",
    "    return group\n",
    "\n",
    "hourly_ne_merged = (\n",
    "    hourly_ne_merged\n",
    "      .groupby('encounter_block', group_keys=False)\n",
    "      .apply(add_last_ne_6h)\n",
    "      .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## confirm duplicates don't exist\n",
    "checkpoint_meds = pyCLIF.remove_duplicates(hourly_ne_merged, [\n",
    "    'encounter_block','recorded_date', 'recorded_hour'\n",
    "], 'final_df')\n",
    "del checkpoint_meds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.merge(final_df, \n",
    "                    hourly_ne_merged, \n",
    "                    on=['encounter_block', 'recorded_date', 'recorded_hour'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_meds_list = [\n",
    "    \"nicardipine\", \"nitroprusside\", \"clevidipine\"\n",
    "]\n",
    "\n",
    "# Filter meds_filtered for the medications in red_meds_list\n",
    "red_meds_df = meds[meds['med_category'].isin(red_meds_list)].copy()\n",
    "\n",
    "# Create a flag for each medication in red_meds_list\n",
    "for med in red_meds_list:\n",
    "    # Create a flag that is 1 if the medication was administered in that hour, 0 otherwise\n",
    "    red_meds_df[med + '_flag'] = np.where((red_meds_df['med_category'] == med) & \n",
    "                                         (red_meds_df['med_dose'] > 0.0) & \n",
    "                                         (red_meds_df['med_dose'].notna()), 1, 0).astype(int)\n",
    "\n",
    "# Aggregate to get the maximum value for each flag (per hospitalization_id, recorded_date, recorded_hour)\n",
    "# This ensures that if the medication was administered even once in the hour, the flag is 1\n",
    "red_meds_flags = red_meds_df.groupby(['encounter_block', 'recorded_date', 'recorded_hour']).agg(\n",
    "    {med + '_flag': 'max' for med in red_meds_list}\n",
    ").reset_index()\n",
    "\n",
    "#  combine all flags into a single 'red_meds_flag', you can do so like this:\n",
    "red_meds_flags['red_meds_flag'] = red_meds_flags[[med + '_flag' for med in red_meds_list]].max(axis=1)\n",
    "\n",
    "# Select the relevant columns\n",
    "red_meds_flags_final = red_meds_flags[[\n",
    "    'encounter_block', 'recorded_date', 'recorded_hour',\n",
    "    'nicardipine_flag', 'nitroprusside_flag',\n",
    "    'clevidipine_flag', 'red_meds_flag'\n",
    "]].drop_duplicates(subset=['encounter_block', 'recorded_date', 'recorded_hour'])\n",
    "\n",
    "red_meds_flags_final['nicardipine_flag'] = red_meds_flags_final['nicardipine_flag'].astype(int)\n",
    "red_meds_flags_final['nitroprusside_flag'] = red_meds_flags_final['nitroprusside_flag'].astype(int)\n",
    "red_meds_flags_final['clevidipine_flag'] = red_meds_flags_final['clevidipine_flag'].astype(int)\n",
    "red_meds_flags_final['red_meds_flag'] = red_meds_flags_final['red_meds_flag'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strobe_counts['final_blocks_with_red_meds'] = red_meds_flags_final['encounter_block'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## confirm duplicates don't exist\n",
    "checkpoint_red_meds = pyCLIF.remove_duplicates(red_meds_flags_final, [\n",
    "    'encounter_block','recorded_date', 'recorded_hour'\n",
    "], 'final_df')\n",
    "del checkpoint_red_meds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.merge(final_df, \n",
    "                    red_meds_flags_final, \n",
    "                    on=['encounter_block', 'recorded_date', 'recorded_hour'], \n",
    "                    how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paralytics_list = [\n",
    "    \"cisatracurium\", \"vecuronium\", \"rocuronium\" \n",
    "]\n",
    "\n",
    "# Filter meds_filtered for the medications in paralytics_list\n",
    "paralytics_df = meds[meds['med_category'].isin(paralytics_list)].copy()\n",
    "\n",
    "# Create a flag for each medication in paralytics_list\n",
    "for med in paralytics_list:\n",
    "    # Create a flag that is 1 if the medication was administered in that hour, 0 otherwise\n",
    "    paralytics_df[med + '_flag'] = np.where((paralytics_df['med_category'] == med) & \n",
    "                                           (paralytics_df['med_dose'] > 0.0) &\n",
    "                                           (paralytics_df['med_dose'].notna()), 1, 0).astype(int)\n",
    "\n",
    "# Aggregate to get the maximum value for each flag (per hospitalization_id, recorded_date, recorded_hour)\n",
    "# This ensures that if the medication was administered even once in the hour, the flag is 1\n",
    "paralytics_flags = paralytics_df.groupby(['encounter_block', 'recorded_date', 'recorded_hour']).agg(\n",
    "    {med + '_flag': 'max' for med in paralytics_list}\n",
    ").reset_index()\n",
    "\n",
    "#  combine all flags into a single 'paralytics_flag', you can do so like this:\n",
    "paralytics_flags['paralytics_flag'] = paralytics_flags[[med + '_flag' for med in paralytics_list]].max(axis=1)\n",
    "\n",
    "# Select the relevant columns\n",
    "paralytics_flags_final = paralytics_flags[[\n",
    "    'encounter_block', 'recorded_date', 'recorded_hour',\n",
    "    'cisatracurium_flag', 'vecuronium_flag',\n",
    "    'rocuronium_flag', 'paralytics_flag'\n",
    "]].drop_duplicates(subset=['encounter_block', 'recorded_date', 'recorded_hour'])\n",
    "\n",
    "paralytics_flags_final['cisatracurium_flag'] = paralytics_flags_final['cisatracurium_flag'].astype(int)\n",
    "paralytics_flags_final['vecuronium_flag'] = paralytics_flags_final['vecuronium_flag'].astype(int)\n",
    "paralytics_flags_final['rocuronium_flag'] = paralytics_flags_final['rocuronium_flag'].astype(int)\n",
    "paralytics_flags_final['paralytics_flag'] = paralytics_flags_final['paralytics_flag'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strobe_counts['final_blocks_with_paralytics'] = paralytics_flags_final['encounter_block'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## confirm duplicates don't exist\n",
    "checkpoint_paralytics_meds = pyCLIF.remove_duplicates(paralytics_flags_final, [\n",
    "    'encounter_block','recorded_date', 'recorded_hour'\n",
    "], 'final_df')\n",
    "del checkpoint_paralytics_meds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.merge(final_df, \n",
    "                    paralytics_flags_final, \n",
    "                    on=['encounter_block', 'recorded_date', 'recorded_hour'], \n",
    "                    how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hourly Labs\n",
    "\n",
    "Get most recent lactate defined as closest lab result time to the start of first intubation event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import clif continuous meds and clif labs table for the cohort on vent during the required time period\n",
    "labs_filters = {\n",
    "    'hospitalization_id': all_ids['hospitalization_id'].unique().tolist(),\n",
    "    'lab_category': labs_of_interest\n",
    "}\n",
    "labs = pyCLIF.load_data('clif_labs', columns=labs_required_columns, filters=labs_filters)\n",
    "print(\"unique encounters in labs\", pyCLIF.count_unique_encounters(labs))\n",
    "labs['hospitalization_id']= labs['hospitalization_id'].astype(str)\n",
    "labs = labs.merge(all_ids, on='hospitalization_id', how='left')\n",
    "labs = labs.sort_values(by=['encounter_block', 'lab_result_dttm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strobe_counts['final_blocks_with_lactate_lab'] = labs['encounter_block'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labs = pyCLIF.convert_datetime_columns_to_site_tz(labs, pyCLIF.helper['timezone'])\n",
    "labs['recorded_hour'] = labs['lab_result_dttm'].dt.hour\n",
    "labs['recorded_date'] = labs['lab_result_dttm'].dt.date\n",
    "\n",
    "lactate_df = pd.merge(labs, block_vent_times, on='encounter_block', how='left')\n",
    "lactate_df['time_since_vent_start_hours'] = (\n",
    "    (lactate_df['lab_result_dttm'] - lactate_df['block_vent_start_dttm']).dt.total_seconds() / 3600\n",
    ")\n",
    "\n",
    "# Calculate the absolute time difference between lab_result_dttm and vent_start_time in hours\n",
    "lactate_df['time_diff_hours'] = abs((lactate_df['lab_result_dttm'] - lactate_df['block_vent_start_dttm']).dt.total_seconds() / 3600)\n",
    "\n",
    "# Sort by encounter_block, recorded_hour, and time_diff_hours to find the closest measurement to vent_start_time\n",
    "lactate_df = lactate_df.sort_values(by=['encounter_block', 'recorded_date', 'recorded_hour', 'time_diff_hours'])\n",
    "\n",
    "# Group by encounter_block and recorded_hour, and get the first row in each group (which is the closest measurement)\n",
    "# closest lactate measurement is defined as closest to the vent_start_time in that hour.\n",
    "# we keep the first recorded value in that hour \n",
    "closest_lactate_df = lactate_df.groupby(['encounter_block', 'recorded_date','recorded_hour']).first().reset_index()\n",
    "\n",
    "labs_final = closest_lactate_df[['encounter_block', 'recorded_date', 'recorded_hour', 'lab_value_numeric']].copy()\n",
    "\n",
    "# Rename the 'lab_value_numeric' column to 'lactate'\n",
    "labs_final = labs_final.rename(columns={'lab_value_numeric': 'lactate'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_labs= pyCLIF.remove_duplicates(final_df, [\n",
    "    'encounter_block', 'recorded_date', 'recorded_hour'\n",
    "], 'final_df')\n",
    "del checkpoint_labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.merge(final_df, \n",
    "                    labs_final, \n",
    "                    on=['encounter_block', 'recorded_date', 'recorded_hour'], \n",
    "                   how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(sofa_score)\n",
    "import sofa_score\n",
    "helper = pyCLIF.load_config()\n",
    "tables_path= helper['tables_path']\n",
    "\n",
    "sofa_input_df = all_ids_w_outcome[['encounter_block', 'block_vent_start_dttm']].copy()\n",
    "sofa_input_df = sofa_input_df.rename(columns={'block_vent_start_dttm': 'start_dttm'})\n",
    "sofa_input_df['stop_dttm'] = sofa_input_df['start_dttm'] + pd.Timedelta(hours=24)\n",
    "id_mappings = all_ids_w_outcome[['encounter_block', 'hospitalization_id' ]].drop_duplicates()\n",
    "\n",
    "sofa_df = sofa_score.compute_sofa(\n",
    "            ids_w_dttm = sofa_input_df,          # id, start_dttm, end_dttm  (local time)\n",
    "            tables_path = tables_path,\n",
    "            use_hospitalization_id = False,         # or False + id_mapping (new id , hospitalization_id)\n",
    "            id_mapping = id_mappings,              # first column should be your new id_variable, second column is hospitalization id\n",
    "            helper_module = pyCLIF,                # ← your existing loader\n",
    "            output_filepath = \"../output/intermediate/sofa.parquet\"\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_blocks = sofa_df.merge(all_ids_w_outcome, on='encounter_block', how='left')\n",
    "final_df_blocks = final_df_blocks.merge(hospitalization[['hospitalization_id', 'admission_dttm', \n",
    "                                      'age_at_admission','admission_type_name', 'admission_type_category']], \n",
    "                                      on='hospitalization_id', how='left')\n",
    "final_df_blocks = final_df_blocks.merge(patient[['patient_id', 'race_category','ethnicity_category', 'sex_category','language_name']], \n",
    "                                      on='patient_id', how='left')\n",
    "\n",
    "# First join ADT with all_ids to get closest ADT row to vent start\n",
    "adt_with_blocks = pd.merge(\n",
    "    all_ids_w_outcome[['encounter_block', 'block_vent_start_dttm', 'hospitalization_id']],\n",
    "    adt,\n",
    "    on='hospitalization_id'\n",
    ")\n",
    "\n",
    "# Calculate time difference between vent start and ADT in_dttm\n",
    "adt_with_blocks['time_diff'] = abs(adt_with_blocks['block_vent_start_dttm'] - adt_with_blocks['in_dttm'])\n",
    "\n",
    "# Get the closest ADT row for each encounter block\n",
    "closest_adt = (adt_with_blocks\n",
    "    .sort_values('time_diff')\n",
    "    .groupby('encounter_block')\n",
    "    .first()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Join with final_df_blocks\n",
    "final_df_blocks = final_df_blocks.merge(\n",
    "    closest_adt[['encounter_block', 'location_name', 'location_category', 'in_dttm', 'out_dttm']],\n",
    "    on='encounter_block',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "final_df_blocks.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_blocks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write analysis dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_parquet('../output/intermediate/final_df_hourly.parquet')\n",
    "final_df_blocks.to_parquet('../output/intermediate/final_df_blocks.parquet')\n",
    "all_ids_w_outcome.to_parquet('../output/intermediate/cohort_all_ids_w_outcome.parquet')\n",
    "# Convert the dictionary to a DataFrame and save it as a CSV file\n",
    "pd.DataFrame(list(strobe_counts.items()), columns=['Metric', 'Value']).to_csv('../output/final/strobe_counts.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strobe_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".mobilization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
