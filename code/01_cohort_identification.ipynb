{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eligibility for mobilization: Cohort ID and Discretizing script\n",
    "\n",
    "Author: Kaveri Chhikara\n",
    "v1: October 30, 2024\n",
    "v2: February 10, 2025\n",
    "v3: March 12, 2025\n",
    "v4: March 20, 2025\n",
    "\n",
    "This script identifies the cohort using CLIF 2.0 tables and discretizes the dataset at an hourly level\n",
    "\n",
    " \n",
    "                        🚨Code will break if the following requirements are not satisfied🚨  \n",
    "#### Requirements:\n",
    "* Required table filenames should be `clif_patient`, `clif_hospitalization`, `clif_adt`, `clif_vitals`, `clif_labs`, `clif_medication_admin_continuous`, `clif_respiratory_support`\n",
    "* Within each table, the following variables and categories are required.\n",
    "\n",
    "| Table Name | Required Variables | Required Categories |\n",
    "| --- | --- | --- |\n",
    "| **patient** | `patient_id`, `race_category`, `ethnicity_category`, `sex_category`, `death_dttm` | - |\n",
    "| **hospitalization** | `patient_id`, `hospitalization_id`, `admission_dttm`, `discharge_dttm`, `age_at_admission` | - |\n",
    "| **adt** |  `hospitalization_id`, `hospital_id`,`in_dttm`, `out_dttm`, `location_category` | - |\n",
    "| **vitals** | `hospitalization_id`, `recorded_dttm`, `vital_category`, `vital_value` | heart_rate, resp_rate, sbp, dbp, map, spo2, weight_kg, height_cm |\n",
    "| **labs** | `hospitalization_id`, `lab_result_dttm`, `lab_category`, `lab_value` | lactate |\n",
    "| **medication_admin_continuous** | `hospitalization_id`, `admin_dttm`, `med_name`, `med_category`, `med_dose`, `med_dose_unit` | norepinephrine, epinephrine, phenylephrine, vasopressin, dopamine, angiotensin(optional), nicardipine, nitroprusside, clevidipine, cisatracurium, vecuronium, rocuronium |\n",
    "| **respiratory_support** | `hospitalization_id`, `recorded_dttm`, `device_category`, `mode_category`, `tracheostomy`, `fio2_set`, `lpm_set`, `resp_rate_set`, `peep_set`, `resp_rate_obs` | - |\n",
    "\n",
    "\n",
    "Updates 2/10:\n",
    "* Get discharge_dttm and death_dttm. Everyone in the cohort must have one of these. If not dead, assume discharged alive.\n",
    "* Include all paralytics in the mCIDE. Instead of excluding anyone who ever received a paralytics, create flags for paralytics. While creating flags for eligibility, exclude hours when the patient was on a paralytic.\n",
    "* Update exclusion criteria - exclude all patients intubated for < 4 hrs instead of 2 hrs to be consistent with the cool-off period.\n",
    "* Add code to stitch encounters when there are multiple hospitals at a site\n",
    "* Extend the analysis to competing risk. Events - 1 - eligible, 2- died, 3 discharged-alive\n",
    "\n",
    "Updates 3/12 \n",
    "* Updated code to handle dttm conversion from UTC to local time from the config file. No updates required in analysis script after the analysis df are imported.\n",
    "\n",
    "Updates 3/20\n",
    "* Updated eligibility flags to only become eligible during business hours. This gives us real time to eligibility from intubation. Use the entire hospitalization as the input for survival analysis.\n",
    "* Updated the `.qmd` script for competing risk analysis. Compare with UMN's results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install pandas numpy duckdb seaborn matplotlib plotly\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "import pyCLIF\n",
    "\n",
    "## import outlier json\n",
    "with open('../config/outlier_config.json', 'r') as f:\n",
    "    outlier_cfg = json.load(f)\n",
    "\n",
    "graphs_folder = '../output/final/graphs'\n",
    "if not os.path.exists(graphs_folder):\n",
    "    os.makedirs(graphs_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required columns and categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rst_required_columns = [\n",
    "    'hospitalization_id',\n",
    "    'recorded_dttm',\n",
    "    'device_name',\n",
    "    'device_category',\n",
    "    'mode_name', \n",
    "    'mode_category',\n",
    "    'tracheostomy',\n",
    "    'fio2_set',\n",
    "    'lpm_set',\n",
    "    'resp_rate_set',\n",
    "    'peep_set',\n",
    "    'resp_rate_obs'\n",
    "]\n",
    "\n",
    "vitals_required_columns = [\n",
    "    'hospitalization_id',\n",
    "    'recorded_dttm',\n",
    "    'vital_category',\n",
    "    'vital_value'\n",
    "]\n",
    "vitals_of_interest = ['heart_rate', 'respiratory_rate', 'sbp', 'dbp', 'map', 'spo2', 'weight_kg', 'height_cm']\n",
    "\n",
    "labs_required_columns = [\n",
    "    'hospitalization_id',\n",
    "    'lab_result_dttm',\n",
    "    'lab_category',\n",
    "    'lab_value',\n",
    "    'lab_value_numeric'\n",
    "]\n",
    "labs_of_interest = ['lactate']\n",
    "\n",
    "meds_required_columns = [\n",
    "    'hospitalization_id',\n",
    "    'admin_dttm',\n",
    "    'med_name',\n",
    "    'med_category',\n",
    "    'med_dose',\n",
    "    'med_dose_unit'\n",
    "]\n",
    "meds_of_interest = [\n",
    "    'norepinephrine', 'epinephrine', 'phenylephrine', 'vasopressin',\n",
    "    'dopamine', 'angiotensin', 'nicardipine', 'nitroprusside',\n",
    "    'clevidipine', 'cisatracurium', 'vecuronium', 'rocuronium '\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient = pyCLIF.load_data('clif_patient')\n",
    "hospitalization = pyCLIF.load_data('clif_hospitalization')\n",
    "adt = pyCLIF.load_data('clif_adt')\n",
    "\n",
    "# ensure id variable is of dtype character\n",
    "hospitalization['hospitalization_id']= hospitalization['hospitalization_id'].astype(str)\n",
    "patient['patient_id']= patient['patient_id'].astype(str)\n",
    "adt['hospitalization_id']= adt['hospitalization_id'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duplicate check\n",
    "\n",
    "If duplicates exist, only the first row is preserved after arranging the data by time. Please check your CLIF tables if there are duplicates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicates\n",
    "# patient table should be unique by patient id\n",
    "patient = pyCLIF.remove_duplicates(patient, ['patient_id'], 'patient')\n",
    "# hospitalization table should be unique by hospitalization id\n",
    "hospitalization = pyCLIF.remove_duplicates(hospitalization, ['hospitalization_id'], 'hospitalization')\n",
    "# adt table should be unique by hospitalization id and in dttm\n",
    "adt = pyCLIF.remove_duplicates(adt, ['hospitalization_id', 'hospital_id', 'in_dttm'], 'adt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total Number of unique encounters in the hospitalization table: {pyCLIF.count_unique_encounters(hospitalization, 'hospitalization_id')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adt.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize all _dttm variables to the same format\n",
    "import importlib\n",
    "importlib.reload(pyCLIF)\n",
    "patient = pyCLIF.standardize_datetime_tz(patient, 'death_dttm', pyCLIF.helper['timezone'])\n",
    "hospitalization = pyCLIF.standardize_datetime_tz(hospitalization, ['admission_dttm', 'discharge_dttm'], pyCLIF.helper['timezone'])\n",
    "adt = pyCLIF.standardize_datetime_tz(adt, ['in_dttm', 'out_dttm'], pyCLIF.helper['timezone'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adt.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cohort Identification\n",
    "\n",
    "**Inclusion Criteria:**\n",
    "\n",
    "* Adult admissions between 2020-03-01 and 2022-03-31\n",
    "* Encounters receiving invasive mechanical ventilation during this period\n",
    "\n",
    "**Exclusion criteria:**\n",
    "\n",
    "1. Encounters that were on vent for less than 4 hours in the first 72 hours of first intubation\n",
    "2. Encounters that were on trach in the first 72 hours of first intubation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up a dictionary to keep track of STROBE counts\n",
    "strobe_counts = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (A) Date and Age Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP A: Basic Data Cleaning + Date/Age Filter\n",
    "#   - Filter hospitalization for date range & adult patients\n",
    "#   - Then reduce ADT to those hospitalization_ids\n",
    "\n",
    "\n",
    "print(\"\\n=== STEP A: Filter by date range & age ===\\n\")\n",
    "date_mask = (hospitalization['admission_dttm'] >= '2020-03-01') & \\\n",
    "            (hospitalization['admission_dttm'] <= '2022-03-31')\n",
    "age_mask = (hospitalization['age_at_admission'] >= 18)\n",
    "\n",
    "hospitalization_cohort = hospitalization[date_mask & age_mask].copy()\n",
    "\n",
    "strobe_counts['A_after_date_age_filter'] = hospitalization_cohort['hospitalization_id'].nunique()\n",
    "print(f\"Number of unique hospitalizations after date & age filter: {strobe_counts['A_after_date_age_filter']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adt['location_category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get total unique hospitalizations without time filter, only age filter\n",
    "age_mask = (hospitalization['age_at_admission'] >= 18)\n",
    "total_adult_hospitalizations = hospitalization[age_mask]['hospitalization_id'].nunique()\n",
    "print(f\"\\nTotal number of unique adult hospitalizations (no date filter): {total_adult_hospitalizations}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter ADT for ICU locations\n",
    "icu_locations = ['icu', 'ICU']\n",
    "icu_mask = adt['location_category'].isin(icu_locations)\n",
    "icu_adt = adt[icu_mask].copy()\n",
    "\n",
    "# Get unique hospitalization IDs that had ICU stays\n",
    "icu_hosp_ids = icu_adt['hospitalization_id'].unique()\n",
    "\n",
    "# Filter hospitalizations for adult patients who were in ICU\n",
    "age_mask = (hospitalization['age_at_admission'] >= 18)\n",
    "icu_hospitalization = hospitalization[\n",
    "    (hospitalization['hospitalization_id'].isin(icu_hosp_ids)) & \n",
    "    age_mask\n",
    "].copy()\n",
    "\n",
    "# strobe_counts['A_after_date_age_filter'] = icu_hospitalization['hospitalization_id'].nunique()\n",
    "print(f\"Number of unique adult hospitalizations with ICU stays: {icu_hospitalization['hospitalization_id'].nunique()}\")\n",
    "\n",
    "# Update hospitalization_cohort to be ICU cohort\n",
    "# hospitalization_cohort = icu_hospitalization.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (B) Stitch hospitalizations\n",
    "\n",
    "Combine multiple `hospitalization_ids` into a single `encounter_block` for patients who transfer between hospital campuses or return soon after discharge. Hospitalizations that have a gap of **6 hours or less** between the discharge dttm and admission dttm are put in one encounter block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter ADT to only those in the cohort set\n",
    "cohort_ids = hospitalization_cohort['hospitalization_id'].unique().tolist()\n",
    "adt_cohort = adt[adt['hospitalization_id'].isin(cohort_ids)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in admission and discharge dates\n",
    "print(\"\\nMissing values in admission_dttm:\", hospitalization_cohort['admission_dttm'].isna().sum())\n",
    "print(\"Missing values in discharge_dttm:\", hospitalization_cohort['discharge_dttm'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP B: Stitch Encounters => 'encounter_block'\n",
    "# Use stitch_encounters from pyCLIF with time_interval=6\n",
    "\n",
    "print(\"\\n=== STEP B: Stitch encounters ===\\n\")\n",
    "stitched_cohort = pyCLIF.stitch_encounters(hospitalization_cohort, adt_cohort, time_interval=6)\n",
    "# stitched_cohort now has: 'patient_id','hospitalization_id','encounter_block' and other ADT variables. This will have duplicate rows because of location category\n",
    "# We only want 1 row per unique encounter_block for the next steps.\n",
    "stitched_unique = stitched_cohort[['patient_id', 'encounter_block']].drop_duplicates()\n",
    "\n",
    "strobe_counts['B_before_stitching'] = stitched_cohort['hospitalization_id'].nunique()\n",
    "strobe_counts['B_after_stitching'] = stitched_unique['encounter_block'].nunique()\n",
    "strobe_counts['B_stitched_hosp_ids'] = strobe_counts['B_before_stitching']-strobe_counts['B_after_stitching']\n",
    "print(f\"Number of unique hospitalizations before stitching: {stitched_cohort['hospitalization_id'].nunique()}\")\n",
    "print(f\"Number of unique encounter blocks after stitching: {strobe_counts['B_after_stitching']}\")\n",
    "print(f\"Number of linked hospitalization ids: {strobe_counts['B_before_stitching']-strobe_counts['B_after_stitching']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping of patient id, hospitalization id and encounter blocks\n",
    "all_ids = stitched_cohort[['patient_id', 'hospitalization_id', 'encounter_block']].drop_duplicates()\n",
    "print(\"\\nUnique values in each column:\")\n",
    "for col in all_ids.columns:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(all_ids[col].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (C) Identify ventilator usage\n",
    "\n",
    "Filter down to encounters that received invasive mechanical ventilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP C: Identify Ventilator Usage\n",
    "# Load respiratory support only for the relevant “hospitalization_id” set\n",
    "# These hospitalizations map to an encounter_block for final grouping.\n",
    "\n",
    "print(\"\\n=== STEP C: Load & process respiratory support => Identify IMV usage ===\\n\")\n",
    "\n",
    "# 1) Load respiratory support\n",
    "resp_support_raw = pyCLIF.load_data(\n",
    "    'clif_respiratory_support',\n",
    "    columns=rst_required_columns,\n",
    "    filters={'hospitalization_id': all_ids['hospitalization_id'].unique().tolist()}\n",
    ")\n",
    "\n",
    "resp_support = resp_support_raw.copy()\n",
    "resp_support = pyCLIF.standardize_datetime_tz(resp_support, 'recorded_dttm', pyCLIF.helper['timezone'])\n",
    "# resp_support['recorded_dttm'] = pd.to_datetime(resp_support['recorded_dttm'])\n",
    "resp_support = pyCLIF.standardize_datetime_utc(resp_support, 'recorded_dttm') #standardize to utc tz naive\n",
    "resp_support['device_category'] = resp_support['device_category'].str.lower()\n",
    "resp_support['mode_category'] = resp_support['mode_category'].str.lower()\n",
    "resp_support['fio2_set'] = pd.to_numeric(resp_support['fio2_set'], errors='coerce')\n",
    "resp_support['lpm_set'] = pd.to_numeric(resp_support['lpm_set'], errors='coerce')\n",
    "resp_support['resp_rate_set'] = pd.to_numeric(resp_support['resp_rate_set'], errors='coerce')\n",
    "resp_support['peep_set'] = pd.to_numeric(resp_support['peep_set'], errors='coerce')\n",
    "resp_support['resp_rate_obs'] = pd.to_numeric(resp_support['resp_rate_obs'], errors='coerce')\n",
    "\n",
    "# del resp_support_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Respiratory Support Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list = []\n",
    "group_cols = 'device_category'  # or a list like ['device_category','mode_category']\n",
    "numeric_cols = ['fio2_set','peep_set','lpm_set', 'resp_rate_set', 'resp_rate_obs']\n",
    "\n",
    "for col in numeric_cols:\n",
    "    tmp = pyCLIF.create_summary_table(\n",
    "        df=resp_support,\n",
    "        numeric_col=col,\n",
    "        group_by_cols=group_cols\n",
    "    )\n",
    "    # tmp might have columns:\n",
    "    #   ['device_category','N','missing','min','q25','median','q75','mean','max']\n",
    "    # Insert a \"variable\" column next to the group-by columns:\n",
    "    tmp['variable'] = col\n",
    "    # We want \"device_category\" (the group col), then \"variable\", then the rest\n",
    "    if isinstance(group_cols, str):\n",
    "        group_cols_list = [group_cols]  # unify into list\n",
    "    else:\n",
    "        group_cols_list = group_cols  # already a list\n",
    "    # Reorder so that group-by columns come first, then 'variable', then the rest\n",
    "    front_cols = group_cols_list + ['variable']\n",
    "    # Build the list of remaining columns\n",
    "    rest_cols = [c for c in tmp.columns if c not in front_cols]\n",
    "    new_cols = front_cols + rest_cols\n",
    "    tmp = tmp[new_cols]\n",
    "    results_list.append(tmp)\n",
    "\n",
    "# Finally, concatenate all results\n",
    "final_summary_resp_support = pd.concat(results_list, ignore_index=True)\n",
    "final_summary_resp_support.to_csv('../output/final/summary_respiratory_support_by_device.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list = []\n",
    "group_cols = ['device_category','mode_category']\n",
    "numeric_cols = ['fio2_set','peep_set','lpm_set', 'resp_rate_set', 'resp_rate_obs']\n",
    "\n",
    "for col in numeric_cols:\n",
    "    tmp = pyCLIF.create_summary_table(\n",
    "        df=resp_support,\n",
    "        numeric_col=col,\n",
    "        group_by_cols=group_cols\n",
    "    )\n",
    "    # tmp might have columns:\n",
    "    #   ['device_category','N','missing','min','q25','median','q75','mean','max']\n",
    "    # Insert a \"variable\" column next to the group-by columns:\n",
    "    tmp['variable'] = col\n",
    "    # We want \"device_category\" (the group col), then \"variable\", then the rest\n",
    "    if isinstance(group_cols, str):\n",
    "        group_cols_list = [group_cols]  # unify into list\n",
    "    else:\n",
    "        group_cols_list = group_cols  # already a list\n",
    "    # Reorder so that group-by columns come first, then 'variable', then the rest\n",
    "    front_cols = group_cols_list + ['variable']\n",
    "    # Build the list of remaining columns\n",
    "    rest_cols = [c for c in tmp.columns if c not in front_cols]\n",
    "    new_cols = front_cols + rest_cols\n",
    "    tmp = tmp[new_cols]\n",
    "    results_list.append(tmp)\n",
    "\n",
    "# Finally, concatenate all results\n",
    "final_summary_resp_support = pd.concat(results_list, ignore_index=True)\n",
    "final_summary_resp_support.to_csv('../output/final/summary_respiratory_support_by_device_mode.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Respiratory Support Waterfall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Process with waterfall logic\n",
    "processed_resp_support = pyCLIF.process_resp_support(resp_support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Merge to get encounter_block for the cohort identified so far\n",
    "resp_stitched = processed_resp_support.merge(\n",
    "    all_ids[['hospitalization_id','encounter_block']],\n",
    "    on='hospitalization_id', how='right'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Apply outlier thresholds ===\\n\")\n",
    "\n",
    "# (Optional) If FiO2 is >1 on average => scale by /100\n",
    "fio2_mean = resp_stitched['fio2_set'].mean(skipna=True)\n",
    "# If the mean is greater than 1, divide 'fio2_set' by 100\n",
    "if fio2_mean and fio2_mean > 1.0:\n",
    "    # Only divide values greater than 1 to avoid re-dividing already correct values\n",
    "    resp_stitched.loc[resp_stitched['fio2_set'] > 1, 'fio2_set'] = \\\n",
    "        resp_stitched.loc[resp_stitched['fio2_set'] > 1, 'fio2_set'] / 100\n",
    "    print(\"Updated fio2_set to be between 0.21 and 1\")\n",
    "else:\n",
    "    print(\"FIO2_SET mean=\", fio2_mean, \"is within the required range\")\n",
    "\n",
    "pyCLIF.apply_outlier_thresholds(resp_stitched, 'fio2_set', *outlier_cfg['fio2_set'])\n",
    "pyCLIF.apply_outlier_thresholds(resp_stitched, 'peep_set', *outlier_cfg['peep_set'])\n",
    "pyCLIF.apply_outlier_thresholds(resp_stitched, 'lpm_set',  *outlier_cfg['lpm_set'])\n",
    "pyCLIF.apply_outlier_thresholds(resp_stitched, 'resp_rate_set', *outlier_cfg['resp_rate_set'])\n",
    "pyCLIF.apply_outlier_thresholds(resp_stitched, 'resp_rate_obs', *outlier_cfg['resp_rate_obs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Identify IMV\n",
    "imv_mask = resp_stitched['device_category'].str.contains(\"imv\", case=False, na=False)\n",
    "resp_stitched_imv = resp_stitched[imv_mask].copy()\n",
    "# this creates a on vent field for everytime the patient is on a vent\n",
    "resp_stitched_imv['on_vent'] = 1\n",
    "\n",
    "\n",
    "strobe_counts['C_imv_hospitalizations'] = resp_stitched_imv['hospitalization_id'].nunique()\n",
    "strobe_counts['C_imv_encounter_blocks'] = resp_stitched_imv['encounter_block'].nunique()\n",
    "\n",
    "print(f\"Total IMV respiratory support hospitalizations: {strobe_counts['C_imv_hospitalizations']}\")\n",
    "print(f\"Total IMV respiratory support encounter blocks: {strobe_counts['C_imv_encounter_blocks']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ids =  all_ids[all_ids['encounter_block'].isin(resp_stitched_imv['encounter_block'].unique())]\n",
    "all_ids = all_ids[all_ids['hospitalization_id'].isin(resp_stitched_imv['hospitalization_id'].unique())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in all_ids.columns:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(all_ids[col].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (D) Vent start and end times \n",
    "\n",
    "Calculate vent start times for the first episode of invasive mechanical intubation.   \n",
    "Limitation: the vent end time might not be associated with the same intubation episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP E: Determine Vent Start/End for Each Hospitalization and Encounter block\n",
    "\n",
    "print(\"\\n=== STEP D: Determine ventilation times (start/end) at d encounter block level ===\\n\")\n",
    "\n",
    "# at the hospitalization id level\n",
    "vent_start_end = resp_stitched_imv.groupby('hospitalization_id').agg(\n",
    "    vent_start_time=('recorded_dttm','min'),\n",
    "    vent_end_time=('recorded_dttm','max')\n",
    ").reset_index()\n",
    "\n",
    "# Exclude edge case: if start_time == end_time \n",
    "# these would otherwise have been excluded when we remove encounters on vent for less than 4 hours\n",
    "check_same_vent_start_end = vent_start_end[vent_start_end['vent_start_time'] == vent_start_end['vent_end_time']].copy()\n",
    "vent_start_end= vent_start_end[vent_start_end['vent_start_time'] != vent_start_end['vent_end_time']].copy()\n",
    "\n",
    "strobe_counts['D_hospitalizations_with_valid_vent'] = vent_start_end['hospitalization_id'].nunique()\n",
    "strobe_counts['D_hospitalizations_with_same_vent_start_end'] = check_same_vent_start_end['hospitalization_id'].nunique()\n",
    "print(f\"Unique hospitalizations with valid IMV start/end: {strobe_counts['D_hospitalizations_with_valid_vent']}\")\n",
    "\n",
    "# at the block level\n",
    "block_vent_times = resp_stitched_imv.groupby('encounter_block', dropna=True).agg(\n",
    "    block_vent_start_dttm=('recorded_dttm','min'),\n",
    "    block_vent_end_dttm=('recorded_dttm','max')\n",
    ").reset_index()\n",
    "\n",
    "# If start==end, no real vent- there was just ONE vent entry, this exclusion can count under \n",
    "block_same_vent = block_vent_times[block_vent_times['block_vent_start_dttm']==block_vent_times['block_vent_end_dttm']].copy()\n",
    "block_vent_times = block_vent_times[block_vent_times['block_vent_start_dttm']!=block_vent_times['block_vent_end_dttm']].copy()\n",
    "\n",
    "strobe_counts['D_blocks_with_valid_vent'] = block_vent_times['encounter_block'].nunique()\n",
    "strobe_counts['D_blocks_with_same_vent_start_end'] = block_same_vent['encounter_block'].nunique()\n",
    "print(f\"Unique encounter blocks with valid IMV start/end: {strobe_counts['D_blocks_with_valid_vent']}\")\n",
    "\n",
    "valid_blocks_vent = block_vent_times['encounter_block'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strobe_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter all_ids to only keep rows where encounter_block is in valid_blocks_vent\n",
    "all_ids = all_ids[all_ids['encounter_block'].isin(valid_blocks_vent)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in all_ids.columns:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(all_ids[col].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (E) Hourly Sequence \n",
    "\n",
    "This section achieves the following steps:  \n",
    "* Identifies the first and last recorded times for vitals for each encounter block\n",
    "* These times are used to generate an hourly sequence of patients hospitalization journey\n",
    "* Combines with hourly vent usage data from the respiratory support table\n",
    "* Excludes encounters on vent for less than 4 hours in the first 72 hours\n",
    "* Excludes encounters on trach in the first 72 hours \n",
    "* Creates a final dataframe with the identified cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP E: Generate Hourly Sequence & Exclude encounter blocks with <4 Vent Hours\n",
    "#  Create an hourly timeline from vent_start to last vital or outcome time for each encounter block\n",
    "# We stop operating at hospitalization id level \n",
    "\n",
    "print(\"\\n=== STEP E: Hourly sequence generation & < 4 hour vent exclusion BLOCK level===\\n\")\n",
    "\n",
    "# 1) define the 'end_time' for the sequence from vitals or outcome.\n",
    "vitals_cohort = pyCLIF.load_data('clif_vitals',\n",
    "    columns=vitals_required_columns,\n",
    "    filters={'hospitalization_id': all_ids['hospitalization_id'].unique().tolist(), \n",
    "             'vital_category': vitals_of_interest}\n",
    ")\n",
    "vitals_cohort = pyCLIF.standardize_datetime_tz(vitals_cohort, 'recorded_dttm', pyCLIF.helper['timezone'])\n",
    "# vitals_cohort['recorded_dttm'] = pd.to_datetime(vitals_cohort['recorded_dttm'])\n",
    "# vitals_cohort = pyCLIF.standardize_datetime_utc(vitals_cohort, 'recorded_dttm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_vitals = pyCLIF.create_summary_table(\n",
    "        df=vitals_cohort,\n",
    "        numeric_col='vital_value',\n",
    "        group_by_cols='vital_category'\n",
    "    )\n",
    "summary_vitals.to_csv('../output/final/summary_vitals_by_category.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge to get encounter_block on each vital\n",
    "vitals_stitched = vitals_cohort.merge(all_ids, on='hospitalization_id', how='left')\n",
    "# Group by block => find earliest & latest vital for that block\n",
    "vital_bounds_block = vitals_stitched.groupby('encounter_block', dropna=True)['recorded_dttm'].agg(['min','max']).reset_index()\n",
    "vital_bounds_block.columns = ['encounter_block','block_first_vital_dttm','block_last_vital_dttm']\n",
    "\n",
    "# 2) Merge block_vent_times with vital_bounds_block\n",
    "final_blocks = block_vent_times.merge(vital_bounds_block, on='encounter_block', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) If block_last_vital_dttm < vent_start_time => weird edge case. Ideally shouldn't happen. \n",
    "# If such bad blocks exist, check your CLIF tables bro\n",
    "bad_block = final_blocks[final_blocks['block_last_vital_dttm'] < final_blocks['block_vent_start_dttm']]\n",
    "if len(bad_block) > 0:\n",
    "    print(\"Warning: Some blocks have last vital < vent start:\\n\", bad_block)\n",
    "else:\n",
    "    print(\"There are no bad blocks! Good job CLIF-ing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Generate the hourly sequence at block level\n",
    "def generate_hourly_sequence_block(row):\n",
    "    blk  = row['encounter_block'].iloc[0]\n",
    "    start_time = row['block_vent_start_dttm'].iloc[0]\n",
    "    end_time   = row['block_last_vital_dttm'].iloc[0]\n",
    "    hourly_timestamps = pd.date_range(start=start_time, end=end_time, freq='h')\n",
    "    return pd.DataFrame({\n",
    "        'encounter_block': blk,\n",
    "        'recorded_dttm': hourly_timestamps\n",
    "    })\n",
    "\n",
    "hourly_seq_block = final_blocks.groupby('encounter_block', as_index=False).apply(generate_hourly_sequence_block)\n",
    "hourly_seq_block = hourly_seq_block.reset_index(drop=True)\n",
    "\n",
    "# hourly_seq_block['recorded_dttm'] = hourly_seq_block['recorded_dttm'].dt.tz_convert('UTC')\n",
    "hourly_seq_block['recorded_date'] = hourly_seq_block['recorded_dttm'].dt.date\n",
    "hourly_seq_block['recorded_hour'] = hourly_seq_block['recorded_dttm'].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates in hourly_seq_block- could be because of DST, so we have converted all tz to UTC\n",
    "# if duplicates exist, check why\n",
    "hourly_seq_block_check = pyCLIF.remove_duplicates(hourly_seq_block, ['encounter_block', 'recorded_date', 'recorded_hour'], \n",
    "                                                  'hour_sequence_check')\n",
    "del hourly_seq_block_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Add time_from_vent & 4-hr “cool-off”\n",
    "hourly_seq_block['time_from_vent'] = hourly_seq_block.groupby('encounter_block').cumcount()\n",
    "hourly_seq_block['time_from_vent_adjusted'] = np.where(\n",
    "    hourly_seq_block['time_from_vent'] < 4, -1, hourly_seq_block['time_from_vent'] - 4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Combine with actual vent usage by hour\n",
    "resp_stitched_imv = resp_stitched_imv[resp_stitched_imv['encounter_block'].isin(all_ids['encounter_block'])]\n",
    "hourly_vent_block = resp_stitched_imv.groupby(['encounter_block','recorded_date','recorded_hour']).agg(\n",
    "    min_fio2_set=('fio2_set','min'),\n",
    "    max_fio2_set=('fio2_set','max'),\n",
    "    min_peep_set=('peep_set','min'),\n",
    "    max_peep_set=('peep_set','max'),\n",
    "    min_lpm_set=('lpm_set', 'min'),\n",
    "    max_lpm_set=('lpm_set', 'max'),\n",
    "    min_resp_rate_obs=('resp_rate_obs', 'min'),\n",
    "    max_resp_rate_obs=('resp_rate_obs', 'max'),\n",
    "    hourly_trach=('tracheostomy', lambda x: 1 if x.max()==1 else 0), # 1 if the any value within that hour is 1\n",
    "    hourly_on_vent=('on_vent','max'),\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check- Find encounter_blocks that are in hourly_seq_block but not in hourly_vent_block and vice versa\n",
    "# This is possible when the patient is put on IMV in the ED, and dies a shortly after. \n",
    "# Still might be worth exploring the trajectory for these patients in your data\n",
    "seq_blocks = set(hourly_seq_block['encounter_block'].unique())\n",
    "vent_blocks = set(hourly_vent_block['encounter_block'].unique())\n",
    "\n",
    "blocks_in_seq_not_vent = seq_blocks - vent_blocks\n",
    "blocks_in_vent_not_seq = vent_blocks - seq_blocks\n",
    "\n",
    "print(\"Blocks in hourly_seq_block but not in hourly_vent_block:\", len(blocks_in_seq_not_vent))\n",
    "if len(blocks_in_seq_not_vent) > 0:\n",
    "    print(sorted(list(blocks_in_seq_not_vent)))\n",
    "\n",
    "print(\"\\nBlocks in hourly_vent_block but not in hourly_seq_block:\", len(blocks_in_vent_not_seq))\n",
    "if len(blocks_in_vent_not_seq) > 0:\n",
    "    print(sorted(list(blocks_in_vent_not_seq)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_block = pd.merge(\n",
    "    hourly_seq_block,\n",
    "    hourly_vent_block,\n",
    "    on=['encounter_block','recorded_date','recorded_hour'],\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of final_df_block:\", final_df_block.shape)\n",
    "print(\"\\nUnique counts:\")\n",
    "print(f\"Encounter blocks: {final_df_block['encounter_block'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Count how many vent hours per block in the first 72 hours after first intubation,\n",
    "#  Exclude <4 hours on vent in first 72 hours at block level- They cannot meaningfully be studied for early mobilization if they’re barely intubated.. including them could bias results\n",
    "first_72_hours = final_df_block[(final_df_block['time_from_vent'] >= 0) & (final_df_block['time_from_vent'] < 72)]\n",
    "vent_hours_per_block = first_72_hours.groupby('encounter_block')['hourly_on_vent'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks_under_4 = vent_hours_per_block[vent_hours_per_block < 4].index\n",
    "blocks_under_4_df = final_df_block[final_df_block['encounter_block'].isin(blocks_under_4)]\n",
    "final_df_block = final_df_block[~final_df_block['encounter_block'].isin(blocks_under_4)]\n",
    "\n",
    "strobe_counts['G_blocks_with_vent_4_or_more'] = final_df_block['encounter_block'].nunique()\n",
    "strobe_counts['G_blocks_with_vent_less_than_4'] = len(blocks_under_4)\n",
    "print(f\"Unique encounter blocks with valid IMV start/end: {strobe_counts['G_blocks_with_vent_4_or_more']}\")\n",
    "print(f\"Excluded {len(blocks_under_4)} encounter blocks with <4 vent hours in first 72 hours of intubation.\\n\")\n",
    "\n",
    "# 8) Exclude blocks with early trach in first 72\n",
    "trach_flag_block = first_72_hours.groupby('encounter_block')['hourly_trach'].max()\n",
    "blocks_with_trach = trach_flag_block[trach_flag_block==1].index\n",
    "\n",
    "final_df_block = final_df_block[~final_df_block['encounter_block'].isin(blocks_with_trach)]\n",
    "print(f\"Excluded {len(blocks_with_trach)} encounter blocks with trach in first 72 hours of intubation.\\n\")\n",
    "\n",
    "strobe_counts['G_final_blocks_without_trach'] = final_df_block['encounter_block'].nunique()\n",
    "strobe_counts['G_final_blocks_with_trach'] = len(blocks_with_trach)\n",
    "print(f\"Final cohort size (unique blocks) after all exclusions: {strobe_counts['G_final_blocks_without_trach']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strobe_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.merge(\n",
    "    final_df_block,\n",
    "    all_ids,\n",
    "    on='encounter_block',\n",
    "    how='left'\n",
    ").reindex(columns=[\n",
    "    'patient_id', 'hospitalization_id', 'encounter_block', \n",
    "    'recorded_dttm', 'recorded_date', 'recorded_hour',\n",
    "    'time_from_vent', 'time_from_vent_adjusted',\n",
    "    'min_fio2_set', 'max_fio2_set', 'min_peep_set', 'max_peep_set',\n",
    "    'min_lpm_set', 'max_lpm_set', 'min_resp_rate_obs', 'max_resp_rate_obs',\n",
    "    'hourly_trach', 'hourly_on_vent'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "key_cols = ['encounter_block', 'recorded_date', 'recorded_hour']\n",
    "duplicates = final_df.duplicated(subset=key_cols).sum()\n",
    "print(f\"Number of duplicate rows: {duplicates}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ids = all_ids[all_ids['encounter_block'].isin(final_df['encounter_block'])]\n",
    "all_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in all_ids.columns:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(all_ids[col].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (E) Add final outcome dttm\n",
    "\n",
    "Calculate final outcome dttm for each encounter block using last vital recorded dttm and discharge disposition.   \n",
    "\n",
    "To get the `final_outcome_dttm`, we use the `block_last_vital_dttm`. Added a `is_dead` flag when `discharge_category` == `Expired`. \n",
    "We initially tried using `death_dttm`, then `discharge_dttm`, and finally `block_last_vital_dttm` to get to `final_outcome_dttm`, but that was not always accurate because the `death_dttm`, then `discharge_dttm` are not always accurate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Merge `all_ids` (patient_id, hospitalization_id, encounter_block)\n",
    "#    with final blocks DataFrame (which has block-level columns -  \tblock_vent_start_dttm,\tblock_vent_end_dttm block_first_vital_dttm, block_last_vital_dttm, ).\n",
    "all_ids_w_outcome = pd.merge(\n",
    "    all_ids,\n",
    "    final_blocks,           \n",
    "    on='encounter_block',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# 2) Merge with hospitalization table to get discharge_dttm optional discharge_disposition\n",
    "all_ids_w_outcome = pd.merge(\n",
    "    all_ids_w_outcome,\n",
    "    hospitalization[['hospitalization_id', 'discharge_dttm', 'discharge_category']],\n",
    "    on='hospitalization_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# 3) Merge with patient table to get death_dttm\n",
    "all_ids_w_outcome = pd.merge(\n",
    "    all_ids_w_outcome,\n",
    "    patient[['patient_id','death_dttm']],\n",
    "    on='patient_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# 4) Define final_outcome_dttm\n",
    "#    Logic: if death_dttm is not null, that overrides discharge_dttm\n",
    "#    else use discharge_dttm, else fallback to block_last_vital_dttm\n",
    "# all_ids_w_outcome['final_outcome_dttm'] = np.where(\n",
    "#     all_ids_w_outcome['death_dttm'].notna(),\n",
    "#     all_ids_w_outcome['death_dttm'],\n",
    "#     all_ids_w_outcome['discharge_dttm']\n",
    "# )\n",
    "\n",
    "# If both death_dttm & discharge_dttm are missing, fall back on block_last_vital_dttm:\n",
    "# mask_missing = all_ids_w_outcome['final_outcome_dttm'].isna()\n",
    "# all_ids_w_outcome.loc[mask_missing, 'final_outcome_dttm'] = all_ids_w_outcome.loc[mask_missing, 'block_last_vital_dttm']\n",
    "\n",
    "# New logic: Use block_last_vital_dttm as the final_outcome_dttm\n",
    "all_ids_w_outcome['final_outcome_dttm'] = all_ids_w_outcome['block_last_vital_dttm']\n",
    "\n",
    "# Add is_dead flag based on discharge_category\n",
    "all_ids_w_outcome['is_dead'] = (all_ids_w_outcome['discharge_category'].str.lower() == 'expired').astype(int)\n",
    "\n",
    "all_ids_w_outcome.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SANITY CHECK- check blocks where death_dttm is before block_last_vital_dttm\n",
    "mask_death_before_vitals = (all_ids_w_outcome['death_dttm'].notna()) & (all_ids_w_outcome['death_dttm'] < all_ids_w_outcome['block_last_vital_dttm'])\n",
    "print(\"Number of blocks where death_dttm is before block_last_vital_dttm:\", mask_death_before_vitals.sum())\n",
    "print(\"\\nExample cases:\")\n",
    "check = all_ids_w_outcome[mask_death_before_vitals][['patient_id', 'hospitalization_id', 'encounter_block', 'death_dttm', 'block_last_vital_dttm', 'final_outcome_dttm']]\n",
    "\n",
    "# Calculate the difference in hours between death_dttm and block_last_vital_dttm\n",
    "check['diff_hour'] = (check['death_dttm'] - check['block_last_vital_dttm']).dt.total_seconds() / 3600\n",
    "\n",
    "check.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in all_ids_w_outcome.columns[:3]:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(all_ids_w_outcome[col].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hourly Vitals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get height , weight to calculate bmi\n",
    "# Filter vitals to include only height and weight\n",
    "vitals_bmi = vitals_stitched[\n",
    "    (vitals_stitched['vital_category'].isin(['weight_kg', 'height_cm'])) &\n",
    "    (vitals_stitched['encounter_block'].isin(all_ids_w_outcome['encounter_block']))\n",
    "].copy()\n",
    "\n",
    "# Remove outliers\n",
    "# Extract the min/max from the config\n",
    "min_height, max_height = outlier_cfg['height_cm']\n",
    "min_weight, max_weight = outlier_cfg['weight_kg']\n",
    "\n",
    "# For height rows: set out-of-range to NaN\n",
    "is_height = vitals_bmi['vital_category'] == 'height_cm'\n",
    "height_mask_low  = is_height & (vitals_bmi['vital_value'] < min_height)\n",
    "height_mask_high = is_height & (vitals_bmi['vital_value'] > max_height)\n",
    "vitals_bmi.loc[height_mask_low | height_mask_high, 'vital_value'] = np.nan\n",
    "\n",
    "# For weight rows: set out-of-range to NaN\n",
    "is_weight = vitals_bmi['vital_category'] == 'weight_kg'\n",
    "weight_mask_low  = is_weight & (vitals_bmi['vital_value'] < min_weight)\n",
    "weight_mask_high = is_weight & (vitals_bmi['vital_value'] > max_weight)\n",
    "vitals_bmi.loc[weight_mask_low | weight_mask_high, 'vital_value'] = np.nan\n",
    "\n",
    "# Merge with vent_start_end to get ventilation start time\n",
    "vitals_bmi = vitals_bmi.merge(\n",
    "    block_vent_times[['encounter_block','block_vent_start_dttm']],\n",
    "    on='encounter_block',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Calculate time difference between recorded_dttm and vent_start_time\n",
    "vitals_bmi['time_diff'] = (vitals_bmi['recorded_dttm'] - vitals_bmi['block_vent_start_dttm']).dt.total_seconds() / 3600  # in hours\n",
    "\n",
    "# Define whether measurement is before or after vent_start_time\n",
    "vitals_bmi['before_vent_start'] = (vitals_bmi['time_diff'] <= 0).astype(int)\n",
    "\n",
    "# Calculate absolute time difference\n",
    "vitals_bmi['abs_time_diff'] = vitals_bmi['time_diff'].abs()\n",
    "\n",
    "# Sort data to prioritize measurements before vent start and closest in time\n",
    "vitals_bmi = vitals_bmi.sort_values(['encounter_block', 'vital_category', 'before_vent_start', 'abs_time_diff'], \n",
    "                                    ascending=[True, True, False, True])\n",
    "\n",
    "# Drop duplicates to keep the closest measurement for each vital_category per encounter block\n",
    "vitals_bmi = vitals_bmi.drop_duplicates(subset=['encounter_block', 'vital_category'], keep='first')\n",
    "\n",
    "# Pivot to get height and weight per encounter block\n",
    "vitals_bmi_pivot = vitals_bmi.pivot(index='encounter_block', \n",
    "                                    columns='vital_category', \n",
    "                                    values='vital_value'\n",
    "                                    ).reset_index()\n",
    "\n",
    "# Calculate BMI\n",
    "vitals_bmi_pivot['bmi'] = vitals_bmi_pivot['weight_kg'] / ((vitals_bmi_pivot['height_cm'] / 100) ** 2)\n",
    "\n",
    "print(f\"Number of unique encounter blocks with BMI data: {vitals_bmi_pivot['encounter_block'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract 'recorded_date' and 'recorded_hour' from recorded_dttm\n",
    "vitals_stitched['recorded_date'] = vitals_stitched['recorded_dttm'].dt.date\n",
    "vitals_stitched['recorded_hour'] = vitals_stitched['recorded_dttm'].dt.hour\n",
    "print(f\"Number of unique encounter blocks BEFORE: {vitals_stitched['encounter_block'].nunique()}\")\n",
    "vitals_stitched = vitals_stitched[vitals_stitched['encounter_block'].isin(all_ids_w_outcome['encounter_block'])]\n",
    "print(f\"Number of unique encounter blocks AFTER: {vitals_stitched['encounter_block'].nunique()}\")\n",
    "strobe_counts['final_blocks_with_vitals'] = vitals_stitched['encounter_block'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MAP if it doesn't exist\n",
    "# Check if 'map' exists\n",
    "if 'map' not in vitals_stitched['vital_category'].unique():\n",
    "    print(\"map is not present, so we'll calculate it...\")\n",
    "    # 1) Filter for sbp & dbp\n",
    "    sbp_dbp = vitals_stitched[vitals_stitched['vital_category'].isin(['sbp','dbp'])].copy()\n",
    "    \n",
    "    # 2) Pivot at the encounter_block + recorded_dttm level\n",
    "    sbp_dbp_pivot = sbp_dbp.pivot_table(\n",
    "        index=['encounter_block','recorded_dttm'],\n",
    "        columns='vital_category',\n",
    "        values='vital_value'\n",
    "    ).reset_index()\n",
    "    \n",
    "    # 3) Drop any row missing sbp or dbp\n",
    "    sbp_dbp_pivot = sbp_dbp_pivot.dropna(subset=['sbp','dbp'])\n",
    "    \n",
    "    # 4) Calculate MAP\n",
    "    sbp_dbp_pivot['map'] = (sbp_dbp_pivot['sbp'] + 2*sbp_dbp_pivot['dbp']) / 3\n",
    "    \n",
    "    # 5) Build a DataFrame for map\n",
    "    map_vitals = sbp_dbp_pivot[['encounter_block','recorded_dttm','map']].copy()\n",
    "    map_vitals['vital_category'] = 'map'\n",
    "    map_vitals['vital_value'] = map_vitals['map']\n",
    "    \n",
    "    # Also add recorded_date/hour\n",
    "    map_vitals['recorded_date'] = map_vitals['recorded_dttm'].dt.date\n",
    "    map_vitals['recorded_hour'] = map_vitals['recorded_dttm'].dt.hour\n",
    "    \n",
    "    # Keep only the needed columns\n",
    "    map_vitals = map_vitals[[\n",
    "        'encounter_block','recorded_dttm','recorded_date','recorded_hour','vital_category','vital_value'\n",
    "    ]]\n",
    "    \n",
    "    # 6) Append 'map' to the main vitals_stitched DataFrame\n",
    "    vitals_stitched = pd.concat([vitals_stitched, map_vitals], ignore_index=True)\n",
    "    print(\"...map was calculated and appended to vitals_stitched.\")\n",
    "else:\n",
    "    print(\"Map exists in your CLIF database\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute min/max vitals  at the BLOCK level\n",
    "# group by encounter_block + recorded_date + recorded_hour + vital_category\n",
    "vitals_min_max = vitals_stitched.groupby(\n",
    "    ['encounter_block','recorded_date','recorded_hour','vital_category']\n",
    ").agg(\n",
    "    min_val=('vital_value','min'),\n",
    "    max_val=('vital_value','max')\n",
    ").reset_index()\n",
    "\n",
    "# 3) Pivot so each row is unique by (encounter_block, recorded_date, recorded_hour),\n",
    "#    with columns like min_sbp, max_sbp, min_map, max_map, etc.\n",
    "vitals_pivot = vitals_min_max.pivot_table(\n",
    "    index=['encounter_block','recorded_date','recorded_hour'],\n",
    "    columns='vital_category',\n",
    "    values=['min_val','max_val']\n",
    ").reset_index()\n",
    "\n",
    "# Flatten the multi-level columns\n",
    "vitals_pivot.columns = [\n",
    "    '_'.join(col).rstrip('_') if isinstance(col, tuple) else col \n",
    "    for col in vitals_pivot.columns\n",
    "]\n",
    "\n",
    "#  Rename columns for clarity\n",
    "rename_dict = {}\n",
    "for c in vitals_pivot.columns:\n",
    "    if c.startswith('min_val_'):\n",
    "        rename_dict[c] = c.replace('min_val_','min_')\n",
    "    elif c.startswith('max_val_'):\n",
    "        rename_dict[c] = c.replace('max_val_','max_')\n",
    "\n",
    "vitals_pivot = vitals_pivot.rename(columns=rename_dict)\n",
    "\n",
    "# The resulting columns might look like:\n",
    "# ['encounter_block','recorded_date','recorded_hour',\n",
    "#  'min_sbp','max_sbp','min_map','max_map','min_resp_rate','max_resp_rate', etc.]\n",
    "\n",
    "print(\"Finished creating block-level min/max vitals pivot:\")\n",
    "vitals_pivot.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## confirm duplicates don't exist\n",
    "checkpoint_vitals = pyCLIF.remove_duplicates(final_df, [\n",
    "    'encounter_block','recorded_date', 'recorded_hour'\n",
    "], 'final_df')\n",
    "del checkpoint_vitals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge vitals with final_df\n",
    "final_df = pd.merge(final_df, vitals_pivot, on=['encounter_block', 'recorded_date', 'recorded_hour'], \n",
    "                   how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hourly Meds\n",
    "\n",
    "* Handle med dose unit conversion for all vasoactives\n",
    "* Calculate NE equivalent levels using \"norepinephrine\", \"epinephrine\", \"phenylephrine\", \"vasopressin\", \"dopamine\",  \"angiotensin\"\n",
    "* Create flags for \"nicardipine\", \"nitroprusside\", \"clevidipine\" for the red criteria under consensus criteria\n",
    "* Identify encounters on paralytics - cisatracurium, vecuronium, rocuronium- and create flags for each of these paralytic meds. These patients will not be considered eligible for mobilization during the hour they were receiving paralytic medication. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import clif continuous meds for the cohort on vent during the required time period\n",
    "meds_filters = {\n",
    "    'hospitalization_id': all_ids['hospitalization_id'].unique().tolist(),\n",
    "    'med_category': meds_of_interest\n",
    "}\n",
    "meds = pyCLIF.load_data('clif_medication_admin_continuous', columns=meds_required_columns, filters=meds_filters)\n",
    "meds = meds.merge(all_ids, on='hospitalization_id', how='left')\n",
    "print(\"Unique encounters in meds\", pyCLIF.count_unique_encounters(meds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure correct format\n",
    "meds['hospitalization_id']= meds['hospitalization_id'].astype(str)\n",
    "meds['med_dose_unit'] = meds['med_dose_unit'].str.lower()\n",
    "meds = pyCLIF.standardize_datetime_tz(meds, 'admin_dttm', pyCLIF.helper['timezone'])\n",
    "# meds['admin_dttm'] = pd.to_datetime(meds['admin_dttm'])\n",
    "meds = pyCLIF.standardize_datetime_utc(meds, 'admin_dttm')\n",
    "meds['med_dose'] = pd.to_numeric(meds['med_dose'], errors='coerce')\n",
    "# Create 'date' and 'hour_of_day' columns\n",
    "meds['recorded_date'] = meds['admin_dttm'].dt.date\n",
    "meds['recorded_hour'] = meds['admin_dttm'].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary table for each med_category\n",
    "summary_meds= meds.groupby('med_category').agg(\n",
    "    total_N=('med_category', 'size'),\n",
    "    min=('med_dose', 'min'),\n",
    "    max=('med_dose', 'max'),\n",
    "    first_quantile=('med_dose', lambda x: x.quantile(0.25)),\n",
    "    second_quantile=('med_dose', lambda x: x.quantile(0.5)),\n",
    "    third_quantile=('med_dose', lambda x: x.quantile(0.75)),\n",
    "    missing_values=('med_dose', lambda x: x.isna().sum())\n",
    ").reset_index()\n",
    "\n",
    "summary_meds.to_csv('../output/final/summary_meds_by_category.csv', index=False)\n",
    "## check the distrbituon of required continuous meds\n",
    "summary_meds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary table for each med_category and med_dose_unit combination\n",
    "summary_meds_cat_dose= meds.groupby(['med_category', 'med_dose_unit']).agg(\n",
    "    total_N=('med_category', 'size'),\n",
    "    min=('med_dose', 'min'),\n",
    "    max=('med_dose', 'max'),\n",
    "    first_quantile=('med_dose', lambda x: x.quantile(0.25)),\n",
    "    second_quantile=('med_dose', lambda x: x.quantile(0.5)),\n",
    "    third_quantile=('med_dose', lambda x: x.quantile(0.75)),\n",
    "    missing_values=('med_dose', lambda x: x.isna().sum())\n",
    ").reset_index()\n",
    "summary_meds_cat_dose.to_csv('../output/final/summary_meds_by_category_dose_units.csv', index=False)\n",
    "## check the distrbituon of required continuous meds\n",
    "summary_meds_cat_dose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by med_category and med_dose_unit\n",
    "grouped_data = meds.groupby(['med_category', 'med_dose_unit'])\n",
    "\n",
    "# Dynamically determine the number of required subplots\n",
    "n_plots = len(grouped_data.groups.keys())\n",
    "n_cols = 4\n",
    "n_rows = (n_plots + n_cols - 1) // n_cols  # Round up to determine rows\n",
    "\n",
    "fig, axs = plt.subplots(n_rows, n_cols, figsize=(20, n_rows * 5))\n",
    "\n",
    "# Flatten the axs array for easier indexing\n",
    "axs = axs.flatten()\n",
    "\n",
    "# Loop through each group and plot the histogram\n",
    "for i, ((med_category, med_dose_unit), group) in enumerate(grouped_data):\n",
    "    ax = axs[i]\n",
    "    ax.hist(group['med_dose'], bins=20, alpha=0.7, label=f\"N = {len(group)}\")\n",
    "    ax.set_title(f\"{med_category} - {med_dose_unit}\")\n",
    "    ax.set_xlabel('Med Dose')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "# Hide any unused axes\n",
    "for j in range(i + 1, len(axs)):\n",
    "    axs[j].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../output/final/graphs/meds_histograms.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define medications and their unit conversion information\n",
    "meds_list = [\n",
    "    \"norepinephrine\", \"epinephrine\", \"phenylephrine\",\n",
    "    \"vasopressin\", \"dopamine\", \"angiotensin\", \"metaraminol\"\n",
    "]\n",
    "\n",
    "med_unit_info = {\n",
    "    'norepinephrine': {\n",
    "        'required_unit': 'mcg/kg/min',\n",
    "        'acceptable_units': ['mcg/kg/min', 'mcg/kg/hr', 'mg/kg/hr', 'mcg/min', 'mg/hr'],\n",
    "    },\n",
    "    'epinephrine': {\n",
    "        'required_unit': 'mcg/kg/min',\n",
    "        'acceptable_units': ['mcg/kg/min', 'mcg/kg/hr', 'mg/kg/hr', 'mcg/min', 'mg/hr'],\n",
    "    },\n",
    "    'phenylephrine': {\n",
    "        'required_unit': 'mcg/kg/min',\n",
    "        'acceptable_units': ['mcg/kg/min', 'mcg/kg/hr', 'mg/kg/hr', 'mcg/min', 'mg/hr'],\n",
    "    },\n",
    "    'dopamine': {\n",
    "        'required_unit': 'mcg/kg/min',\n",
    "        'acceptable_units': ['mcg/kg/min', 'mcg/kg/hr', 'mg/kg/hr', 'mcg/min', 'mg/hr'],\n",
    "    },\n",
    "    'metaraminol': {\n",
    "        'required_unit': 'mcg/kg/min',\n",
    "        'acceptable_units': ['mg/hr', 'mcg/min'],\n",
    "    },\n",
    "    'angiotensin': {\n",
    "        'required_unit': 'mcg/kg/min',\n",
    "        'acceptable_units': ['ng/kg/min', 'ng/kg/hr'],\n",
    "    },\n",
    "    'vasopressin': {\n",
    "        'required_unit': 'units/min',\n",
    "        'acceptable_units': ['units/min', 'units/hr', 'milliunits/min', 'milliunits/hr'],\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SANITY CHECKS- Check the med_dose_unit for each med_category in the meds table\n",
    "med_dose_unit_check = meds.groupby(['med_category', 'med_dose_unit']).size().reset_index(name='count')\n",
    "\n",
    "# Create a new column to flag invalid dose units\n",
    "def check_dose_unit(row):\n",
    "    med_category = row['med_category']\n",
    "    med_dose_unit = row['med_dose_unit']\n",
    "    # Check if med_category exists in med_unit_info\n",
    "    if med_category in med_unit_info:\n",
    "        # Check if med_dose_unit is in the acceptable units\n",
    "        if med_dose_unit in med_unit_info[med_category]['acceptable_units']:\n",
    "            return \"Valid\"\n",
    "        else:\n",
    "            return \"Not an acceptable unit\"\n",
    "    else:\n",
    "        return \"Not a vasoactive\"\n",
    "\n",
    "# Apply the function to the DataFrame\n",
    "med_dose_unit_check['unit_validity'] = med_dose_unit_check.apply(check_dose_unit, axis=1)\n",
    "\n",
    "# # Optional: Filter for invalid units\n",
    "invalid_units = med_dose_unit_check[med_dose_unit_check['unit_validity'] == 'Not an acceptable unit']\n",
    "print(\"Invalid units. These will be dropped:\\n\")\n",
    "print(invalid_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Norepinephrine equivalent calculation\n",
    "# Goradia S, Sardaneh AA, Narayan SW, Penm J, Patanwala AE. Vasopressor dose equivalence: \n",
    "# A scoping review and suggested formula. J Crit Care. 2021 Feb;61:233-240. doi: 10.1016/j.jcrc.2020.11.002. Epub 2020 Nov 14. PMID: 33220576.\n",
    "\n",
    "meds_list = [\n",
    "    \"norepinephrine\", \"epinephrine\", \"phenylephrine\", \n",
    "    \"vasopressin\", \"dopamine\",  \n",
    "    \"angiotensin\"\n",
    "]\n",
    "\n",
    "# Function to check if 'med_dose_unit' contains '/hr' or '/min'\n",
    "def has_per_hour_or_min(unit):\n",
    "    if pd.isnull(unit):\n",
    "        return False\n",
    "    unit = unit.lower()\n",
    "    return '/hr' in unit or '/min' in unit\n",
    "\n",
    "# Filter meds to include only rows with '/hr' or '/min' in 'med_dose_unit'\n",
    "meds_filtered = meds[meds['med_dose_unit'].apply(has_per_hour_or_min)].copy()\n",
    "\n",
    "ne_df = meds_filtered[meds_filtered['med_category'].isin(meds_list)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **2. Convert Medication Doses to Required Units**\n",
    "### Function to get conversion factor for each medication\n",
    "def get_conversion_factor(med_category, med_dose_unit, weight_kg):\n",
    "    med_info = med_unit_info.get(med_category, None)\n",
    "    if not med_info:\n",
    "        # Medication not in the list\n",
    "        return None\n",
    "    required_unit = med_info['required_unit']\n",
    "    acceptable_units = med_info['acceptable_units']\n",
    "    med_dose_unit = med_dose_unit.lower()\n",
    "    if med_category in ['norepinephrine', 'epinephrine', 'phenylephrine', 'dopamine', 'metaraminol']:\n",
    "        # Required unit: mcg/kg/min\n",
    "        if med_dose_unit == 'mcg/kg/min':\n",
    "            factor = 1.0\n",
    "        elif med_dose_unit == 'mcg/kg/hr':\n",
    "            factor = 1 / 60\n",
    "        elif med_dose_unit == 'mg/kg/hr':\n",
    "            factor = 1000 / 60\n",
    "        elif med_dose_unit == 'mcg/min':\n",
    "            factor = 1 / weight_kg\n",
    "        elif med_dose_unit == 'mg/hr':\n",
    "            factor = 1000 / 60 / weight_kg\n",
    "        else:\n",
    "            return None\n",
    "    elif med_category == 'angiotensin':\n",
    "        # Required unit: mcg/kg/min\n",
    "        if med_dose_unit == 'ng/kg/min':\n",
    "            factor = 1 / 1000\n",
    "        elif med_dose_unit == 'ng/kg/hr':\n",
    "            factor = 1 / 1000 / 60\n",
    "        else:\n",
    "            return None\n",
    "    elif med_category == 'vasopressin':\n",
    "        # Required unit: units/min\n",
    "        if med_dose_unit == 'units/min':\n",
    "            factor = 1.0\n",
    "        elif med_dose_unit == 'units/hr':\n",
    "            factor = 1 / 60\n",
    "        elif med_dose_unit == 'milliunits/min':\n",
    "            factor = 1 / 1000\n",
    "        elif med_dose_unit == 'milliunits/hr':\n",
    "            factor = 1 / 1000 / 60\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "    return factor\n",
    "\n",
    "# Merge weight_kg into meds_filtered (assuming 'vitals_bmi_pivot' is available)\n",
    "meds_filtered = meds_filtered.merge(vitals_bmi_pivot[['encounter_block', 'weight_kg']], on='encounter_block', how='left')\n",
    "\n",
    "# Remove rows with missing weight_kg\n",
    "meds_filtered = meds_filtered[~meds_filtered['weight_kg'].isnull()].copy()\n",
    "\n",
    "# Function to convert doses\n",
    "def convert_dose(row):\n",
    "    med_category = row['med_category']\n",
    "    med_dose = row['med_dose']\n",
    "    med_dose_unit = row['med_dose_unit']\n",
    "    weight_kg = row['weight_kg']\n",
    "    factor = get_conversion_factor(med_category, med_dose_unit, weight_kg)\n",
    "    if factor is None:\n",
    "        return np.nan\n",
    "    return med_dose * factor\n",
    "\n",
    "# Apply the conversion to get 'med_dose_converted'\n",
    "meds_filtered['med_dose_converted'] = meds_filtered.apply(convert_dose, axis=1)\n",
    "\n",
    "# Drop rows with NaN in 'med_dose_converted' (unrecognized units)\n",
    "meds_filtered = meds_filtered[~meds_filtered['med_dose_converted'].isnull()].copy()\n",
    "\n",
    "# Filter doses within acceptable ranges\n",
    "meds_filtered = meds_filtered[meds_filtered.apply(pyCLIF.is_dose_within_range, axis=1, args=(outlier_cfg,))].copy()\n",
    "\n",
    "# **4. Flag Medications Not in the Dataset**\n",
    "\n",
    "for med in meds_list:\n",
    "    if med not in meds_filtered['med_category'].unique():\n",
    "        print(f\"{med} is not in the dataset.\")\n",
    "\n",
    "# Pivot and Aggregate the Data**\n",
    "\n",
    "# Create 'recorded_date' and 'recorded_hour' columns\n",
    "meds_filtered['admin_dttm'] = pd.to_datetime(meds_filtered['admin_dttm'])\n",
    "meds_filtered['recorded_date'] = meds_filtered['admin_dttm'].dt.date\n",
    "meds_filtered['recorded_hour'] = meds_filtered['admin_dttm'].dt.hour\n",
    "\n",
    "# Group and aggregate doses\n",
    "group_cols = ['encounter_block', 'recorded_date', 'recorded_hour', 'med_category']\n",
    "dose_agg = meds_filtered.groupby(group_cols)['med_dose_converted'].agg(['min', 'max']).reset_index()\n",
    "\n",
    "# Pivot to have medications as columns\n",
    "dose_pivot_min = dose_agg.pivot_table(index=['encounter_block', 'recorded_date', 'recorded_hour'], columns='med_category', values='min').reset_index()\n",
    "dose_pivot_max = dose_agg.pivot_table(index=['encounter_block', 'recorded_date', 'recorded_hour'], columns='med_category', values='max').reset_index()\n",
    "\n",
    "# Rename columns to indicate min and max\n",
    "dose_pivot_min.columns = ['encounter_block', 'recorded_date', 'recorded_hour'] + ['min_' + col for col in dose_pivot_min.columns if col not in ['encounter_block', 'recorded_date', 'recorded_hour']]\n",
    "dose_pivot_max.columns = ['encounter_block', 'recorded_date', 'recorded_hour'] + ['max_' + col for col in dose_pivot_max.columns if col not in ['encounter_block', 'recorded_date', 'recorded_hour']]\n",
    "\n",
    "# Merge min and max DataFrames\n",
    "dose_pivot = pd.merge(dose_pivot_min, dose_pivot_max, on=['encounter_block', 'recorded_date', 'recorded_hour'], how='outer')\n",
    "\n",
    "# **6. Calculate Norepinephrine Equivalents**\n",
    "\n",
    "# Replace NaN with 0 for calculations\n",
    "dose_pivot.fillna(0, inplace=True)\n",
    "\n",
    "# Calculate NE min\n",
    "dose_pivot['ne_calc_min'] = (\n",
    "    dose_pivot.get('min_norepinephrine', 0) +\n",
    "    dose_pivot.get('min_epinephrine', 0) +\n",
    "    dose_pivot.get('min_phenylephrine', 0) / 10 +\n",
    "    dose_pivot.get('min_dopamine', 0) / 100 +\n",
    "    dose_pivot.get('min_metaraminol', 0) / 8 +\n",
    "    dose_pivot.get('min_vasopressin', 0) * 2.5 +\n",
    "    dose_pivot.get('min_angiotensin', 0) * 10\n",
    ")\n",
    "\n",
    "# Calculate NE max\n",
    "dose_pivot['ne_calc_max'] = (\n",
    "    dose_pivot.get('max_norepinephrine', 0) +\n",
    "    dose_pivot.get('max_epinephrine', 0) +\n",
    "    dose_pivot.get('max_phenylephrine', 0) / 10 +\n",
    "    dose_pivot.get('max_dopamine', 0) / 100 +\n",
    "    dose_pivot.get('max_metaraminol', 0) / 8 +\n",
    "    dose_pivot.get('max_vasopressin', 0) * 2.5 +\n",
    "    dose_pivot.get('max_angiotensin', 0) * 10\n",
    ")\n",
    "\n",
    "# **7. Prepare the Final Dataset**\n",
    "# Keep only the required columns\n",
    "ne_calc_df = dose_pivot[['encounter_block', 'recorded_date', \n",
    "                         'recorded_hour', \n",
    "                         'ne_calc_min', 'ne_calc_max']].drop_duplicates(subset=['encounter_block', 'recorded_date', 'recorded_hour'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "strobe_counts['final_blocks_with_norepi_eq'] = ne_calc_df['encounter_block'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate lowest norepinephrine dose in the past 6 hours for each encounter_block\n",
    "# Ensure the DataFrame is sorted by 'hospitalization_id' and 'time_from_vent'\n",
    "ne_calc_df = ne_calc_df.sort_values(by=['encounter_block', 'recorded_date', 'recorded_hour'])\n",
    "\n",
    "# Define a function to calculate the rolling minimum and fill NaN values\n",
    "def rolling_min(group):\n",
    "    group['min_ne_dose_last_6_hours'] = group['ne_calc_min'].rolling(window=6, min_periods=1).min()\n",
    "    group['min_ne_dose_last_6_hours'] = group['min_ne_dose_last_6_hours'].ffill().fillna(0)  \n",
    "    return group\n",
    "\n",
    "# Apply the rolling minimum function to each group\n",
    "ne_calc_df = ne_calc_df.groupby('encounter_block').apply(rolling_min).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## confirm duplicates don't exist\n",
    "checkpoint_meds = pyCLIF.remove_duplicates(ne_calc_df, [\n",
    "    'encounter_block','recorded_date', 'recorded_hour'\n",
    "], 'final_df')\n",
    "del checkpoint_meds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.merge(final_df, \n",
    "                    ne_calc_df, \n",
    "                    on=['encounter_block', 'recorded_date', 'recorded_hour'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_meds_list = [\n",
    "    \"nicardipine\", \"nitroprusside\", \"clevidipine\"\n",
    "]\n",
    "\n",
    "# Filter meds_filtered for the medications in red_meds_list\n",
    "red_meds_df = meds[meds['med_category'].isin(red_meds_list)].copy()\n",
    "\n",
    "# Create a flag for each medication in red_meds_list\n",
    "for med in red_meds_list:\n",
    "    # Create a flag that is 1 if the medication was administered in that hour, 0 otherwise\n",
    "    red_meds_df[med + '_flag'] = np.where(red_meds_df['med_category'] == med, 1, 0).astype(int)\n",
    "\n",
    "# Aggregate to get the maximum value for each flag (per hospitalization_id, recorded_date, recorded_hour)\n",
    "# This ensures that if the medication was administered even once in the hour, the flag is 1\n",
    "red_meds_flags = red_meds_df.groupby(['encounter_block', 'recorded_date', 'recorded_hour']).agg(\n",
    "    {med + '_flag': 'max' for med in red_meds_list}\n",
    ").reset_index()\n",
    "\n",
    "#  combine all flags into a single 'red_meds_flag', you can do so like this:\n",
    "red_meds_flags['red_meds_flag'] = red_meds_flags[[med + '_flag' for med in red_meds_list]].max(axis=1)\n",
    "\n",
    "# Select the relevant columns\n",
    "red_meds_flags_final = red_meds_flags[[\n",
    "    'encounter_block', 'recorded_date', 'recorded_hour',\n",
    "    'nicardipine_flag', 'nitroprusside_flag',\n",
    "    'clevidipine_flag', 'red_meds_flag'\n",
    "]].drop_duplicates(subset=['encounter_block', 'recorded_date', 'recorded_hour'])\n",
    "\n",
    "red_meds_flags_final['nicardipine_flag'] = red_meds_flags_final['nicardipine_flag'].astype(int)\n",
    "red_meds_flags_final['nitroprusside_flag'] = red_meds_flags_final['nitroprusside_flag'].astype(int)\n",
    "red_meds_flags_final['clevidipine_flag'] = red_meds_flags_final['clevidipine_flag'].astype(int)\n",
    "red_meds_flags_final['red_meds_flag'] = red_meds_flags_final['red_meds_flag'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "strobe_counts['final_blocks_with_red_meds'] = red_meds_flags_final['encounter_block'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## confirm duplicates don't exist\n",
    "checkpoint_red_meds = pyCLIF.remove_duplicates(red_meds_flags_final, [\n",
    "    'encounter_block','recorded_date', 'recorded_hour'\n",
    "], 'final_df')\n",
    "del checkpoint_red_meds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.merge(final_df, \n",
    "                    red_meds_flags_final, \n",
    "                    on=['encounter_block', 'recorded_date', 'recorded_hour'], \n",
    "                    how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "paralytics_list = [\n",
    "    \"cisatracurium\", \"vecuronium\", \"rocuronium\" \n",
    "]\n",
    "\n",
    "# Filter meds_filtered for the medications in paralytics_list\n",
    "paralytics_df = meds[meds['med_category'].isin(paralytics_list)].copy()\n",
    "\n",
    "# Create a flag for each medication in paralytics_list\n",
    "for med in paralytics_list:\n",
    "    # Create a flag that is 1 if the medication was administered in that hour, 0 otherwise\n",
    "    paralytics_df[med + '_flag'] = np.where(paralytics_df['med_category'] == med, 1, 0).astype(int)\n",
    "\n",
    "# Aggregate to get the maximum value for each flag (per hospitalization_id, recorded_date, recorded_hour)\n",
    "# This ensures that if the medication was administered even once in the hour, the flag is 1\n",
    "paralytics_flags = paralytics_df.groupby(['encounter_block', 'recorded_date', 'recorded_hour']).agg(\n",
    "    {med + '_flag': 'max' for med in paralytics_list}\n",
    ").reset_index()\n",
    "\n",
    "#  combine all flags into a single 'paralytics_flag', you can do so like this:\n",
    "paralytics_flags['paralytics_flag'] = paralytics_flags[[med + '_flag' for med in paralytics_list]].max(axis=1)\n",
    "\n",
    "# Select the relevant columns\n",
    "paralytics_flags_final = paralytics_flags[[\n",
    "    'encounter_block', 'recorded_date', 'recorded_hour',\n",
    "    'cisatracurium_flag', 'vecuronium_flag',\n",
    "    'rocuronium_flag', 'paralytics_flag'\n",
    "]].drop_duplicates(subset=['encounter_block', 'recorded_date', 'recorded_hour'])\n",
    "\n",
    "paralytics_flags_final['cisatracurium_flag'] = paralytics_flags_final['cisatracurium_flag'].astype(int)\n",
    "paralytics_flags_final['vecuronium_flag'] = paralytics_flags_final['vecuronium_flag'].astype(int)\n",
    "paralytics_flags_final['rocuronium_flag'] = paralytics_flags_final['rocuronium_flag'].astype(int)\n",
    "paralytics_flags_final['paralytics_flag'] = paralytics_flags_final['paralytics_flag'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "strobe_counts['final_blocks_with_paralytics'] = paralytics_flags_final['encounter_block'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## confirm duplicates don't exist\n",
    "checkpoint_paralytics_meds = pyCLIF.remove_duplicates(paralytics_flags_final, [\n",
    "    'encounter_block','recorded_date', 'recorded_hour'\n",
    "], 'final_df')\n",
    "del checkpoint_paralytics_meds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.merge(final_df, \n",
    "                    paralytics_flags_final, \n",
    "                    on=['encounter_block', 'recorded_date', 'recorded_hour'], \n",
    "                    how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hourly Lab\n",
    "\n",
    "Get most recent lactate defined as closest lab result time to the start of first intubation event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import clif continuous meds and clif labs table for the cohort on vent during the required time period\n",
    "labs_filters = {\n",
    "    'hospitalization_id': all_ids['hospitalization_id'].unique().tolist(),\n",
    "    'lab_category': labs_of_interest\n",
    "}\n",
    "labs = pyCLIF.load_data('clif_labs', columns=labs_required_columns, filters=labs_filters)\n",
    "print(\"unique encounters in labs\", pyCLIF.count_unique_encounters(labs))\n",
    "labs['hospitalization_id']= labs['hospitalization_id'].astype(str)\n",
    "labs = labs.merge(all_ids, on='hospitalization_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "strobe_counts['final_blocks_with_lactate_lab'] = labs['encounter_block'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "labs = pyCLIF.standardize_datetime_tz(labs, 'lab_result_dttm', pyCLIF.helper['timezone'])\n",
    "# labs['lab_result_dttm'] = pd.to_datetime(labs['lab_result_dttm'])\n",
    "labs['recorded_hour'] = labs['lab_result_dttm'].dt.hour\n",
    "labs['recorded_date'] = labs['lab_result_dttm'].dt.date\n",
    "\n",
    "lactate_df = pd.merge(labs, block_vent_times, on='encounter_block', how='left')\n",
    "lactate_df['time_since_vent_start_hours'] = (\n",
    "    (lactate_df['lab_result_dttm'] - lactate_df['block_vent_start_dttm']).dt.total_seconds() / 3600\n",
    ")\n",
    "\n",
    "# Calculate the absolute time difference between lab_result_dttm and vent_start_time in hours\n",
    "lactate_df['time_diff_hours'] = abs((lactate_df['lab_result_dttm'] - lactate_df['block_vent_start_dttm']).dt.total_seconds() / 3600)\n",
    "\n",
    "# Filter for observations within the first 72 hours since vent_start_time\n",
    "lactate_df = lactate_df[(lactate_df['time_since_vent_start_hours'] >= 0) & \n",
    "                        (lactate_df['time_since_vent_start_hours'] <= 72)]\n",
    "\n",
    "# Sort by encounter_block, recorded_hour, and time_diff_hours to find the closest measurement to vent_start_time\n",
    "lactate_df = lactate_df.sort_values(by=['encounter_block', 'recorded_date', 'recorded_hour', 'time_diff_hours'])\n",
    "\n",
    "# Group by encounter_block and recorded_hour, and get the first row in each group (which is the closest measurement)\n",
    "# closest lactate measurement is defined as closest to the vent_start_time in that hour. \n",
    "closest_lactate_df = lactate_df.groupby(['encounter_block', 'recorded_date','recorded_hour']).first().reset_index()\n",
    "\n",
    "labs_final = closest_lactate_df[['encounter_block', 'recorded_date', 'recorded_hour', 'lab_value_numeric']].copy()\n",
    "\n",
    "# Rename the 'lab_value_numeric' column to 'lactate'\n",
    "labs_final = labs_final.rename(columns={'lab_value_numeric': 'lactate'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_labs= pyCLIF.remove_duplicates(final_df, [\n",
    "    'encounter_block', 'recorded_date', 'recorded_hour'\n",
    "], 'final_df')\n",
    "del checkpoint_labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.merge(final_df, \n",
    "                    labs_final, \n",
    "                    on=['encounter_block', 'recorded_date', 'recorded_hour'], \n",
    "                   how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write analysis dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_parquet('../output/intermediate/final_df.parquet')\n",
    "final_blocks.to_parquet('../output/intermediate/final_block_vent_start_end_dttms.parquet')\n",
    "all_ids_w_outcome.to_csv('../output/intermediate/cohort_all_ids_w_outcome.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strobe_counts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".mobilization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
