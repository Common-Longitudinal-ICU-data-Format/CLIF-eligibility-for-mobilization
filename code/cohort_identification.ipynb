{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eligibility for mobilization: Cohort ID and Discretizing script\n",
    "\n",
    "This script identifies the cohort using CLIF 2.0 tables and discretizes the dataset at an hourly level\n",
    "\n",
    " \n",
    "                        ðŸš¨Code will break if the following requirements are not satisfiedðŸš¨  \n",
    "#### Requirements:\n",
    "* Required table filenames should be `clif_patient`, `clif_hospitalization`, `clif_adt`, `clif_vitals`, `clif_labs`, `clif_medication_admin_continuous`, `clif_respiratory_support`\n",
    "* Within each table, the following variables and categories are required.\n",
    "\n",
    "| Table Name | Required Variables | Required Categories |\n",
    "| --- | --- | --- |\n",
    "| **patient** | `patient_id`, `race_category`, `ethnicity_category`, `sex_category` | - |\n",
    "| **hospitalization** | `patient_id`, `hospitalization_id`, `admission_dttm`, `discharge_dttm`, `age_at_admission` | - |\n",
    "| **vitals** | `hospitalization_id`, `recorded_dttm`, `vital_category`, `vital_value` | 'heart_rate', 'resp_rate', 'sbp', 'dbp', 'map', 'spo2', 'weight_kg', 'height_cm' |\n",
    "| **labs** | `hospitalization_id`, `lab_result_dttm`, `lab_category`, `lab_value` | 'lactate' |\n",
    "| **medication_admin_continuous** | `hospitalization_id`, `admin_dttm`, `med_name`, `med_category`, `med_dose`, `med_dose_unit` | \"norepinephrine\", \"epinephrine\", \"phenylephrine\", \"vasopressin\", \"dopamine\", \"angiotensin\"(optional), \"nicardipine\", \"nitroprusside\", \"clevidipine\", \"cisatracurium\" |\n",
    "| **respiratory_support** | `hospitalization_id`, `recorded_dttm`, `device_category`, `mode_category`, `tracheostomy`, `fio2_set`, `lpm_set`, `resp_rate_set`, `peep_set`, `resp_rate_obs` | - |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install pandas numpy duckdb seaborn matplotlib plotly\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyCLIF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required columns and categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "rst_required_columns = [\n",
    "    'hospitalization_id',\n",
    "    'recorded_dttm',\n",
    "    'device_name',\n",
    "    'device_category',\n",
    "    'mode_name', \n",
    "    'mode_category',\n",
    "    'tracheostomy',\n",
    "    'fio2_set',\n",
    "    'lpm_set',\n",
    "    'resp_rate_set',\n",
    "    'peep_set',\n",
    "    'resp_rate_obs',\n",
    "    'tidal_volume_set'\n",
    "]\n",
    "\n",
    "vitals_required_columns = [\n",
    "    'hospitalization_id',\n",
    "    'recorded_dttm',\n",
    "    'vital_category',\n",
    "    'vital_value'\n",
    "]\n",
    "vitals_of_interest = ['heart_rate', 'respiratory_rate', 'sbp', 'dbp', 'map', 'spo2', 'weight_kg', 'height_cm']\n",
    "\n",
    "labs_required_columns = [\n",
    "    'hospitalization_id',\n",
    "    'lab_result_dttm',\n",
    "    'lab_category',\n",
    "    'lab_value',\n",
    "    'lab_value_numeric'\n",
    "]\n",
    "labs_of_interest = ['lactate']\n",
    "\n",
    "meds_required_columns = [\n",
    "    'hospitalization_id',\n",
    "    'admin_dttm',\n",
    "    'med_name',\n",
    "    'med_category',\n",
    "    'med_dose',\n",
    "    'med_dose_unit'\n",
    "]\n",
    "meds_of_interest = [\n",
    "    'norepinephrine', 'epinephrine', 'phenylephrine', 'vasopressin',\n",
    "    'dopamine', 'angiotensin', 'nicardipine', 'nitroprusside',\n",
    "    'clevidipine', 'cisatracurium'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient = pyCLIF.load_data('clif_patient')\n",
    "hospitalization = pyCLIF.load_data('clif_hospitalization')\n",
    "hospitalization['hospitalization_id']= hospitalization['hospitalization_id'].astype(str)\n",
    "patient['patient_id']= patient['patient_id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize all _dttm variables to the same format\n",
    "patient = pyCLIF.standardize_datetime(patient)\n",
    "hospitalization = pyCLIF.standardize_datetime(hospitalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient = pyCLIF.remove_duplicates(patient, ['patient_id'], 'patient')\n",
    "hospitalization = pyCLIF.remove_duplicates(hospitalization, ['hospitalization_id'], 'hospitalization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total Number of unique encounters in the data: {pyCLIF.count_unique_encounters(hospitalization, 'hospitalization_id')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cohort Identification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inclusion Criteria:\n",
    "\n",
    "* Filter Admissions for 2020-03-01 to 2022-03-31\n",
    "* Encounters receiving invasive mechanical ventilation during this period\n",
    "* A cool off period of 4 hours after first intubation for analysis\n",
    "\n",
    "### Exclusion criteria:\n",
    "\n",
    "1. Encounters that were on vent for less than 2 hours\n",
    "2. Encounters that were on trach in the first 72 hours \n",
    "3. Encounters that received Cisatracurium for 4 hours or more within the first 72 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort = hospitalization[\n",
    "    (hospitalization['admission_dttm'] >= '2020-03-01') &\n",
    "    (hospitalization['admission_dttm'] <= '2022-03-31') &\n",
    "    (hospitalization['age_at_admission'] >= 18)\n",
    "].reset_index(drop=True)[['hospitalization_id']].drop_duplicates()\n",
    "\n",
    "cohort_ids = cohort['hospitalization_id'].unique().tolist()\n",
    "print(f\"Number of unique encounters after filtering by date and age:\", cohort['hospitalization_id'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import clif respiratory table for this cohort\n",
    "rst_filters = {\n",
    "    'hospitalization_id': cohort_ids\n",
    "}\n",
    "resp_support_raw = pyCLIF.load_data('clif_respiratory_support', columns=rst_required_columns, filters=rst_filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp_support = resp_support_raw.copy()\n",
    "resp_support['recorded_dttm'] = pd.to_datetime(resp_support['recorded_dttm'])\n",
    "resp_support['device_category'] = resp_support['device_category'].str.lower()\n",
    "resp_support['mode_category'] = resp_support['mode_category'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Nick's Waterfall fill logic for respiratory support table\n",
    "# This can take time: 2 - 12 mins depending on data size\n",
    "processed_resp_support = pyCLIF.process_resp_support(resp_support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the cohort on invasive mechanical ventilation \n",
    "columns_to_keep = [\n",
    "    'hospitalization_id', 'recorded_dttm', 'device_name','device_category',\n",
    "    'mode_name', 'mode_category' , 'tracheostomy',\n",
    "    'fio2_set', 'lpm_set', 'peep_set', \n",
    "    'resp_rate_obs', 'resp_rate_set'\n",
    "]\n",
    "\n",
    "ventilator_usage = processed_resp_support[processed_resp_support['device_category'].str.contains(\"imv\", case=False, na=False)]\n",
    "cohort_on_vent = ventilator_usage.merge(cohort, on='hospitalization_id', how='left')\n",
    "cohort_on_vent = cohort_on_vent[columns_to_keep]\n",
    "cohort_on_vent['on_vent'] = cohort_on_vent['device_category'].str.contains(\"imv\", case=False, na=False).astype(int)\n",
    "cohort_on_vent.loc[:, 'recorded_dttm'] = pd.to_datetime(cohort_on_vent['recorded_dttm'])\n",
    "cohort_on_vent = cohort_on_vent.sort_values(by=['hospitalization_id', 'recorded_dttm'])\n",
    "cohort_on_vent = cohort_on_vent[cohort_on_vent['on_vent'] == 1]\n",
    "\n",
    "\n",
    "# Apply thresholds and replace values outside these with NaN using .loc[]\n",
    "# UPDATE THIS TO USE CSV / JSON FROM OUTLIER DIRECTORY\n",
    "# cohort_on_vent.loc[:, 'fio2_set'] = cohort_on_vent['fio2_set'].where(cohort_on_vent['fio2_set'].between(0.21, 1, inclusive='both'), np.nan)\n",
    "# Calculate the mean of 'fio2_set', excluding NaN values\n",
    "fio2_mean = cohort_on_vent['fio2_set'].mean(skipna=True)\n",
    "print(\"FIO2_SET MEAN\", fio2_mean)\n",
    "# If the mean is greater than 1, divide 'fio2_set' by 100\n",
    "if fio2_mean > 1:\n",
    "    # Only divide values greater than 1 to avoid re-dividing already correct values\n",
    "    print(\"Updated fio2_set to be between 0.21 and 1\")\n",
    "    cohort_on_vent.loc[cohort_on_vent['fio2_set'] > 1, 'fio2_set'] = \\\n",
    "        cohort_on_vent.loc[cohort_on_vent['fio2_set'] > 1, 'fio2_set'] / 100\n",
    "\n",
    "cohort_on_vent.loc[:, 'fio2_set'] = cohort_on_vent['fio2_set'].where(cohort_on_vent['fio2_set'].between(0.21, 1, inclusive='both'), np.nan)\n",
    "cohort_on_vent.loc[:, 'resp_rate_set'] = cohort_on_vent['resp_rate_set'].where(cohort_on_vent['resp_rate_set'].between(0, 60, inclusive='both'), np.nan)\n",
    "cohort_on_vent.loc[:, 'peep_set'] = cohort_on_vent['peep_set'].where(cohort_on_vent['peep_set'].between(0, 50, inclusive='both'), np.nan)\n",
    "cohort_on_vent.loc[:, 'resp_rate_obs'] = cohort_on_vent['resp_rate_obs'].where(cohort_on_vent['resp_rate_obs'].between(0, 100, inclusive='both'), np.nan)\n",
    "cohort_on_vent.loc[:, 'lpm_set'] = cohort_on_vent['lpm_set'].where(cohort_on_vent['lpm_set'].between(0, 60, inclusive='both'), np.nan)\n",
    "\n",
    "cohort_on_vent['recorded_date'] = cohort_on_vent['recorded_dttm'].dt.date\n",
    "cohort_on_vent['recorded_hour'] = cohort_on_vent['recorded_dttm'].dt.hour\n",
    "\n",
    "print(f\"Number of unique encounters after filtering for ventilator usage: {cohort_on_vent['hospitalization_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vent_start_end = cohort_on_vent.groupby('hospitalization_id').agg(\n",
    "    vent_start_time=('recorded_dttm', 'min'),\n",
    "    vent_end_time=('recorded_dttm', 'max')\n",
    ").reset_index()\n",
    "\n",
    "# Exclude encounters where vent start time and end time is the same \n",
    "vent_start_end = vent_start_end[vent_start_end['vent_start_time'] != vent_start_end['vent_end_time']]\n",
    "print(f\"Number of unique encounters after filtering for ventilator usage: {vent_start_end['hospitalization_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required vitals\n",
    "vitals_filters = {\n",
    "    'hospitalization_id': cohort_ids,\n",
    "    'vital_category': vitals_of_interest\n",
    "}\n",
    "vitals = pyCLIF.load_data('clif_vitals', columns=vitals_required_columns, filters=vitals_filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vitals.value_counts('vital_category')\n",
    "## if you don't have MAP, we can calculate here- TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get first_vital_dttm and last_vital_dttm for each hospitalization_id \n",
    "# We use this as proxy for admission and discharge dttm to construct hourly sequence for each hospitalization\n",
    "vital_dttm_bounds = vitals.groupby('hospitalization_id')['recorded_dttm'].agg(['min', 'max']).reset_index()\n",
    "vital_dttm_bounds.columns = ['hospitalization_id', 'first_vital_dttm', 'last_vital_dttm']\n",
    "print(\"unique encounters in vitals\", pyCLIF.count_unique_encounters(vital_dttm_bounds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get height , weight and bmi for \n",
    "# Filter vitals to include only height and weight\n",
    "vitals_bmi = vitals[vitals['vital_category'].isin(['weight_kg', 'height_cm'])].copy()\n",
    "\n",
    "# Remove outliers\n",
    "vitals_bmi = vitals_bmi[\n",
    "    ((vitals_bmi['vital_category'] == 'height_cm') & (vitals_bmi['vital_value'] >= 76.2) & (vitals_bmi['vital_value'] <= 244)) |\n",
    "    ((vitals_bmi['vital_category'] == 'weight_kg') & (vitals_bmi['vital_value'] >= 20) & (vitals_bmi['vital_value'] <= 1100))\n",
    "]\n",
    "\n",
    "# Merge with vent_start_end to get ventilation start time\n",
    "vitals_bmi = vitals_bmi.merge(vent_start_end[['hospitalization_id', 'vent_start_time']], on='hospitalization_id', how='left')\n",
    "\n",
    "# Calculate time difference between recorded_dttm and vent_start_time\n",
    "vitals_bmi['recorded_dttm'] = pd.to_datetime(vitals_bmi['recorded_dttm'])\n",
    "vitals_bmi['vent_start_time'] = pd.to_datetime(vitals_bmi['vent_start_time'])\n",
    "vitals_bmi['time_diff'] = (vitals_bmi['recorded_dttm'] - vitals_bmi['vent_start_time']).dt.total_seconds() / 3600  # in hours\n",
    "\n",
    "# Define whether measurement is before or after vent_start_time\n",
    "vitals_bmi['before_vent_start'] = (vitals_bmi['time_diff'] <= 0).astype(int)\n",
    "\n",
    "# Calculate absolute time difference\n",
    "vitals_bmi['abs_time_diff'] = vitals_bmi['time_diff'].abs()\n",
    "\n",
    "# Sort data to prioritize measurements before vent start and closest in time\n",
    "vitals_bmi = vitals_bmi.sort_values(['hospitalization_id', 'vital_category', 'before_vent_start', 'abs_time_diff'], ascending=[True, True, False, True])\n",
    "\n",
    "# Drop duplicates to keep the closest measurement for each vital_category per hospitalization_id\n",
    "vitals_bmi = vitals_bmi.drop_duplicates(subset=['hospitalization_id', 'vital_category'], keep='first')\n",
    "\n",
    "# Pivot to get height and weight in columns\n",
    "vitals_bmi_pivot = vitals_bmi.pivot(index='hospitalization_id', columns='vital_category', values='vital_value').reset_index()\n",
    "\n",
    "# Calculate BMI\n",
    "vitals_bmi_pivot['bmi'] = vitals_bmi_pivot['weight_kg'] / ((vitals_bmi_pivot['height_cm'] / 100) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hourly sequence for the cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_cohort = vent_start_end.merge(vital_dttm_bounds, on='hospitalization_id', how='inner')\n",
    "print(\"unique encounters in resp filtered\", pyCLIF.count_unique_encounters(final_cohort))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check - last recorded vital shouldn't be less than vent start time\n",
    "# if this happens, check your CLIF tables bro\n",
    "cases_before_vent_start = final_cohort[final_cohort['last_vital_dttm'] < final_cohort['vent_start_time']]\n",
    "print(\"Cases where last vital dttm is before vent_start time:\", len(cases_before_vent_start))\n",
    "cases_before_vent_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save IDs in this cohort to filter other tables\n",
    "cohort_ids = final_cohort['hospitalization_id'].unique().tolist()\n",
    "print(\"total number of unique hospitalizations in the identified cohort\", len(cohort_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate hourly sequence for each group (hospitalization_id)\n",
    "def generate_hourly_sequence(group):\n",
    "    # Get the vent start time and discharge time\n",
    "    start_time = group['vent_start_time'].iloc[0]\n",
    "    end_time = group['last_vital_dttm'].iloc[0]\n",
    "    \n",
    "    # Generate the sequence of hourly timestamps\n",
    "    hourly_timestamps = pd.date_range(start=start_time, end=end_time, freq='h')\n",
    "    \n",
    "    # Create a new DataFrame for this sequence\n",
    "    return pd.DataFrame({\n",
    "        'hospitalization_id': group['hospitalization_id'].iloc[0],\n",
    "        'recorded_dttm': hourly_timestamps\n",
    "    })\n",
    "\n",
    "# Apply the function to each group and concatenate the results\n",
    "hour_sequence = final_cohort.groupby('hospitalization_id')\\\n",
    "    .apply(generate_hourly_sequence)\\\n",
    "    .reset_index(drop=True)\n",
    "\n",
    "# Add `recorded_date` and `recorded_hour` columns\n",
    "# Convert recorded_dttm to datetime sanity check\n",
    "hour_sequence['recorded_dttm'] = pd.to_datetime(hour_sequence['recorded_dttm'])\n",
    "hour_sequence['recorded_date'] = hour_sequence['recorded_dttm'].dt.date\n",
    "hour_sequence['recorded_hour'] = hour_sequence['recorded_dttm'].dt.hour\n",
    "hour_sequence = hour_sequence.drop('recorded_dttm', axis=1)\n",
    "hour_sequence = hour_sequence.drop_duplicates()\n",
    "hour_sequence['time_from_vent'] = hour_sequence.groupby('hospitalization_id').cumcount()\n",
    "## add a cool off period of 4 hours after first intubation\n",
    "hour_sequence['time_from_vent_adjusted'] = hour_sequence['time_from_vent'].apply(lambda x: x - 4 if x >= 4 else -1).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SHOULDN'T HAVE ANY DUPLICATES\n",
    "hour_sequence_check = pyCLIF.remove_duplicates(hour_sequence, ['hospitalization_id', 'recorded_date', 'recorded_hour'], 'hour_sequence_check')\n",
    "del hour_sequence_check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hourly Respiratory support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_vent_df = cohort_on_vent.groupby(['hospitalization_id', 'recorded_date', 'recorded_hour']).agg(\n",
    "    min_resp_rate_obs=pd.NamedAgg(column='resp_rate_obs', aggfunc='min'),\n",
    "    min_lpm_set=pd.NamedAgg(column='lpm_set', aggfunc='min'),\n",
    "    min_fio2_set=pd.NamedAgg(column='fio2_set', aggfunc='min'),\n",
    "    min_peep_set=pd.NamedAgg(column='peep_set', aggfunc='min'),\n",
    "    max_resp_rate_obs=pd.NamedAgg(column='resp_rate_obs', aggfunc='max'),\n",
    "    max_lpm_set=pd.NamedAgg(column='lpm_set', aggfunc='max'),\n",
    "    max_fio2_set=pd.NamedAgg(column='fio2_set', aggfunc='max'),\n",
    "    max_peep_set=pd.NamedAgg(column='peep_set', aggfunc='max'),\n",
    "    hourly_trach=pd.NamedAgg(column='tracheostomy', aggfunc=lambda x: 1 if x.max() == 1 else 0),\n",
    "    hourly_on_vent=pd.NamedAgg(column='on_vent', aggfunc=lambda x: 1 if x.max() == 1 else 0)\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge hourly_vent_df with hour_sequence on hospitalization_id, recorded_date, and recorded_hour\n",
    "final_df = pd.merge(hour_sequence, hourly_vent_df, on=['hospitalization_id', 'recorded_date', 'recorded_hour'], \n",
    "                     how='left')\n",
    "print(\"unique encounters who were ever on vent (before applying exclusion criteria)\", pyCLIF.count_unique_encounters(final_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total hours on vent for each encounter within the first 72 hours\n",
    "first_72_hours = hour_sequence[(hour_sequence['time_from_vent_adjusted'] >= 0) & (hour_sequence['time_from_vent_adjusted'] < 72)]\n",
    "final_df_72 = pd.merge(first_72_hours, hourly_vent_df, on=['hospitalization_id', 'recorded_date', 'recorded_hour'], \n",
    "                     how='left')\n",
    "vent_hours_per_encounter = final_df_72.groupby('hospitalization_id')['hourly_on_vent'].sum()\n",
    "# Identify encounters with less than 2 hours on vent\n",
    "encounters_less_than_2_hours = vent_hours_per_encounter[vent_hours_per_encounter <= 2].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude those encounters that were on the vent for less than 2 hours in the first 72 hours\n",
    "final_df = final_df[~final_df['hospitalization_id'].isin(encounters_less_than_2_hours)]\n",
    "print(\"\\n encounters that were on the vent for less than 2 hours in the first 72 hours\", len(encounters_less_than_2_hours))\n",
    "print(\"\\n unique encounters after excluding encounters on vent for 2 hrs or less\", pyCLIF.count_unique_encounters(final_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude encounters with tracheostomy in the first 72 hours\n",
    "# Identify encounters with trach in the first 72 hours\n",
    "encounters_with_trach = final_df_72.groupby('hospitalization_id')['hourly_trach'].max()\n",
    "# Identify encounters where trach is present\n",
    "encounters_with_trach = encounters_with_trach[encounters_with_trach == 1].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude encounters with trach in the first 72 hours\n",
    "final_df = final_df[~final_df['hospitalization_id'].isin(encounters_with_trach)]\n",
    "print(\"\\n encounters with trach in the first 72 hours\", len(encounters_with_trach))\n",
    "print(\"\\n unique encounters after excluding encounters on trach during the first 72 hours\", pyCLIF.count_unique_encounters(final_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hourly Meds\n",
    "\n",
    "* Exclude encounters that are on cisatracurium for more than 4 hours within the first 72 hours\n",
    "* Calculate NE equivalent levels using \"norepinephrine\", \"epinephrine\", \"phenylephrine\", \"vasopressin\", \"dopamine\",  \"angiotensin\"\n",
    "* Create flags for \"nicardipine\", \"nitroprusside\", \"clevidipine\" for the red criteria under consensus criteria\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import clif continuous meds for the cohort on vent during the required time period\n",
    "meds_filters = {\n",
    "    'hospitalization_id': cohort_ids,\n",
    "    'med_category': meds_of_interest\n",
    "}\n",
    "meds = pyCLIF.load_data('clif_medication_admin_continuous', columns=meds_required_columns, filters=meds_filters)\n",
    "print(\"unique encounters in meds\", pyCLIF.count_unique_encounters(meds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "meds['admin_dttm'] = pd.to_datetime(meds['admin_dttm'], format='%Y-%m-%d %H:%M:%S')\n",
    "meds['med_dose'] = pd.to_numeric(meds['med_dose'], errors='coerce')\n",
    "# Create 'date' and 'hour_of_day' columns\n",
    "meds['recorded_date'] = meds['admin_dttm'].dt.date\n",
    "meds['recorded_hour'] = meds['admin_dttm'].dt.hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exclude encounters that are on cisatracurium for more than 4 hours within the first 72 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'admin_dttm' is in datetime format\n",
    "cisatracurium_filtered = meds[meds['med_category'].str.contains(\"cisatracurium\", case=False, na=False)].drop_duplicates()\n",
    "# Sort by 'hospitalization_id' and 'admin_dttm'\n",
    "cisatracurium_filtered = cisatracurium_filtered.sort_values(['hospitalization_id', 'recorded_date', 'recorded_hour'])\n",
    "\n",
    "# Merge with vent_start_end to get vent_start_time\n",
    "cisatracurium_filtered = cisatracurium_filtered.merge(\n",
    "    final_df_72[['hospitalization_id', 'recorded_date', 'recorded_hour']], \n",
    "    on=['hospitalization_id', 'recorded_date', 'recorded_hour'], \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Define the maximum allowed gap between doses (e.g., 1 hour)\n",
    "max_gap = pd.Timedelta(hours=1)\n",
    "\n",
    "# Function to identify continuous periods\n",
    "def identify_continuous_periods(group):\n",
    "    group = group.copy()\n",
    "    group['time_diff'] = group['admin_dttm'].diff()\n",
    "    group['new_period'] = (group['time_diff'] > max_gap) | (group['time_diff'].isna())\n",
    "    group['period_id'] = group['new_period'].cumsum()\n",
    "    return group\n",
    "\n",
    "# Apply the function to each 'hospitalization_id'\n",
    "cis_periods = cisatracurium_filtered.groupby('hospitalization_id').apply(identify_continuous_periods).reset_index(drop=True)\n",
    "\n",
    "# Calculate the duration of each continuous period\n",
    "period_durations = cis_periods.groupby(['hospitalization_id', 'period_id']).agg(\n",
    "    period_start=('admin_dttm', 'min'),\n",
    "    period_end=('admin_dttm', 'max')\n",
    ").reset_index()\n",
    "\n",
    "period_durations['period_duration'] = (\n",
    "    period_durations['period_end'] - period_durations['period_start']\n",
    ").dt.total_seconds() / 3600  # Convert to hours\n",
    "\n",
    "# Identify patients with any continuous period >= 4 hours\n",
    "cis_flag_df = period_durations.groupby('hospitalization_id').agg(\n",
    "    max_period_duration=('period_duration', 'max')\n",
    ").reset_index()\n",
    "\n",
    "cis_flag_df['cis_flag'] = (cis_flag_df['max_period_duration'] >= 4).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encounters_with_cis = cis_flag_df[cis_flag_df['cis_flag'] == 1]['hospitalization_id'].drop_duplicates()\n",
    "final_df = final_df[~final_df['hospitalization_id'].isin(encounters_with_cis)]\n",
    "print(\"\\n encounters with cis for more than 4 hours  in the first 72 hours\", len(encounters_with_cis))\n",
    "print(\"\\n unique encounters after excluding encounters on trach during the first 72 hours\", pyCLIF.count_unique_encounters(final_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Norepinephrine equivalent calculation\n",
    "# Goradia S, Sardaneh AA, Narayan SW, Penm J, Patanwala AE. Vasopressor dose equivalence: \n",
    "# A scoping review and suggested formula. J Crit Care. 2021 Feb;61:233-240. doi: 10.1016/j.jcrc.2020.11.002. Epub 2020 Nov 14. PMID: 33220576.\n",
    "\n",
    "meds_list = [\n",
    "    \"norepinephrine\", \"epinephrine\", \"phenylephrine\", \n",
    "    \"vasopressin\", \"dopamine\",  \n",
    "    \"angiotensin\"\n",
    "]\n",
    "\n",
    "# Function to check if 'med_dose_unit' contains '/hr' or '/min'\n",
    "def has_per_hour_or_min(unit):\n",
    "    if pd.isnull(unit):\n",
    "        return False\n",
    "    unit = unit.lower()\n",
    "    return '/hr' in unit or '/min' in unit\n",
    "\n",
    "# Filter meds to include only rows with '/hr' or '/min' in 'med_dose_unit'\n",
    "meds_filtered = meds[meds['med_dose_unit'].apply(has_per_hour_or_min)].copy()\n",
    "\n",
    "ne_df = meds_filtered[meds_filtered['med_category'].isin(meds_list)].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary table for each med_category\n",
    "summary_table = ne_df.groupby('med_category').agg(\n",
    "    total_N=('med_category', 'size'),\n",
    "    min=('med_dose', 'min'),\n",
    "    max=('med_dose', 'max'),\n",
    "    first_quantile=('med_dose', lambda x: x.quantile(0.25)),\n",
    "    second_quantile=('med_dose', lambda x: x.quantile(0.5)),\n",
    "    third_quantile=('med_dose', lambda x: x.quantile(0.75)),\n",
    "    missing_values=('med_dose', lambda x: x.isna().sum())\n",
    ").reset_index()\n",
    "\n",
    "## check the distrbituon of required continuous meds\n",
    "summary_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the med_dose_unit for each med_category in the meds table\n",
    "med_dose_unit_check = meds.groupby(['med_category', 'med_dose_unit']).size().reset_index(name='count')\n",
    "# Display the results\n",
    "med_dose_unit_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# **2. Convert Medication Doses to Required Units**\n",
    "\n",
    "# Define medications and their unit conversion information\n",
    "meds_list = [\n",
    "    \"norepinephrine\", \"epinephrine\", \"phenylephrine\",\n",
    "    \"vasopressin\", \"dopamine\", \"angiotensin\"\n",
    "]\n",
    "\n",
    "med_unit_info = {\n",
    "    'norepinephrine': {\n",
    "        'required_unit': 'mcg/kg/min',\n",
    "        'acceptable_units': ['mcg/kg/min', 'mcg/kg/hr', 'mg/kg/hr', 'mcg/min', 'mg/hr'],\n",
    "    },\n",
    "    'epinephrine': {\n",
    "        'required_unit': 'mcg/kg/min',\n",
    "        'acceptable_units': ['mcg/kg/min', 'mcg/kg/hr', 'mg/kg/hr', 'mcg/min', 'mg/hr'],\n",
    "    },\n",
    "    'phenylephrine': {\n",
    "        'required_unit': 'mcg/kg/min',\n",
    "        'acceptable_units': ['mcg/kg/min', 'mcg/kg/hr', 'mg/kg/hr', 'mcg/min', 'mg/hr'],\n",
    "    },\n",
    "    'dopamine': {\n",
    "        'required_unit': 'mcg/kg/min',\n",
    "        'acceptable_units': ['mcg/kg/min', 'mcg/kg/hr', 'mg/kg/hr', 'mcg/min', 'mg/hr'],\n",
    "    },\n",
    "    'metaraminol': {\n",
    "        'required_unit': 'mcg/kg/min',\n",
    "        'acceptable_units': ['mg/hr', 'mcg/min'],\n",
    "    },\n",
    "    'angiotensin': {\n",
    "        'required_unit': 'mcg/kg/min',\n",
    "        'acceptable_units': ['ng/kg/min', 'ng/kg/hr'],\n",
    "    },\n",
    "    'vasopressin': {\n",
    "        'required_unit': 'units/min',\n",
    "        'acceptable_units': ['units/min', 'units/hr', 'milliunits/min', 'milliunits/hr'],\n",
    "    },\n",
    "}\n",
    "\n",
    "# Function to get conversion factor for each medication\n",
    "def get_conversion_factor(med_category, med_dose_unit, weight_kg):\n",
    "    med_info = med_unit_info.get(med_category, None)\n",
    "    if not med_info:\n",
    "        # Medication not in the list\n",
    "        return None\n",
    "    required_unit = med_info['required_unit']\n",
    "    acceptable_units = med_info['acceptable_units']\n",
    "    med_dose_unit = med_dose_unit.lower()\n",
    "    if med_category in ['norepinephrine', 'epinephrine', 'phenylephrine', 'dopamine', 'metaraminol']:\n",
    "        # Required unit: mcg/kg/min\n",
    "        if med_dose_unit == 'mcg/kg/min':\n",
    "            factor = 1.0\n",
    "        elif med_dose_unit == 'mcg/kg/hr':\n",
    "            factor = 1 / 60\n",
    "        elif med_dose_unit == 'mg/kg/hr':\n",
    "            factor = 1000 / 60\n",
    "        elif med_dose_unit == 'mcg/min':\n",
    "            factor = 1 / weight_kg\n",
    "        elif med_dose_unit == 'mg/hr':\n",
    "            factor = 1000 / 60 / weight_kg\n",
    "        else:\n",
    "            return None\n",
    "    elif med_category == 'angiotensin':\n",
    "        # Required unit: mcg/kg/min\n",
    "        if med_dose_unit == 'ng/kg/min':\n",
    "            factor = 1 / 1000\n",
    "        elif med_dose_unit == 'ng/kg/hr':\n",
    "            factor = 1 / 1000 / 60\n",
    "        else:\n",
    "            return None\n",
    "    elif med_category == 'vasopressin':\n",
    "        # Required unit: units/min\n",
    "        if med_dose_unit == 'units/min':\n",
    "            factor = 1.0\n",
    "        elif med_dose_unit == 'units/hr':\n",
    "            factor = 1 / 60\n",
    "        elif med_dose_unit == 'milliunits/min':\n",
    "            factor = 1 / 1000\n",
    "        elif med_dose_unit == 'milliunits/hr':\n",
    "            factor = 1 / 1000 / 60\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "    return factor\n",
    "\n",
    "# Merge weight_kg into meds_filtered (assuming 'vitals_bmi_pivot' is available)\n",
    "meds_filtered = meds_filtered.merge(vitals_bmi_pivot[['hospitalization_id', 'weight_kg']], on='hospitalization_id', how='left')\n",
    "\n",
    "# Remove rows with missing weight_kg\n",
    "meds_filtered = meds_filtered[~meds_filtered['weight_kg'].isnull()].copy()\n",
    "\n",
    "# Function to convert doses\n",
    "def convert_dose(row):\n",
    "    med_category = row['med_category']\n",
    "    med_dose = row['med_dose']\n",
    "    med_dose_unit = row['med_dose_unit']\n",
    "    weight_kg = row['weight_kg']\n",
    "    factor = get_conversion_factor(med_category, med_dose_unit, weight_kg)\n",
    "    if factor is None:\n",
    "        return np.nan\n",
    "    return med_dose * factor\n",
    "\n",
    "# Apply the conversion to get 'med_dose_converted'\n",
    "meds_filtered['med_dose_converted'] = meds_filtered.apply(convert_dose, axis=1)\n",
    "\n",
    "# Drop rows with NaN in 'med_dose_converted' (unrecognized units)\n",
    "meds_filtered = meds_filtered[~meds_filtered['med_dose_converted'].isnull()].copy()\n",
    "\n",
    "# Define acceptable dose ranges\n",
    "med_dose_ranges = {\n",
    "    'norepinephrine': (0.01, 3),\n",
    "    'epinephrine': (0.01, 0.1),\n",
    "    'phenylephrine': (0.1, 5),\n",
    "    'dopamine': (2, 20),\n",
    "    'metaraminol': (0.5, 10),  # Hypothetical range\n",
    "    'angiotensin': (0.02, 0.2),  # After conversion to mcg/kg/min\n",
    "    'vasopressin': (0.01, 0.1),  # Units/min acceptable range\n",
    "}\n",
    "\n",
    "# Function to check if dose is within range\n",
    "def is_dose_within_range(row):\n",
    "    med_category = row['med_category']\n",
    "    med_dose_converted = row['med_dose_converted']\n",
    "    dose_range = med_dose_ranges.get(med_category, None)\n",
    "    if dose_range is None:\n",
    "        return False\n",
    "    min_dose, max_dose = dose_range\n",
    "    return min_dose <= med_dose_converted <= max_dose\n",
    "\n",
    "# Filter doses within acceptable ranges\n",
    "meds_filtered = meds_filtered[meds_filtered.apply(is_dose_within_range, axis=1)].copy()\n",
    "\n",
    "# **4. Flag Medications Not in the Dataset**\n",
    "\n",
    "for med in meds_list:\n",
    "    if med not in meds_filtered['med_category'].unique():\n",
    "        print(f\"{med} is not in the dataset.\")\n",
    "\n",
    "# Pivot and Aggregate the Data**\n",
    "\n",
    "# Create 'recorded_date' and 'recorded_hour' columns\n",
    "meds_filtered['admin_dttm'] = pd.to_datetime(meds_filtered['admin_dttm'])\n",
    "meds_filtered['recorded_date'] = meds_filtered['admin_dttm'].dt.date\n",
    "meds_filtered['recorded_hour'] = meds_filtered['admin_dttm'].dt.hour\n",
    "\n",
    "# Group and aggregate doses\n",
    "group_cols = ['hospitalization_id', 'recorded_date', 'recorded_hour', 'med_category']\n",
    "dose_agg = meds_filtered.groupby(group_cols)['med_dose_converted'].agg(['min', 'max']).reset_index()\n",
    "\n",
    "# Pivot to have medications as columns\n",
    "dose_pivot_min = dose_agg.pivot_table(index=['hospitalization_id', 'recorded_date', 'recorded_hour'], columns='med_category', values='min').reset_index()\n",
    "dose_pivot_max = dose_agg.pivot_table(index=['hospitalization_id', 'recorded_date', 'recorded_hour'], columns='med_category', values='max').reset_index()\n",
    "\n",
    "# Rename columns to indicate min and max\n",
    "dose_pivot_min.columns = ['hospitalization_id', 'recorded_date', 'recorded_hour'] + ['min_' + col for col in dose_pivot_min.columns if col not in ['hospitalization_id', 'recorded_date', 'recorded_hour']]\n",
    "dose_pivot_max.columns = ['hospitalization_id', 'recorded_date', 'recorded_hour'] + ['max_' + col for col in dose_pivot_max.columns if col not in ['hospitalization_id', 'recorded_date', 'recorded_hour']]\n",
    "\n",
    "# Merge min and max DataFrames\n",
    "dose_pivot = pd.merge(dose_pivot_min, dose_pivot_max, on=['hospitalization_id', 'recorded_date', 'recorded_hour'], how='outer')\n",
    "\n",
    "# **6. Calculate Norepinephrine Equivalents**\n",
    "\n",
    "# Replace NaN with 0 for calculations\n",
    "dose_pivot.fillna(0, inplace=True)\n",
    "\n",
    "# Calculate NE min\n",
    "dose_pivot['ne_calc_min'] = (\n",
    "    dose_pivot.get('min_norepinephrine', 0) +\n",
    "    dose_pivot.get('min_epinephrine', 0) +\n",
    "    dose_pivot.get('min_phenylephrine', 0) / 10 +\n",
    "    dose_pivot.get('min_dopamine', 0) / 100 +\n",
    "    dose_pivot.get('min_metaraminol', 0) / 8 +\n",
    "    dose_pivot.get('min_vasopressin', 0) * 2.5 +\n",
    "    dose_pivot.get('min_angiotensin', 0) * 10\n",
    ")\n",
    "\n",
    "# Calculate NE max\n",
    "dose_pivot['ne_calc_max'] = (\n",
    "    dose_pivot.get('max_norepinephrine', 0) +\n",
    "    dose_pivot.get('max_epinephrine', 0) +\n",
    "    dose_pivot.get('max_phenylephrine', 0) / 10 +\n",
    "    dose_pivot.get('max_dopamine', 0) / 100 +\n",
    "    dose_pivot.get('max_metaraminol', 0) / 8 +\n",
    "    dose_pivot.get('max_vasopressin', 0) * 2.5 +\n",
    "    dose_pivot.get('max_angiotensin', 0) * 10\n",
    ")\n",
    "\n",
    "# **7. Prepare the Final Dataset**\n",
    "\n",
    "# Keep only the required columns\n",
    "ne_calc_df = dose_pivot[['hospitalization_id', 'recorded_date', \n",
    "                         'recorded_hour', \n",
    "                         'ne_calc_min', 'ne_calc_max']].drop_duplicates(subset=['hospitalization_id', 'recorded_date', 'recorded_hour'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.merge(final_df, ne_calc_df, on=['hospitalization_id', 'recorded_date', 'recorded_hour'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_meds_list = [\n",
    "    \"nicardipine\", \"nitroprusside\", \"clevidipine\"\n",
    "]\n",
    "\n",
    "# Filter meds_filtered for the medications in red_meds_list\n",
    "red_meds_df = meds[meds['med_category'].isin(red_meds_list)].copy()\n",
    "\n",
    "# Create a flag for each medication in red_meds_list\n",
    "for med in red_meds_list:\n",
    "    # Create a flag that is 1 if the medication was administered in that hour, 0 otherwise\n",
    "    red_meds_df[med + '_flag'] = np.where(red_meds_df['med_category'] == med, 1, 0).astype(int)\n",
    "\n",
    "# Aggregate to get the maximum value for each flag (per hospitalization_id, recorded_date, recorded_hour)\n",
    "# This ensures that if the medication was administered even once in the hour, the flag is 1\n",
    "red_meds_flags = red_meds_df.groupby(['hospitalization_id', 'recorded_date', 'recorded_hour']).agg(\n",
    "    {med + '_flag': 'max' for med in red_meds_list}\n",
    ").reset_index()\n",
    "\n",
    "#  combine all flags into a single 'red_meds_flag', you can do so like this:\n",
    "red_meds_flags['red_meds_flag'] = red_meds_flags[[med + '_flag' for med in red_meds_list]].max(axis=1)\n",
    "\n",
    "# Select the relevant columns\n",
    "red_meds_flags_final = red_meds_flags[[\n",
    "    'hospitalization_id', 'recorded_date', 'recorded_hour',\n",
    "    'nicardipine_flag', 'nitroprusside_flag',\n",
    "    'clevidipine_flag', 'red_meds_flag'\n",
    "]].drop_duplicates(subset=['hospitalization_id', 'recorded_date', 'recorded_hour'])\n",
    "\n",
    "red_meds_flags_final['nicardipine_flag'] = red_meds_flags_final['nicardipine_flag'].astype(int)\n",
    "red_meds_flags_final['nitroprusside_flag'] = red_meds_flags_final['nitroprusside_flag'].astype(int)\n",
    "red_meds_flags_final['clevidipine_flag'] = red_meds_flags_final['clevidipine_flag'].astype(int)\n",
    "red_meds_flags_final['red_meds_flag'] = red_meds_flags_final['red_meds_flag'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.merge(final_df, red_meds_flags_final, on=['hospitalization_id', 'recorded_date', 'recorded_hour'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hourly Vitals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vitals.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'recorded_dttm' is datetime\n",
    "vitals['recorded_dttm'] = pd.to_datetime(vitals['recorded_dttm'])\n",
    "# Extract 'recorded_date' and 'recorded_hour'\n",
    "vitals['recorded_hour'] = vitals['recorded_dttm'].dt.hour\n",
    "vitals['recorded_date'] = vitals['recorded_dttm'].dt.date\n",
    "\n",
    "# Check if 'map' exists in 'vital_category'\n",
    "if 'map' not in vitals['vital_category'].unique():\n",
    "    print(\"map is not present, so we'll calculate it\")\n",
    "\n",
    "    # Filter vitals to include only 'sbp' and 'dbp'\n",
    "    sbp_dbp_vitals = vitals[vitals['vital_category'].isin(['sbp', 'dbp'])].copy()\n",
    "\n",
    "    # Pivot to have 'sbp' and 'dbp' as columns\n",
    "    sbp_dbp_pivot = sbp_dbp_vitals.pivot_table(\n",
    "        index=['hospitalization_id', 'recorded_dttm'],\n",
    "        columns='vital_category',\n",
    "        values='vital_value'\n",
    "    ).reset_index()\n",
    "\n",
    "    # Drop rows where either 'sbp' or 'dbp' is missing\n",
    "    sbp_dbp_pivot = sbp_dbp_pivot.dropna(subset=['sbp', 'dbp'])\n",
    "\n",
    "    # Calculate 'map' using the formula\n",
    "    sbp_dbp_pivot['map'] = (sbp_dbp_pivot['sbp'] + 2 * sbp_dbp_pivot['dbp']) / 3\n",
    "\n",
    "    # Create a DataFrame for 'map' vitals\n",
    "    map_vitals = sbp_dbp_pivot[['hospitalization_id', 'recorded_dttm', 'map']].copy()\n",
    "\n",
    "    # Add 'vital_category' and 'vital_value' columns\n",
    "    map_vitals['vital_category'] = 'map'\n",
    "    map_vitals['vital_value'] = map_vitals['map']\n",
    "    # Extract 'recorded_date' and 'recorded_hour' for map_vitals\n",
    "    map_vitals['recorded_date'] = map_vitals['recorded_dttm'].dt.date\n",
    "    map_vitals['recorded_hour'] = map_vitals['recorded_dttm'].dt.hour\n",
    "\n",
    "    # Keep only the necessary columns\n",
    "    map_vitals = map_vitals[['hospitalization_id', 'recorded_dttm', 'recorded_date', \n",
    "                             'recorded_hour', 'vital_category', 'vital_value']]\n",
    "\n",
    "\n",
    "    # Append 'map' vitals back to the original 'vitals' DataFrame\n",
    "    vitals = pd.concat([vitals, map_vitals], ignore_index=True)\n",
    "\n",
    "# Proceed with grouping and pivoting\n",
    "vitals_min_max = vitals.groupby(\n",
    "    ['hospitalization_id', 'recorded_date', 'recorded_hour', 'vital_category']\n",
    ").agg(\n",
    "    min=pd.NamedAgg(column='vital_value', aggfunc='min'),\n",
    "    max=pd.NamedAgg(column='vital_value', aggfunc='max')\n",
    ").reset_index()\n",
    "\n",
    "# Pivot the table to reshape it\n",
    "vitals_pivot = vitals_min_max.pivot_table(\n",
    "    index=['hospitalization_id', 'recorded_date', 'recorded_hour'],\n",
    "    columns='vital_category',\n",
    "    values=['min', 'max']\n",
    ").reset_index()\n",
    "\n",
    "# Flatten the column multi-index after pivot\n",
    "vitals_pivot.columns = [\n",
    "    '_'.join(col).strip() if isinstance(col, tuple) else col for col in vitals_pivot.columns\n",
    "]\n",
    "\n",
    "# Remove trailing underscores\n",
    "vitals_pivot.columns = [col.rstrip('_') for col in vitals_pivot.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge vitals with final_df\n",
    "final_df = pd.merge(final_df, vitals_pivot, on=['hospitalization_id', 'recorded_date', 'recorded_hour'], \n",
    "                   how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## confirm duplicates don't exist\n",
    "checkpoint_vitals = pyCLIF.remove_duplicates(final_df, [\n",
    "    'hospitalization_id','recorded_date', 'recorded_hour'\n",
    "], 'final_df')\n",
    "del checkpoint_vitals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hourly Lab\n",
    "\n",
    "Get most recent lactate defined as closest lab result time to the start of first intubation event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import clif continuous meds and clif labs table for the cohort on vent during the required time period\n",
    "labs_filters = {\n",
    "    'hospitalization_id': cohort_ids,\n",
    "    'lab_category': labs_of_interest\n",
    "}\n",
    "labs = pyCLIF.load_data('clif_labs', columns=labs_required_columns, filters=labs_filters)\n",
    "print(\"unique encounters in labs\", pyCLIF.count_unique_encounters(labs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "labs['lab_result_dttm'] = pd.to_datetime(labs['lab_result_dttm'])\n",
    "labs['recorded_hour'] = labs['lab_result_dttm'].dt.hour\n",
    "labs['recorded_date'] = labs['lab_result_dttm'].dt.date\n",
    "\n",
    "lactate_df = pd.merge(labs, vent_start_end, on='hospitalization_id', how='left')\n",
    "lactate_df['time_since_vent_start_hours'] = (\n",
    "    (lactate_df['lab_result_dttm'] - lactate_df['vent_start_time']).dt.total_seconds() / 3600\n",
    ")\n",
    "\n",
    "# Calculate the absolute time difference between lab_result_dttm and vent_start_time in hours\n",
    "lactate_df['time_diff_hours'] = abs((lactate_df['lab_result_dttm'] - lactate_df['vent_start_time']).dt.total_seconds() / 3600)\n",
    "\n",
    "# Filter for observations within the first 72 hours since vent_start_time\n",
    "lactate_df = lactate_df[(lactate_df['time_since_vent_start_hours'] >= 0) & \n",
    "                        (lactate_df['time_since_vent_start_hours'] <= 72)]\n",
    "\n",
    "# Sort by hospitalization_id, recorded_hour, and time_diff_hours to find the closest measurement to vent_start_time\n",
    "lactate_df = lactate_df.sort_values(by=['hospitalization_id', 'recorded_date', 'recorded_hour', 'time_diff_hours'])\n",
    "\n",
    "# Group by hospitalization_id and recorded_hour, and get the first row in each group (which is the closest measurement)\n",
    "# closest lactate measurement is defined as closest to the vent_start_time in that hour. \n",
    "closest_lactate_df = lactate_df.groupby(['hospitalization_id', 'recorded_date','recorded_hour']).first().reset_index()\n",
    "\n",
    "labs_final = closest_lactate_df[['hospitalization_id', 'recorded_date', 'recorded_hour', 'lab_value_numeric']].copy()\n",
    "\n",
    "# Rename the 'lab_value_numeric' column to 'lactate'\n",
    "labs_final = labs_final.rename(columns={'lab_value_numeric': 'lactate'})\n",
    "\n",
    "final_df = pd.merge(final_df, labs_final, on=['hospitalization_id', 'recorded_date', 'recorded_hour'], \n",
    "                   how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_labs= pyCLIF.remove_duplicates(final_df, [\n",
    "    'hospitalization_id', 'recorded_date', 'recorded_hour'\n",
    "], 'final_df')\n",
    "del checkpoint_labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write analysis dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_parquet('../output/intermediate/final_df.parquet')\n",
    "vent_start_end.to_parquet('../output/intermediate/vent_start_end.parquet')\n",
    "final_df['hospitalization_id'].to_csv('../output/intermediate/cohort_ids.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mobilization)",
   "language": "python",
   "name": ".mobilization"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
