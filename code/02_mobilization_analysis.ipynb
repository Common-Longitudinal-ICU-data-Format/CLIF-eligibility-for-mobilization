{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eligibility for mobilization - Analysis\n",
    "\n",
    "Run this script after running the [cohort_identification.ipynb](cohort_identification.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install pandas numpy duckdb seaborn matplotlib tableone\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from datetime import datetime\n",
    "from tableone import TableOne\n",
    "import pyCLIF\n",
    "\n",
    "from lifelines import KaplanMeierFitter\n",
    "import matplotlib.pyplot as plt\n",
    "from lifelines import KaplanMeierFitter\n",
    "from upsetplot import UpSet, from_indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.read_parquet('../output/intermediate/final_df_hourly.parquet')\n",
    "all_ids_w_outcome = pd.read_parquet('../output/intermediate/cohort_all_ids_w_outcome.parquet')\n",
    "sofa_df = pd.read_parquet('../output/intermediate/sofa.parquet')\n",
    "final_df_blocks = pd.read_parquet('../output/intermediate/final_df_blocks.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward and Backward fill the final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_filling = final_df.isnull().sum() / len(final_df) * 100\n",
    "before_filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 ── safety ordering ───────────────────────────────────────\n",
    "final_df = final_df.sort_values(\n",
    "    by=['encounter_block', 'recorded_date', 'recorded_hour']\n",
    ")\n",
    "\n",
    "# 1 ── identify column groups ────────────────────────────────\n",
    "flag_columns = [\n",
    "    'hourly_trach','hourly_on_vent','nicardipine_flag','nitroprusside_flag',\n",
    "    'clevidipine_flag','red_meds_flag','cisatracurium_flag','vecuronium_flag',\n",
    "    'rocuronium_flag','paralytics_flag'\n",
    "]\n",
    "exclude_columns = [\n",
    "    'patient_id','hospitalization_id','encounter_block',\n",
    "    'recorded_dttm','recorded_date','recorded_hour',\n",
    "    'time_from_vent','time_from_vent_adjusted', 'lactate',\n",
    "    'last_ne_dose_last_6_hours','ne_calc_last']\n",
    "\n",
    "all_cols           = set(final_df.columns)\n",
    "potential_fill     = all_cols - set(flag_columns) - set(exclude_columns) \n",
    "continuous_columns = [\n",
    "    c for c in potential_fill\n",
    "    if pd.api.types.is_numeric_dtype(final_df[c])\n",
    "]\n",
    "\n",
    "# 2 ── binary flags → 0/1 ints ───────────────────────────────\n",
    "for col in flag_columns:\n",
    "    final_df[col] = final_df[col].fillna(0).astype(int)\n",
    "\n",
    "# 3 ── forward / backward fill for numeric variables ---------\n",
    "final_df[continuous_columns] = (\n",
    "    final_df\n",
    "      .groupby('encounter_block')[continuous_columns]\n",
    "      .transform(lambda s: s.ffill().bfill())\n",
    ")\n",
    "\n",
    "# 4 ── lactate: forward-fill but **only 24 h** (24 rows) -----\n",
    "final_df['lactate'] = (\n",
    "    final_df\n",
    "      .groupby('encounter_block')\n",
    "      .apply(lambda g: g['lactate'].fillna(method='ffill', limit=24))\n",
    "      .reset_index(level=0, drop=True)\n",
    ")\n",
    "\n",
    "# 5 ── tracheostomy flag stays 1 once first seen -------------\n",
    "final_df['hourly_trach'] = (\n",
    "    final_df.groupby('encounter_block')['hourly_trach']\n",
    "            .transform(lambda s: s.cummax())\n",
    "            .astype(int)\n",
    ")\n",
    "\n",
    "final_df['ne_calc_last'] = (\n",
    "    final_df\n",
    "      .groupby('encounter_block')['ne_calc_last']\n",
    "      .transform(lambda s: s.ffill())\n",
    ")\n",
    "\n",
    "\n",
    "# 6 ── norepinephrine: exact-6-hour look-backs, but\n",
    "#     **only replace rows that were NA already**\n",
    "# ------------------------------------------------\n",
    "def add_exact_6h_ne(block: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    • If a row already has last_ne_dose_last_6_hours \n",
    "      it is **left unchanged**.\n",
    "    • Otherwise we insert the value from 6 hours earlier\n",
    "      (0 if no row exists 6 hours before).\n",
    "    Expect *block* to be time-sorted.\n",
    "    \"\"\"\n",
    "    block = block.copy()\n",
    "\n",
    "    # make sure the columns exist\n",
    "    if 'last_ne_dose_last_6_hours' not in block.columns:\n",
    "        block[col] = np.nan\n",
    "\n",
    "    # candidate fill values = value 6 rows earlier\n",
    "    fill_last = block['ne_calc_last'].shift(6)\n",
    "\n",
    "    # only overwrite where current value is NA\n",
    "    block.loc[block['last_ne_dose_last_6_hours'].isna(), 'last_ne_dose_last_6_hours'] = fill_last\n",
    "\n",
    "    # still-missing ⇒ 0  (= “no vasopressor recorded 6 h ago”)\n",
    "    block[['last_ne_dose_last_6_hours']] = block[[\n",
    "               'last_ne_dose_last_6_hours']].fillna(0)\n",
    "\n",
    "    return block\n",
    "\n",
    "\n",
    "final_df = (\n",
    "    final_df\n",
    "      .sort_values(['encounter_block', 'recorded_dttm'])   # guarantee order\n",
    "      .groupby('encounter_block', group_keys=False)\n",
    "      .apply(add_exact_6h_ne)\n",
    "      .reset_index(drop=True)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check2 = final_df[['encounter_block', 'hospitalization_id', 'recorded_date', 'recorded_hour', 'ne_calc_last', 'last_ne_dose_last_6_hours']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_filling = final_df.isnull().sum() / len(final_df) * 100\n",
    "after_filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint- useful to compare to the original df and check filling logic\n",
    "final_df.to_parquet(f'../output/intermediate/final_df_filled_{datetime.now().date()}.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Criteria Flags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patel et al. Criteria:\n",
    "\n",
    "Cardio\n",
    "* Mean arterial blood pressure: 65-110 mm Hg\n",
    "* Systolic blood pressure: ≤ 200 mm Hg\n",
    "* Heart rate: 40-130 beats per minute\n",
    "\n",
    "Respiratory\n",
    "* Respiratory rate: 5-40 breaths per minute\n",
    "* Pulse oximetry: ≥ 88%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Patel et al. Criteria\n",
    "\n",
    "# 1. Mean arterial blood pressure: 65-110 mm Hg\n",
    "final_df['patel_map_flag'] = (\n",
    "    (final_df['min_map'] >= 65) & (final_df['max_map'] <= 110)\n",
    ").astype(int)\n",
    "\n",
    "# 2. Systolic blood pressure: ≤ 200 mm Hg\n",
    "final_df['patel_sbp_flag'] = (\n",
    "    final_df['max_sbp'].isna() |\n",
    "    (final_df['max_sbp'] <= 200)\n",
    ").astype(int)\n",
    "\n",
    "# 3. Heart rate (Pulse): 40-130 beats per minute\n",
    "final_df['patel_pulse_flag'] = (\n",
    "    (final_df['min_heart_rate'] >= 40) & (final_df['max_heart_rate'] <= 130)\n",
    ").astype(int)\n",
    "\n",
    "# 4. Respiratory rate: 5-40 breaths per minute\n",
    "final_df['patel_resp_rate_flag'] = (\n",
    "    (final_df['min_respiratory_rate'] >= 5) & (final_df['max_respiratory_rate'] <= 40)\n",
    ").astype(int)\n",
    "\n",
    "# 5. Pulse oximetry (SpO2): ≥ 88%\n",
    "final_df['patel_spo2_flag'] = (\n",
    "    final_df['min_spo2'] >= 88\n",
    ").astype(int)\n",
    "\n",
    "# Resp flag: Combines respiratory rate and SpO2 criteria\n",
    "final_df['patel_resp_flag'] = (\n",
    "    final_df['patel_resp_rate_flag'] &\n",
    "    final_df['patel_spo2_flag'] &\n",
    "    (final_df['hourly_trach'] == 0) &\n",
    "    (final_df['paralytics_flag'] == 0)\n",
    ").astype(int)\n",
    "\n",
    "# Cardio flag: Combines MAP, SBP, and Pulse criteria\n",
    "final_df['patel_cardio_flag'] = (\n",
    "    final_df['patel_map_flag'] &\n",
    "    final_df['patel_sbp_flag'] &\n",
    "    final_df['patel_pulse_flag'] &\n",
    "    (final_df['hourly_trach'] == 0) &\n",
    "    (final_df['paralytics_flag'] == 0)\n",
    ").astype(int)\n",
    "\n",
    "# Step 2: Create the overall Patel flag\n",
    "final_df['patel_flag'] = (\n",
    "    final_df['patel_map_flag'] &\n",
    "    final_df['patel_sbp_flag'] &\n",
    "    final_df['patel_pulse_flag'] &\n",
    "    final_df['patel_resp_rate_flag'] &\n",
    "    final_df['patel_spo2_flag'] &\n",
    "    (final_df['hourly_trach'] == 0) &\n",
    "    (final_df['paralytics_flag'] == 0) &\n",
    "    (final_df['recorded_hour'] >= 8) &\n",
    "    (final_df['recorded_hour'] < 17) &\n",
    "    (final_df['time_from_vent_adjusted'] != -1)\n",
    ").astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEAM criteria\n",
    "\n",
    "Cardio\n",
    "* Heart rate: ≤ 150 bpm\n",
    "* Most recent lactate: ≤ 4.0 mmol/L\n",
    "* Noradrenaline infusion rate: <0.2 mcg/kg/min or if infusion rate has increased by more than 25% in the last 6 hours, dose must be <0.1 mcg/kg/min.\n",
    "Respiratory\n",
    "* Sufficient respiratory stability:\n",
    "    *  FiO2: ≤ 0.6\n",
    "    *  PEEP: ≤ 16 cm H2O (use peep_observed)\n",
    "* Current respiratory rate: ≤ 45 (use resp_rate_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Heart rate: ≤ 150 bpm\n",
    "final_df['team_pulse_flag'] = np.where(\n",
    "    final_df['max_heart_rate'].isna(),\n",
    "    1,\n",
    "    (final_df['max_heart_rate'] <= 150).astype(int)\n",
    ")\n",
    "\n",
    "# 2. Most recent lactate: ≤ 4.0 mmol/L\n",
    "final_df['team_lactate_flag'] = np.where(\n",
    "    final_df['lactate'].isna(),\n",
    "    1,\n",
    "    (final_df['lactate'] <= 4.0).astype(int)\n",
    ")\n",
    "\n",
    "# 3. Noradrenaline infusion rate: <0.2 mcg/kg/min \n",
    "# final_df['team_ne_flag'] = np.where(\n",
    "#     final_df['ne_calc_max'].isna(),\n",
    "#     1,\n",
    "#     (final_df['ne_calc_max'] <= 0.2).astype(int)\n",
    "# )\n",
    "\n",
    "# final_df['team_ne_flag'] = (\n",
    "#     # (final_df['ne_calc_min'] >= 0.1) & (final_df['ne_calc_max'] <= 0.2)\n",
    "#     final_df['ne_calc_max'] <= 0.2\n",
    "# ).astype(int)\n",
    "\n",
    "# print the number of team_ne_flag == 1\n",
    "# print(\"TEAM NE flag counts when ne < 0.2\\n\", final_df['team_ne_flag'].value_counts(), \"\\n\")\n",
    " \n",
    "# #3b. set the flag to 0 if infusion rate has increased by more than 25% in the last 6 hours and the dose is >0.1 mcg/kg/min.\n",
    "# final_df['team_ne_flag'] = np.where(\n",
    "#     (final_df['ne_calc_max'] > 1.25 * final_df['min_ne_dose_last_6_hours']) & (final_df['ne_calc_max'] > 0.1),\n",
    "#     0,\n",
    "#     final_df['team_ne_flag']\n",
    "# )\n",
    "\n",
    "\n",
    "final_df['team_ne_flag'] = np.where(\n",
    "    final_df['ne_calc_last'].isna(),\n",
    "    1,\n",
    "    (final_df['ne_calc_last'] <= 0.2).astype(int)\n",
    ")\n",
    "\n",
    "# print the number of team_ne_flag == 1\n",
    "print(\"TEAM NE flag counts when ne < 0.2\\n\", final_df['team_ne_flag'].value_counts(), \"\\n\")\n",
    " \n",
    "#3b. set the flag to 0 if infusion rate has increased by more than 25% in the last 6 hours and the dose is >0.1 mcg/kg/min.\n",
    "final_df['team_ne_flag'] = np.where(\n",
    "    (final_df['ne_calc_last'] > 1.25 * final_df['last_ne_dose_last_6_hours']) & (final_df['ne_calc_last'] > 0.1),\n",
    "    0,\n",
    "    final_df['team_ne_flag']\n",
    ")\n",
    "print(\"TEAM NE flag counts adjusting for change in the last 6 hrs\\n\", final_df['team_ne_flag'].value_counts(), \"\\n\")\n",
    "\n",
    "# 4. Sufficient respiratory stability:\n",
    "#    a. FiO2: ≤ 0.6\n",
    "final_df['team_fio2_flag'] = np.where(\n",
    "    final_df['min_fio2_set'].isna(),\n",
    "    1,\n",
    "    (final_df['min_fio2_set'] <= 0.6).astype(int)\n",
    ")\n",
    "\n",
    "#    b. PEEP: ≤ 16 cm H2O\n",
    "final_df['team_peep_flag'] = np.where(\n",
    "    final_df['max_peep_set'].isna(),\n",
    "    1,\n",
    "    (final_df['max_peep_set'] <= 16).astype(int)\n",
    ")\n",
    "\n",
    "# 5. Current respiratory rate: ≤ 45\n",
    "final_df['team_resp_rate_flag'] = np.where(\n",
    "    final_df['max_respiratory_rate'].isna(),\n",
    "    1,\n",
    "    (final_df['max_respiratory_rate'] <= 45).astype(int)\n",
    ")\n",
    "\n",
    "# Cardio flag: Combines heart rate, lactate, and norepinephrine criteria\n",
    "final_df['team_cardio_flag'] = (\n",
    "    final_df['team_pulse_flag'] &\n",
    "    final_df['team_lactate_flag'] &\n",
    "    final_df['team_ne_flag'] &\n",
    "    (final_df['hourly_trach'] == 0) &\n",
    "    (final_df['paralytics_flag'] == 0)\n",
    ").astype(int)\n",
    "\n",
    "# Resp flag: Combines FiO2, PEEP, and respiratory rate criteria\n",
    "final_df['team_resp_flag'] = (\n",
    "    final_df['team_fio2_flag'] &\n",
    "    final_df['team_peep_flag'] &\n",
    "    final_df['team_resp_rate_flag'] &\n",
    "    (final_df['hourly_trach'] == 0) &\n",
    "    (final_df['paralytics_flag'] == 0)\n",
    ").astype(int)\n",
    "\n",
    "\n",
    "# Create the overall TEAM flag\n",
    "final_df['team_flag'] = (\n",
    "    final_df['team_pulse_flag'] &\n",
    "    final_df['team_lactate_flag'] &\n",
    "    final_df['team_ne_flag'] &\n",
    "    final_df['team_fio2_flag'] &\n",
    "    final_df['team_peep_flag'] &\n",
    "    final_df['team_resp_rate_flag'] &\n",
    "    (final_df['hourly_trach'] == 0) &\n",
    "    (final_df['paralytics_flag'] == 0) & \n",
    "    (final_df['recorded_hour'] >= 8) &\n",
    "    (final_df['recorded_hour'] < 17) &\n",
    "    (final_df['time_from_vent_adjusted'] != -1)\n",
    ").astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consensus criteria\n",
    "\n",
    "* Green Criteria\n",
    "    * Respiratory\n",
    "        * Saturation  90% and\n",
    "        * Respiratory rate ≤ 30 breaths/min\n",
    "        * Current FiO2 ≤ 0.6 and\n",
    "        * PEEP≤ 10cm H20\n",
    "    * Cardiovascular:\n",
    "        * Blood pressure greater than lower limit of target range (MAP 65+) while on no or low level of support (low support- define as <0.1 μg/kg/min of Norepi equivalents)\n",
    "        * Heart rate <120 beats/min\n",
    "        * lactate < 4mmol/L\n",
    "        * HR > 40\n",
    "* Yellow Criteria\n",
    "    * Respiratory\n",
    "        * Sat >= 90%\n",
    "        * Current FiO2 >0.6\n",
    "        * Respiratory rate >30breaths/min\n",
    "        * PEEP >10cm H20\n",
    "    * Cardiovascular\n",
    "        * Blood pressure greater than lower limit of target range (MAP 65+) while receiving moderate level of support (medium-define as 0.1–0.3 μg/kg/min of Norepi equivalents)\n",
    "        * Heart rate 120-150 beats/min\n",
    "        * Shock of any cause with lactate >4mmol/L\n",
    "        * HR > 40\n",
    "* Red Criteria\n",
    "    * Respiratory\n",
    "        * Sat <90%\n",
    "    * Cardiovascular\n",
    "        * Below target MAP despite support (MAP <65) or\n",
    "        * greater than lower limit MAP (MAP 65+) but on high level support (high defined as >0.3 μg/kg/min of Norepi equivalents)\n",
    "        * IV therapy for hypertensive emergency (SBP >200mmHg or MAP >110 and on nicardipine, nitroprusside, or clevidipine gtt)\n",
    "        * HR >150 bpm\n",
    "        * Bradycardia <40\n",
    "\n",
    "\n",
    "### Consensus criteria - redefined \n",
    "\n",
    "* all_red: All red subcomponents must be met.\n",
    "* all_green: All green subcomponents must be met, and no red subcomponents are met.\n",
    "* all_yellow: All yellow subcomponents must be met, no red subcomponents are met, and all green subcomponents are not met.\n",
    "* any_yellow: Any yellow subcomponent is met, no green subcomponents are fully met, and no red subcomponents are met.\n",
    "* any_yellow_or_green_no_red: Any yellow or green subcomponents are met, but no red subcomponents are met.\n",
    "* no_red: No red criteria is met"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Red Cardiovascular Criteria\n",
    "final_df['red_resp_spo2_flag'] = ((final_df['min_spo2'] < 90) | final_df['min_spo2'].isna()).astype(int)\n",
    "final_df['red_map_flag'] = ((final_df['min_map'] < 65) | final_df['min_map'].isna()).astype(int)\n",
    "\n",
    "# High support (Norepinephrine equivalents > 0.3 μg/kg/min)\n",
    "final_df['red_high_support_flag'] = ((final_df['ne_calc_last'] > 0.3)).astype(int)\n",
    "\n",
    "# Hypertensive emergency criteria (SBP > 200 mmHg or MAP > 110 mmHg and on certain medications)\n",
    "final_df['red_hypertensive_flag'] = (\n",
    "    (((final_df['max_sbp'] > 200) | (final_df['max_map'] > 110)) &\n",
    "    (final_df['red_meds_flag'] == 1)) \n",
    ").astype(int)\n",
    "\n",
    "# High heart rate criteria (HR > 150 bpm)\n",
    "final_df['red_pulse_high_flag'] = ((final_df['max_heart_rate'] > 150)).astype(int)\n",
    "# Low heart rate criteria (HR < 40 bpm)\n",
    "final_df['red_pulse_low_flag'] = ((final_df['min_heart_rate'] < 40) | final_df['min_heart_rate'].isna()).astype(int)\n",
    "\n",
    "# Yellow Respiratory Criteria\n",
    "final_df['yellow_resp_spo2_flag'] = ((final_df['min_spo2'] >= 90)).astype(int)\n",
    "final_df['yellow_fio2_flag'] = ((final_df['min_fio2_set'] > 0.6)).astype(int)\n",
    "final_df['yellow_resp_rate_flag'] = ((final_df['max_resp_rate_obs'] > 30)).astype(int)\n",
    "final_df['yellow_peep_flag'] = ((final_df['min_peep_set'] > 10)).astype(int)\n",
    "\n",
    "# Yellow Cardiovascular Criteria\n",
    "final_df['yellow_map_flag'] = (((final_df['min_map'] >= 65) & (final_df['ne_calc_last'].between(0.1, 0.3)))).astype(int)\n",
    "final_df['yellow_pulse_flag'] = ((final_df['min_heart_rate'].between(120, 150))).astype(int)\n",
    "final_df['yellow_lactate_flag'] = ((final_df['lactate'] > 4) | final_df['lactate'].isna()).astype(int)\n",
    "\n",
    "# Step 3: Implement Green Criteria\n",
    "final_df['green_resp_spo2_flag'] = ((final_df['min_spo2'] >= 90)).astype(int)\n",
    "final_df['green_resp_rate_flag'] = ((final_df['max_resp_rate_obs'] <= 30) | final_df['max_resp_rate_obs'].isna()).astype(int)\n",
    "final_df['green_fio2_flag'] = ((final_df['min_fio2_set'] <= 0.6) | final_df['min_fio2_set'].isna()).astype(int)\n",
    "final_df['green_peep_flag'] = ((final_df['min_peep_set'] <= 10) | final_df['min_peep_set'].isna()).astype(int)\n",
    "\n",
    "# Green Cardiovascular Criteria\n",
    "final_df['green_map_flag'] = ((final_df['min_map'] >= 65) & (final_df['ne_calc_last'] < 0.1) | final_df['min_map'].isna() | final_df['ne_calc_last'].isna()).astype(int)\n",
    "final_df['green_pulse_flag'] = ((final_df['min_heart_rate'] < 120) | final_df['min_heart_rate'].isna()).astype(int)\n",
    "final_df['green_lactate_flag'] = ((final_df['lactate'] < 4) | final_df['lactate'].isna()).astype(int)\n",
    "final_df['green_hr_flag'] = ((final_df['min_heart_rate'] > 40) | final_df['min_heart_rate'].isna()).astype(int)\n",
    "\n",
    "final_df['any_red'] = (\n",
    "    (final_df['red_resp_spo2_flag'] |\n",
    "    final_df['red_map_flag'] |\n",
    "    final_df['red_high_support_flag'] |\n",
    "    final_df['red_hypertensive_flag'] |\n",
    "    final_df['red_pulse_high_flag'] |\n",
    "    final_df['red_pulse_low_flag']) &\n",
    "    (final_df['hourly_trach'] == 0) &\n",
    "    (final_df['paralytics_flag'] == 0) &\n",
    "    (final_df['recorded_hour'] >= 8) &\n",
    "    (final_df['recorded_hour'] < 17) &\n",
    "    (final_df['time_from_vent_adjusted'] != -1)\n",
    ").astype(int)\n",
    "\n",
    "final_df['no_red'] = (~(final_df['red_resp_spo2_flag'] |\n",
    "       final_df['red_map_flag'] |\n",
    "       final_df['red_high_support_flag'] |\n",
    "       final_df['red_hypertensive_flag'] |\n",
    "       final_df['red_pulse_high_flag'] |\n",
    "       final_df['red_pulse_low_flag']) &\n",
    "    (final_df['hourly_trach'] == 0) &\n",
    "    (final_df['paralytics_flag'] == 0) &\n",
    "    (final_df['recorded_hour'] >= 8) &\n",
    "    (final_df['recorded_hour'] < 17) &\n",
    "    (final_df['time_from_vent_adjusted'] != -1)\n",
    ").astype(int)\n",
    "\n",
    "final_df['any_yellow'] = (\n",
    "    (final_df['yellow_resp_spo2_flag'] |\n",
    "    final_df['yellow_fio2_flag'] |\n",
    "    final_df['yellow_resp_rate_flag'] |\n",
    "    final_df['yellow_peep_flag'] |\n",
    "    final_df['yellow_map_flag'] |\n",
    "    final_df['yellow_pulse_flag'] |\n",
    "    final_df['yellow_lactate_flag']) &\n",
    "    (final_df['hourly_trach'] == 0) &\n",
    "    (final_df['paralytics_flag'] == 0) &\n",
    "    (final_df['recorded_hour'] >= 8) &\n",
    "    (final_df['recorded_hour'] < 17) &\n",
    "    (final_df['time_from_vent_adjusted'] != -1)\n",
    ").astype(int)\n",
    "\n",
    "final_df['any_green'] = (\n",
    "    (final_df['green_resp_spo2_flag'] |\n",
    "    final_df['green_resp_rate_flag'] |\n",
    "    final_df['green_fio2_flag'] |\n",
    "    final_df['green_peep_flag'] |\n",
    "    final_df['green_map_flag'] |\n",
    "    final_df['green_pulse_flag'] |\n",
    "    final_df['green_lactate_flag'] |\n",
    "    final_df['green_hr_flag']) &\n",
    "    (final_df['hourly_trach'] == 0) &\n",
    "    (final_df['paralytics_flag'] == 0) &\n",
    "    (final_df['recorded_hour'] >= 8) &\n",
    "    (final_df['recorded_hour'] < 17) &\n",
    "    (final_df['time_from_vent_adjusted'] != -1)\n",
    ").astype(int)\n",
    "\n",
    "final_df['all_green'] = (\n",
    "    final_df['green_resp_spo2_flag'] &\n",
    "    final_df['green_resp_rate_flag'] &\n",
    "    final_df['green_fio2_flag'] &\n",
    "    final_df['green_peep_flag'] &\n",
    "    final_df['green_map_flag'] &\n",
    "    final_df['green_pulse_flag'] &\n",
    "    final_df['green_lactate_flag'] &\n",
    "    final_df['green_hr_flag'] &\n",
    "    (final_df['hourly_trach'] == 0) &\n",
    "    (final_df['paralytics_flag'] == 0) &\n",
    "    (final_df['recorded_hour'] >= 8) &\n",
    "    (final_df['recorded_hour'] < 17) &\n",
    "    (final_df['time_from_vent_adjusted'] != -1)\n",
    ").astype(int)\n",
    "\n",
    "final_df['all_green_no_red'] = (\n",
    "    final_df['green_resp_spo2_flag'] &\n",
    "    final_df['green_resp_rate_flag'] &\n",
    "    final_df['green_fio2_flag'] &\n",
    "    final_df['green_peep_flag'] &\n",
    "    final_df['green_map_flag'] &\n",
    "    final_df['green_pulse_flag'] &\n",
    "    final_df['green_lactate_flag'] &\n",
    "    final_df['green_hr_flag'] &\n",
    "    (final_df['any_red'] == 0) &\n",
    "    (final_df['hourly_trach'] == 0) &\n",
    "    (final_df['paralytics_flag'] == 0) &\n",
    "    (final_df['recorded_hour'] >= 8) &\n",
    "    (final_df['recorded_hour'] < 17) &\n",
    "    (final_df['time_from_vent_adjusted'] != -1)\n",
    ").astype(int)\n",
    "\n",
    "final_df['all_green_no_red_yellow'] = (\n",
    "    final_df['green_resp_spo2_flag'] &\n",
    "    final_df['green_resp_rate_flag'] &\n",
    "    final_df['green_fio2_flag'] &\n",
    "    final_df['green_peep_flag'] &\n",
    "    final_df['green_map_flag'] &\n",
    "    final_df['green_pulse_flag'] &\n",
    "    final_df['green_lactate_flag'] &\n",
    "    final_df['green_hr_flag'] &\n",
    "    (final_df['any_red'] == 0) &\n",
    "    (final_df['any_yellow'] == 0) &\n",
    "    (final_df['hourly_trach'] == 0) &\n",
    "    (final_df['paralytics_flag'] == 0) &\n",
    "    (final_df['recorded_hour'] >= 8) &\n",
    "    (final_df['recorded_hour'] < 17) &\n",
    "    (final_df['time_from_vent_adjusted'] != -1)\n",
    ").astype(int)\n",
    "\n",
    "final_df['all_yellow_no_red_green'] = (\n",
    "    final_df['yellow_resp_spo2_flag'] &\n",
    "    final_df['yellow_fio2_flag'] &\n",
    "    final_df['yellow_resp_rate_flag'] &\n",
    "    final_df['yellow_peep_flag'] &\n",
    "    final_df['yellow_map_flag'] &\n",
    "    final_df['yellow_pulse_flag'] &\n",
    "    final_df['yellow_lactate_flag'] &\n",
    "    (final_df['any_red'] == 0) &\n",
    "    (final_df['any_green'] == 0) &\n",
    "    (final_df['hourly_trach'] == 0) &\n",
    "    (final_df['paralytics_flag'] == 0) &\n",
    "    (final_df['recorded_hour'] >= 8) &\n",
    "    (final_df['recorded_hour'] < 17) &\n",
    "    (final_df['time_from_vent_adjusted'] != -1)\n",
    ").astype(int)\n",
    "\n",
    "final_df['any_yellow_no_red_green'] = (\n",
    "    (final_df['yellow_resp_spo2_flag'] |\n",
    "    final_df['yellow_fio2_flag'] |\n",
    "    final_df['yellow_resp_rate_flag'] |\n",
    "    final_df['yellow_peep_flag'] |\n",
    "    final_df['yellow_map_flag'] |\n",
    "    final_df['yellow_pulse_flag'] |\n",
    "    final_df['yellow_lactate_flag']) &\n",
    "    (final_df['any_red'] == 0) &\n",
    "    (final_df['any_green'] == 0) &\n",
    "    (final_df['hourly_trach'] == 0) &\n",
    "    (final_df['paralytics_flag'] == 0) &\n",
    "    (final_df['recorded_hour'] >= 8) &\n",
    "    (final_df['recorded_hour'] < 17) &\n",
    "    (final_df['time_from_vent_adjusted'] != -1)\n",
    ").astype(int)\n",
    "\n",
    "final_df['any_yellow_or_green_no_red'] = (\n",
    "    (final_df['yellow_resp_spo2_flag'] |\n",
    "    final_df['yellow_fio2_flag'] |\n",
    "    final_df['yellow_resp_rate_flag'] |\n",
    "    final_df['yellow_peep_flag'] |\n",
    "    final_df['yellow_map_flag'] |\n",
    "    final_df['yellow_pulse_flag'] |\n",
    "    final_df['yellow_lactate_flag'] |\n",
    "    final_df['green_resp_spo2_flag'] |\n",
    "    final_df['green_resp_rate_flag'] |\n",
    "    final_df['green_fio2_flag'] |\n",
    "    final_df['green_peep_flag'] |\n",
    "    final_df['green_map_flag'] |\n",
    "    final_df['green_pulse_flag'] |\n",
    "    final_df['green_lactate_flag'] |\n",
    "    final_df['green_hr_flag']) &\n",
    "    (final_df['any_red'] == 0) &\n",
    "    (final_df['hourly_trach'] == 0) &\n",
    "    (final_df['paralytics_flag'] == 0) &\n",
    "    (final_df['recorded_hour'] >= 8) &\n",
    "    (final_df['recorded_hour'] < 17) &\n",
    "    (final_df['time_from_vent_adjusted'] != -1)\n",
    ").astype(int)\n",
    "\n",
    "final_df['yellow_resp_flag'] = (\n",
    "    (final_df['yellow_resp_spo2_flag'] |\n",
    "    final_df['yellow_fio2_flag'] |\n",
    "    final_df['yellow_resp_rate_flag'] |\n",
    "    final_df['yellow_peep_flag'] |\n",
    "    final_df['green_resp_spo2_flag'] |\n",
    "    final_df['green_resp_rate_flag'] |\n",
    "    final_df['green_fio2_flag'] |\n",
    "    final_df['green_peep_flag']) &\n",
    "    (final_df['any_red'] == 0) &\n",
    "    (final_df['hourly_trach'] == 0) &\n",
    "    (final_df['paralytics_flag'] == 0) &\n",
    "    (final_df['recorded_hour'] >= 8) &\n",
    "    (final_df['recorded_hour'] < 17) &\n",
    "    (final_df['time_from_vent_adjusted'] != -1)\n",
    ").astype(int)\n",
    "\n",
    "final_df['yellow_cardio_flag'] = (\n",
    "    (final_df['yellow_map_flag'] |\n",
    "    final_df['yellow_pulse_flag'] |\n",
    "    final_df['yellow_lactate_flag'] |\n",
    "    final_df['green_map_flag'] |\n",
    "    final_df['green_pulse_flag'] |\n",
    "    final_df['green_lactate_flag'] |\n",
    "    final_df['green_hr_flag']) &\n",
    "    (final_df['any_red'] == 0) &\n",
    "    (final_df['hourly_trach'] == 0) &\n",
    "    (final_df['paralytics_flag'] == 0) &\n",
    "    (final_df['recorded_hour'] >= 8) &\n",
    "    (final_df['recorded_hour'] < 17) &\n",
    "    (final_df['time_from_vent_adjusted'] != -1)\n",
    ").astype(int)\n",
    "\n",
    "final_df['yellow_all_green'] = (\n",
    "    final_df['all_green_no_red'] &\n",
    "    (final_df['any_yellow'] == 0)\n",
    ").astype(int)\n",
    "\n",
    "final_df['yellow_not_all_green'] = (\n",
    "    final_df['any_yellow_or_green_no_red'] &\n",
    "    (final_df['all_green_no_red'] == 0)\n",
    ").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print value counts for each flag\n",
    "print(final_df[['any_red', 'any_yellow', 'any_green' ,  'all_green',\n",
    "                'all_green_no_red', 'all_green_no_red_yellow', 'all_yellow_no_red_green', \n",
    "                'any_yellow_no_red_green','any_yellow_or_green_no_red','no_red' ,'yellow_all_green',\n",
    "                 'yellow_not_all_green' ]].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_parquet(f'../output/intermediate/final_df_w_criteria{datetime.now().date()}.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TableOne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create criteria_results df with one row per encounter showing if criteria were ever met\n",
    "criteria_block_results = final_df.groupby('encounter_block').agg({\n",
    "    'patel_flag': 'max',  # 1 if criteria ever met\n",
    "    'team_flag': 'max',\n",
    "    'any_yellow_or_green_no_red': 'max',\n",
    "    'all_green': 'max'\n",
    "}).reset_index()\n",
    "final_df_blocks = final_df_blocks.merge(criteria_block_results, on='encounter_block', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  aggregate the values from final_df at encounter_block level\n",
    "vaso_peep_fio2_stats = final_df.groupby('encounter_block').agg({\n",
    "    'ne_calc_last': 'max',  # If any value is > 0, the block received vasopressors\n",
    "    'max_peep_set': 'mean',  # Average of max PEEP in the block\n",
    "    'min_fio2_set': 'mean'   # Average of min FiO2 in the block\n",
    "}).reset_index()\n",
    "\n",
    "# Merge these stats with final_df_blocks\n",
    "all_encounters = final_df_blocks.copy()\n",
    "all_encounters = pd.merge(all_encounters, vaso_peep_fio2_stats, on='encounter_block', how='left')\n",
    "\n",
    "# Create subsets and map race for each\n",
    "def map_race_column(df, race_column='race'):\n",
    "    race_mapping = {\n",
    "        'Black or African-American': 'Black',\n",
    "        'Black or African American': 'Black',\n",
    "        'White': 'White',\n",
    "        'Asian': 'Other',\n",
    "        'American Indian or Alaska Native': 'Other',\n",
    "        'Native Hawaiian or Other Pacific Islander': 'Other',\n",
    "        'Other': 'Other',\n",
    "        'Unknown': 'Other'\n",
    "    }\n",
    "    df['race_new'] = df[race_column].map(race_mapping).fillna('Missing')\n",
    "    return df\n",
    "\n",
    "# Map race and create subsets\n",
    "all_encounters = map_race_column(all_encounters, 'race_category')\n",
    "patel_subset = map_race_column(all_encounters[all_encounters['patel_flag'] == 1].copy(), 'race_category')\n",
    "team_subset = map_race_column(all_encounters[all_encounters['team_flag'] == 1].copy(), 'race_category')\n",
    "yellow_subset = map_race_column(all_encounters[all_encounters['any_yellow_or_green_no_red'] == 1].copy(), 'race_category')\n",
    "green_subset = map_race_column(all_encounters[all_encounters['all_green'] == 1].copy(), 'race_category')\n",
    "\n",
    "# Calculate vasopressor usage for each subset\n",
    "def calculate_vasopressor_stats(df):\n",
    "    # Count encounters with any vasopressor use (ne_calc_last > 0)\n",
    "    vaso_usage = df['ne_calc_last'].notna() & (df['ne_calc_last'] > 0)\n",
    "    n_vaso = vaso_usage.sum()\n",
    "    n_zero = (df['ne_calc_last'] == 0).sum()\n",
    "    n_missing = df['ne_calc_last'].isna().sum()\n",
    "    total = len(df)\n",
    "    return n_vaso, n_zero, n_missing, total\n",
    "\n",
    "# Calculate stats for each group\n",
    "vaso_stats = {\n",
    "    'All Encounters': calculate_vasopressor_stats(all_encounters),\n",
    "    'Patel Criteria': calculate_vasopressor_stats(patel_subset),\n",
    "    'TEAM Criteria': calculate_vasopressor_stats(team_subset),\n",
    "    'Yellow Criteria': calculate_vasopressor_stats(yellow_subset),\n",
    "    'Green Criteria': calculate_vasopressor_stats(green_subset)\n",
    "}\n",
    "\n",
    "# Define variables for the table\n",
    "categorical = ['sex_category', 'race_new', 'ethnicity_category', \n",
    "              'location_category', 'is_dead']\n",
    "\n",
    "continuous = ['age_at_admission', 'sofa_cv_97', 'sofa_coag', 'sofa_renal',\n",
    "             'sofa_liver', 'sofa_resp', 'sofa_cns', 'sofa_total',\n",
    "             'ne_calc_last', 'max_peep_set', 'min_fio2_set']\n",
    "\n",
    "# Create individual tables\n",
    "# All Encounters - This will be our template\n",
    "table_all = TableOne(all_encounters, \n",
    "                    columns=categorical + continuous,\n",
    "                    categorical=categorical,\n",
    "                    groupby=None,\n",
    "                    nonnormal=continuous,\n",
    "                    pval=False)\n",
    "df_all = table_all.tableone.reset_index()\n",
    "\n",
    "# Filter out the 'n' row from the template\n",
    "# df_all = df_all[~((df_all['level_0'] == 'n') & (df_all['level_1'].isna()))]\n",
    "\n",
    "# Get the last column and the index columns\n",
    "df_template = pd.DataFrame({\n",
    "    'Characteristics': df_all['level_0'],\n",
    "    'Category': df_all['level_1'],\n",
    "    'All Encounters': df_all[df_all.columns[-1]]\n",
    "})\n",
    "\n",
    "# Function to process each criteria subset\n",
    "def process_criteria_subset(subset_df, criteria_name, template):\n",
    "    table = TableOne(subset_df,\n",
    "                    columns=categorical + continuous,\n",
    "                    categorical=categorical,\n",
    "                    groupby=None,\n",
    "                    nonnormal=continuous,\n",
    "                    pval=False)\n",
    "    df = table.tableone.reset_index()\n",
    "    \n",
    "    # Filter out the 'n' row\n",
    "    df = df[~((df['level_0'] == 'n') & (df['level_1'].isna()))]\n",
    "    \n",
    "    # Create a DataFrame with the same structure as template\n",
    "    result = pd.DataFrame({\n",
    "        'Characteristics': df['level_0'],\n",
    "        'Category': df['level_1'],\n",
    "        criteria_name: df[df.columns[-1]]\n",
    "    })\n",
    "    \n",
    "    # Merge with template to ensure all categories are present\n",
    "    merged = pd.merge(template[['Characteristics', 'Category']], \n",
    "                     result,\n",
    "                     on=['Characteristics', 'Category'],\n",
    "                     how='left')\n",
    "    \n",
    "    return merged[criteria_name]\n",
    "\n",
    "# Process each criteria subset\n",
    "patel_col = process_criteria_subset(patel_subset, 'Patel Criteria', df_template)\n",
    "team_col = process_criteria_subset(team_subset, 'TEAM Criteria', df_template)\n",
    "yellow_col = process_criteria_subset(yellow_subset, 'Yellow Criteria', df_template)\n",
    "green_col = process_criteria_subset(green_subset, 'Green Criteria', df_template)\n",
    "\n",
    "# Combine all columns\n",
    "final_table = pd.concat([\n",
    "    df_template[['Characteristics', 'Category', 'All Encounters']],\n",
    "    patel_col,\n",
    "    team_col,\n",
    "    yellow_col,\n",
    "    green_col\n",
    "], axis=1)\n",
    "\n",
    "# Clean up the table\n",
    "# Remove the 'Missing' category if it exists and has count of 0\n",
    "final_table = final_table[~((final_table['Category'] == 'Missing') & \n",
    "                          (final_table['All Encounters'].str.startswith('0')))]\n",
    "\n",
    "# Format mortality rows\n",
    "mortality_rows = final_table.loc[final_table['Characteristics'] == 'is_dead']\n",
    "for col in final_table.columns[2:]:  # Skip 'Characteristics' and 'Category'\n",
    "    if col == 'All Encounters':\n",
    "        total = len(all_encounters)\n",
    "        deaths = all_encounters['is_dead'].sum()\n",
    "    elif col == 'Patel Criteria':\n",
    "        total = len(patel_subset)\n",
    "        deaths = patel_subset['is_dead'].sum()\n",
    "    elif col == 'TEAM Criteria':\n",
    "        total = len(team_subset)\n",
    "        deaths = team_subset['is_dead'].sum()\n",
    "    elif col == 'Yellow Criteria':\n",
    "        total = len(yellow_subset)\n",
    "        deaths = yellow_subset['is_dead'].sum()\n",
    "    else:  # Green Criteria\n",
    "        total = len(green_subset)\n",
    "        deaths = green_subset['is_dead'].sum()\n",
    "    \n",
    "    percentage = (deaths / total * 100) if total > 0 else 0\n",
    "    mortality_rows.loc[mortality_rows['Category'] == '1', col] = f\"{deaths} ({percentage:.1f})\"\n",
    "\n",
    "# Replace the original mortality rows\n",
    "final_table.loc[final_table['Characteristics'] == 'is_dead'] = mortality_rows\n",
    "\n",
    "# Clean up labels\n",
    "final_table.loc[final_table['Characteristics'] == 'is_dead', 'Characteristics'] = 'Mortality'\n",
    "final_table.loc[final_table['Category'] == '1', 'Category'] = ''\n",
    "\n",
    "# Add vasopressor usage rows\n",
    "vaso_rows = []\n",
    "for status in ['Received Vasopressors', 'No Vasopressors', 'Missing Vasopressor Data']:\n",
    "    row_data = {'Characteristics': 'Vasopressor Status', 'Category': status}\n",
    "    for col in final_table.columns[2:]:  # Skip 'Characteristics' and 'Category'\n",
    "        if col in vaso_stats:\n",
    "            n_vaso, n_zero, n_missing, total = vaso_stats[col]\n",
    "            if status == 'Received Vasopressors':\n",
    "                value = n_vaso\n",
    "            elif status == 'No Vasopressors':\n",
    "                value = n_zero\n",
    "            else:  # Missing Vasopressor Data\n",
    "                value = n_missing\n",
    "            \n",
    "            percentage = (value / total * 100) if total > 0 else 0\n",
    "            row_data[col] = f\"{value} ({percentage:.1f})\"\n",
    "    vaso_rows.append(row_data)\n",
    "\n",
    "vaso_df = pd.DataFrame(vaso_rows)\n",
    "final_table = pd.concat([final_table, vaso_df], ignore_index=True)\n",
    "\n",
    "# Add n row at the top (only once)\n",
    "n_row = pd.DataFrame({\n",
    "    'Characteristics': ['n'],\n",
    "    'Category': [''],\n",
    "    'All Encounters': [str(len(all_encounters))],\n",
    "    'Patel Criteria': [str(len(patel_subset))],\n",
    "    'TEAM Criteria': [str(len(team_subset))],\n",
    "    'Yellow Criteria': [str(len(yellow_subset))],\n",
    "    'Green Criteria': [str(len(green_subset))]\n",
    "})\n",
    "\n",
    "final_table = pd.concat([n_row, final_table]).reset_index(drop=True)\n",
    "\n",
    "# Save to CSV\n",
    "final_table.to_csv('../output/final/table1_results.csv', index=False)\n",
    "\n",
    "print(\"Table 1 has been generated and saved to table1_results.csv\")\n",
    "final_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restrict to business hours in the first 72 hours after intubation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note: `time_from_vent_adjusted` is -1 until hour 4, then it counts up from 0. This builds in the 4-hour cool off period.\n",
    "business_hours_df = final_df[(final_df['time_from_vent_adjusted'] >= 0) & (final_df['time_from_vent_adjusted'] < 72)]\n",
    "#business_hours_df = final_df[(final_df['time_from_vent'] >= 0) & (final_df['time_from_vent'] < 72)]\n",
    "\n",
    "# recorded_hour is the hour of the day (0-23), so business hours are 8 (8 AM) - 17 (5 PM).\n",
    "business_hours_df = business_hours_df[(business_hours_df['recorded_hour'] >= 8) & (business_hours_df['recorded_hour'] < 17)].copy()\n",
    "business_hours_df['time_biz'] = business_hours_df.groupby('hospitalization_id').cumcount()\n",
    "\n",
    "business_hours_df.head(n = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Some encounters are dropped when there are no business hours are availble\n",
    "pyCLIF.count_unique_encounters(business_hours_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table1_business_hours = pyCLIF.generate_table_one_new(business_hours_df, all_ids_w_outcome, filename=\"table1_business_hours\")\n",
    "# table1_business_hours\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missingess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_variables =['encounter_block','hospitalization_id', 'recorded_dttm',\t\n",
    "                'recorded_date'\t,'recorded_hour', 'time_from_vent',\n",
    "                'hourly_trach','paralytics_flag',]\n",
    "\n",
    "reqd_team_fields = ['hourly_trach','paralytics_flag',\n",
    "                    'lactate', 'max_heart_rate', 'ne_calc_last',\n",
    "                    'last_ne_dose_last_6_hours', 'min_fio2_set', 'max_peep_set', \n",
    "                    'max_resp_rate_obs', \"team_pulse_flag\", \"team_lactate_flag\", \"team_ne_flag\",\n",
    "                    \"team_fio2_flag\",  \"team_peep_flag\",    \"team_resp_rate_flag\"]\n",
    "\n",
    "reqd_yellow_fields =[\n",
    "    # Clinical Measurements\n",
    "    'min_spo2', 'min_map', 'max_map', 'ne_calc_last', 'max_sbp',\n",
    "    'max_heart_rate', 'min_heart_rate', 'min_fio2_set',\n",
    "    'max_resp_rate_obs', 'min_peep_set', 'lactate',\n",
    "    \n",
    "    # Red Flags\n",
    "    'red_resp_spo2_flag', 'red_map_flag', 'red_high_support_flag',\n",
    "    'red_hypertensive_flag', 'red_pulse_high_flag', 'red_pulse_low_flag',\n",
    "    'red_meds_flag',\n",
    "    \n",
    "    # Yellow Flags\n",
    "    'yellow_resp_spo2_flag', 'yellow_fio2_flag', 'yellow_resp_rate_flag',\n",
    "    'yellow_peep_flag', 'yellow_map_flag', 'yellow_pulse_flag',\n",
    "    'yellow_lactate_flag',\n",
    "    \n",
    "    # Green Flags\n",
    "    'green_resp_spo2_flag', 'green_resp_rate_flag', 'green_fio2_flag',\n",
    "    'green_peep_flag', 'green_map_flag', 'green_pulse_flag',\n",
    "    'green_lactate_flag', 'green_hr_flag',\n",
    "    \n",
    "    # Composite Flags\n",
    "    'any_red', 'any_yellow', 'any_green', 'all_green',\n",
    "    'all_green_no_red', 'all_green_no_red_yellow',\n",
    "    'all_yellow_no_red_green', 'any_yellow_no_red_green',\n",
    "    'any_yellow_or_green_no_red', 'yellow_resp_flag',\n",
    "    'yellow_cardio_flag', 'yellow_all_green', 'yellow_not_all_green'\n",
    "]\n",
    "\n",
    "reqd_patel_fields = ['min_map', 'max_map','max_sbp', 'min_sbp',\n",
    "                   'min_heart_rate','max_heart_rate', 'min_respiratory_rate','min_spo2', \n",
    "                    'max_respiratory_rate','patel_map_flag','patel_sbp_flag',\n",
    "                    'patel_pulse_flag', 'patel_resp_rate_flag' , 'patel_spo2_flag', \n",
    "                    'patel_resp_flag', 'patel_cardio_flag' ]\n",
    "\n",
    "reqd_green_fields =[\n",
    "    # Clinical Measurements\n",
    "    'min_spo2', 'min_map', 'max_map', 'ne_calc_last', 'max_sbp',\n",
    "    'max_heart_rate', 'min_heart_rate', 'min_fio2_set',\n",
    "    'max_resp_rate_obs', 'min_peep_set', 'lactate',\n",
    "\n",
    "    \n",
    "    # Green Flags\n",
    "    'green_resp_spo2_flag', 'green_resp_rate_flag', 'green_fio2_flag',\n",
    "    'green_peep_flag', 'green_map_flag', 'green_pulse_flag',\n",
    "    'green_lactate_flag', 'green_hr_flag',\n",
    "    \n",
    "    # Composite Flags\n",
    "     'all_green',\n",
    "    'all_green_no_red', \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate percentage of missing values per encounter block\n",
    "def calculate_missing_percentage(df, variable_list, exclude_flags=True):\n",
    "    # Filter out flag variables if requested\n",
    "    if exclude_flags:\n",
    "        vars_to_check = [var for var in variable_list if 'flag' not in var.lower()]\n",
    "    else:\n",
    "        vars_to_check = variable_list\n",
    "        \n",
    "    # Remove key variables that are administrative\n",
    "    vars_to_check = [var for var in vars_to_check if var not in ['encounter_block', 'hospitalization_id', \n",
    "                                                                'recorded_dttm', 'recorded_date', 'recorded_hour', \n",
    "                                                                'time_from_vent', 'hourly_trach', 'paralytics_flag']]\n",
    "    \n",
    "    # Calculate percentage of blocks where variable was never measured\n",
    "    missing_pct = {}\n",
    "    total_blocks = df['encounter_block'].nunique()\n",
    "    \n",
    "    for var in vars_to_check:\n",
    "        blocks_never_measured = df.groupby('encounter_block')[var].apply(lambda x: x.isna().all()).sum()\n",
    "        missing_pct[var] = (blocks_never_measured / total_blocks) * 100\n",
    "        \n",
    "    return pd.Series(missing_pct).sort_values(ascending=False)\n",
    "\n",
    "# Calculate for each criteria set\n",
    "team_missing = calculate_missing_percentage(final_df, reqd_team_fields)\n",
    "yellow_missing = calculate_missing_percentage(final_df, reqd_yellow_fields)\n",
    "patel_missing = calculate_missing_percentage(final_df, reqd_patel_fields)\n",
    "green_missing = calculate_missing_percentage(final_df, reqd_green_fields)\n",
    "\n",
    "def plot_missing_data(missing_series, title):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x=missing_series.values, y=missing_series.index)\n",
    "    plt.title(f\"Percentage of Blocks with Never Measured Variables - {title}\")\n",
    "    plt.xlabel(\"Percentage of Blocks (%)\")\n",
    "    plt.ylabel(\"Variables\")\n",
    "    plt.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot for each criteria set\n",
    "plot_missing_data(team_missing, \"TEAM Criteria\")\n",
    "plot_missing_data(yellow_missing, \"Yellow Criteria\")\n",
    "plot_missing_data(patel_missing, \"Patel Criteria\")\n",
    "plot_missing_data(green_missing, \"Patel Criteria\")\n",
    "\n",
    "# Print tabular summaries\n",
    "print(\"\\nTEAM Criteria Missing Data Summary:\")\n",
    "print(team_missing.round(2))\n",
    "print(\"\\nYellow Criteria Missing Data Summary:\")\n",
    "print(yellow_missing.round(2))\n",
    "print(\"\\nPatel Criteria Missing Data Summary:\")\n",
    "print(patel_missing.round(2))\n",
    "\n",
    "team_missing.to_csv('../output/final/team_missing_data.csv')\n",
    "yellow_missing.to_csv('../output/final/yellow_missing_data.csv')\n",
    "patel_missing.to_csv('../output/final/patel_missing_data.csv')\n",
    "green_missing.to_csv('../output/final/green_missing_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Competing Risk Analysis Setup\n",
    "\n",
    "Create a dataframe for each criteria with the following columns \n",
    "\n",
    "1. encounter_block: identify the patient encounter\n",
    "2. time_eligibility: earliest eligibility time from first intubation episode per encounter block\n",
    "3. time_death: time from ventilation start to death, if applicable. Missing if not dead\n",
    "4. time_discharge_alive: time from ventilation start to discharge. If not dead, assumed discharged and the last recorded vital time is discharge time.\n",
    "5. t_event: earliest of the above three times\n",
    "6. outcome: 1(eligibility), 2(death), 3(discharge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Competing risk updated (4/14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# Helper: build block‑level data set for competing‑risk analysis\n",
    "##############################################################################\n",
    "def create_competing_risk_dataset(\n",
    "    criteria_df: pd.DataFrame,\n",
    "    all_ids_w_outcome: pd.DataFrame,\n",
    "    flag_col: str = \"patel_flag\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    One row per encounter_block with\n",
    "      time_eligibility        - first hour where <flag_col> == 1\n",
    "      time_death              - hours from vent start to death   (NaN if alive)\n",
    "      time_discharge_alive    - hours from vent start to discharge (NaN if died)\n",
    "      t_event                 - min of the three times\n",
    "      outcome                 - 1(eligible)/2(death)/3(discharge)\n",
    "\n",
    "    Assumptions\n",
    "    -----------\n",
    "    • time_from_vent (in hours) is already *after* the 4hour cool-off.  \n",
    "    • all_ids_w_outcome has one row per encounter_block.\n",
    "    \"\"\"\n",
    "\n",
    "    ###################################################################\n",
    "    # 0) Basic column checks\n",
    "    ###################################################################\n",
    "    needed_cols = [\n",
    "        \"encounter_block\",\n",
    "        \"time_from_vent\",          # raw hours since intubation (already cooled‑off)\n",
    "        \"recorded_dttm\",\n",
    "        flag_col\n",
    "    ]\n",
    "    missing = [c for c in needed_cols if c not in criteria_df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"criteria_df is missing columns: {missing}\")\n",
    "\n",
    "    ###################################################################\n",
    "    # 1) FIRST ELIGIBILITY TIME  (earliest hour where flag==1)\n",
    "    ###################################################################\n",
    "    first_elig = (\n",
    "        criteria_df\n",
    "        .loc[criteria_df[flag_col] == 1, [\"encounter_block\", \"time_from_vent\"]]\n",
    "        .groupby(\"encounter_block\", as_index=False)\n",
    "        .min()\n",
    "        .rename(columns={\"time_from_vent\": \"time_eligibility\"})\n",
    "    )\n",
    "\n",
    "    ###################################################################\n",
    "    # 2) BLOCK‑LEVEL death / discharge times\n",
    "    ###################################################################\n",
    "    block_cols = [\n",
    "        \"encounter_block\",\n",
    "        \"block_vent_start_dttm\",\n",
    "        \"final_outcome_dttm\",\n",
    "        \"is_dead\"\n",
    "    ]\n",
    "    block_level = (\n",
    "        all_ids_w_outcome\n",
    "        .loc[all_ids_w_outcome[\"encounter_block\"].isin(criteria_df[\"encounter_block\"]),\n",
    "             block_cols]\n",
    "        .copy()\n",
    "    )\n",
    "\n",
    "    # convert to datetime once\n",
    "    block_level[\"block_vent_start_dttm\"] = pd.to_datetime(\n",
    "        block_level[\"block_vent_start_dttm\"], errors=\"coerce\"\n",
    "    )\n",
    "    block_level[\"final_outcome_dttm\"] = pd.to_datetime(\n",
    "        block_level[\"final_outcome_dttm\"], errors=\"coerce\"\n",
    "    )\n",
    "\n",
    "    # hours from vent start to the *final* outcome\n",
    "    hrs_from_vent = (\n",
    "        (block_level[\"final_outcome_dttm\"] - block_level[\"block_vent_start_dttm\"])\n",
    "        .dt.total_seconds() / 3600\n",
    "    )\n",
    "\n",
    "    block_level[\"time_death\"]            = np.where(\n",
    "        block_level[\"is_dead\"] == 1, hrs_from_vent, np.nan\n",
    "    )\n",
    "    block_level[\"time_discharge_alive\"]  = np.where(\n",
    "        block_level[\"is_dead\"] == 0, hrs_from_vent, np.nan\n",
    "    )\n",
    "\n",
    "    ###################################################################\n",
    "    # 3) MERGE and decide which event happened first\n",
    "    ###################################################################\n",
    "    final_df = (\n",
    "        block_level[[\"encounter_block\", \"time_death\", \"time_discharge_alive\"]]\n",
    "        .merge(first_elig, on=\"encounter_block\", how=\"left\")\n",
    "    )\n",
    "\n",
    "    # earliest non‑NaN time\n",
    "    final_df[\"t_event\"] = final_df[[\"time_eligibility\",\n",
    "                                    \"time_death\",\n",
    "                                    \"time_discharge_alive\"]].min(axis=1, skipna=True)\n",
    "\n",
    "    # outcome code\n",
    "    def pick_outcome(r):\n",
    "        if np.isfinite(r[\"time_eligibility\"]) and r[\"t_event\"] == r[\"time_eligibility\"]:\n",
    "            return 1\n",
    "        if np.isfinite(r[\"time_death\"])       and r[\"t_event\"] == r[\"time_death\"]:\n",
    "            return 2\n",
    "        return 3   # discharge must be earliest\n",
    "\n",
    "    final_df[\"outcome\"] = final_df.apply(pick_outcome, axis=1)\n",
    "\n",
    "    return final_df[\n",
    "        [\"encounter_block\",\n",
    "         \"time_eligibility\",\n",
    "         \"time_death\",\n",
    "         \"time_discharge_alive\",\n",
    "         \"t_event\",\n",
    "         \"outcome\"]\n",
    "    ].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.merge(\n",
    "        final_df,\n",
    "        all_ids_w_outcome[['encounter_block',\n",
    "       'block_vent_start_dttm', 'block_vent_end_dttm',\n",
    "       'block_first_vital_dttm', 'block_last_vital_dttm', 'discharge_dttm',\n",
    "       'discharge_category', 'death_dttm', 'final_outcome_dttm', 'is_dead']],\n",
    "        on=  'encounter_block',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "df_merged_team   = df_merged.copy()\n",
    "df_merged_yellow = df_merged.copy()\n",
    "df_merged_patel  = df_merged.copy()\n",
    "df_merged_green = df_merged.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patel_competing = create_competing_risk_dataset(\n",
    "    criteria_df        = df_merged_patel,\n",
    "    all_ids_w_outcome  = all_ids_w_outcome,\n",
    "    flag_col           = \"patel_flag\"\n",
    ")\n",
    "df_patel_competing.to_parquet(\"../output/intermediate/competing_risk_patel_final.parquet\")\n",
    "\n",
    "df_team_competing = create_competing_risk_dataset(\n",
    "    criteria_df        = df_merged_team,\n",
    "    all_ids_w_outcome  = all_ids_w_outcome,\n",
    "    flag_col           = \"team_flag\"\n",
    ")\n",
    "df_team_competing.to_parquet(\"../output/intermediate/competing_risk_team_final.parquet\")\n",
    "\n",
    "df_yellow_competing = create_competing_risk_dataset(\n",
    "    criteria_df        = df_merged_yellow,\n",
    "    all_ids_w_outcome  = all_ids_w_outcome,\n",
    "    flag_col           = \"any_yellow_or_green_no_red\"\n",
    ")\n",
    "df_yellow_competing.to_parquet(\"../output/intermediate/competing_risk_yellow_final.parquet\")\n",
    "\n",
    "df_green_competing = create_competing_risk_dataset(\n",
    "    criteria_df        = df_merged_green,\n",
    "    all_ids_w_outcome  = all_ids_w_outcome,\n",
    "    flag_col           = \"all_green\"\n",
    ")\n",
    "df_green_competing.to_parquet(\"../output/intermediate/competing_risk_green_final.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 0.  Prerequisites\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=r\"upsetplot\")\n",
    "\n",
    "# df_patel_competing, df_team_competing, df_yellow_competing  already in memory\n",
    "# final_df  – the full hour‑level table with TEAM flag columns\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 1.  Helper: analyse one criterion\n",
    "# ---------------------------------------------------------------------------\n",
    "def analyse_discharge_without_elig(competing_df,      # block‑level table\n",
    "                                   flag_prefix,       # \"team_\", \"patel_\", ...\n",
    "                                   title):\n",
    "    \"\"\"\n",
    "    competing_df must have columns: encounter_block, outcome (1/2/3)\n",
    "    final_df must be in outer scope (hour‑level with the *_flag columns)\n",
    "    \"\"\"\n",
    "\n",
    "    # 1a) encounter‑blocks that were discharged alive without eligibility\n",
    "    no_elig_blocks = competing_df.loc[competing_df[\"outcome\"] == 3,\n",
    "                                      \"encounter_block\"].unique()\n",
    "    df_fail = final_df[final_df[\"encounter_block\"].isin(no_elig_blocks)]\n",
    "\n",
    "    # 1b) sub‑criterion columns for this prefix\n",
    "    crit_cols = [c for c in df_fail.columns\n",
    "                 if c.startswith(flag_prefix) and c.endswith(\"_flag\")\n",
    "                 and c not in (f\"{flag_prefix}flag\",\n",
    "                               f\"{flag_prefix}cardio_flag\",\n",
    "                               f\"{flag_prefix}resp_flag\")]\n",
    "\n",
    "    # 1c) for each block: did that criterion ever fail?  (0=True fail, 1=True pass)\n",
    "    ever_fail = (df_fail[crit_cols] == 0).groupby(df_fail[\"encounter_block\"]).max()\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    # 2.  BAR PLOT  – single‑criterion failure frequencies\n",
    "    # ----------------------------------------------------------------------\n",
    "    freq = ever_fail.mean().sort_values(ascending=True)  # proportion of blocks\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.barh(freq.index.str.replace(f\"{flag_prefix}\", \"\"), freq.values, color=\"#4c72b0\")\n",
    "    plt.xlabel(\"Proportion of encounter‑blocks where criterion ever failed\")\n",
    "    plt.title(f\"{title}: which sub‑criteria blocked eligibility?\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    # 3.  UPSET PLOT  – combinations of failures\n",
    "    # ----------------------------------------------------------------------\n",
    "    upset_data = from_indicators(ever_fail.columns, ever_fail)\n",
    "    UpSet(upset_data, show_counts=True,\n",
    "          sort_by=\"cardinality\",\n",
    "          intersection_plot_elements=15,\n",
    "          element_size=None).plot()\n",
    "    plt.suptitle(f\"{title}: top combinations of failed sub‑criteria\", y=1.02)\n",
    "    plt.show()\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    # 4.  Return the summary table for further use\n",
    "    # ----------------------------------------------------------------------\n",
    "    return freq.to_frame(\"prop_blocks_failed\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 2.  Run for each criterion\n",
    "# ---------------------------------------------------------------------------\n",
    "team_summary   = analyse_discharge_without_elig(df_team_competing,\n",
    "                                               flag_prefix=\"team_\",\n",
    "                                               title=\"TEAM\")\n",
    "\n",
    "patel_summary  = analyse_discharge_without_elig(df_patel_competing,\n",
    "                                               flag_prefix=\"patel_\",\n",
    "                                               title=\"Patel\")\n",
    "\n",
    "yellow_summary = analyse_discharge_without_elig(df_yellow_competing,\n",
    "                                               flag_prefix=\"yellow_\",\n",
    "                                               title=\"Yellow\")\n",
    "\n",
    "green_summary = analyse_discharge_without_elig(df_green_competing,\n",
    "                                               flag_prefix=\"green_\",\n",
    "                                               title=\"Green\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 3.  Example: compare the three summaries side‑by‑side\n",
    "# ---------------------------------------------------------------------------\n",
    "compare = (team_summary.rename(columns={\"prop_blocks_failed\": \"TEAM\"})\n",
    "           .join(patel_summary.rename(columns={\"prop_blocks_failed\": \"Patel\"}), how=\"outer\")\n",
    "           .join(yellow_summary.rename(columns={\"prop_blocks_failed\": \"Yellow\"}), how=\"outer\")\n",
    "           .join(green_summary.rename(columns={\"prop_blocks_failed\": \"Green\"}), how=\"outer\")\n",
    "           .fillna(0)\n",
    "           .sort_index())\n",
    "\n",
    "display(compare.style.format(\"{:.1%}\").background_gradient(cmap=\"Blues\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reqd_team_fields = ['encounter_block','hospitalization_id', 'recorded_dttm',\t'recorded_date'\t,'recorded_hour',\n",
    "                    'time_from_vent','hourly_trach','paralytics_flag',\n",
    "                    'lactate', 'max_heart_rate', 'ne_calc_max','ne_calc_last','last_ne_dose_last_6_hours', 'min_fio2_set', 'max_peep_set', \n",
    "                    'max_resp_rate_obs', \"team_pulse_flag\", \"team_lactate_flag\", \"team_ne_flag\",\n",
    "                    \"team_fio2_flag\",  \"team_peep_flag\",    \"team_resp_rate_flag\", \"team_flag\"]\n",
    "\n",
    "\n",
    "no_elig_blocks_team = df_team_competing.loc[df_team_competing[\"outcome\"] == 3,\n",
    "                                      \"encounter_block\"].unique()\n",
    "df_fail_team = final_df[final_df[\"encounter_block\"].isin(no_elig_blocks_team)]\n",
    "df_fail_team_filtered = df_fail_team[reqd_team_fields].copy()\n",
    "df_fail_team_filtered = df_fail_team_filtered.merge(all_ids_w_outcome[['discharge_category', 'encounter_block']], on='encounter_block', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fail_team_filtered.groupby('discharge_category')['encounter_block'].nunique().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_hospitalization_ids = df_fail_yellow_filtered['hospitalization_id'].unique().tolist()\n",
    "# unique_hospitalization_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reqd_patel_fields = ['encounter_block', 'hospitalization_id','recorded_dttm',\t'recorded_date'\t,'recorded_hour',\n",
    "                    'time_from_vent', 'hourly_trach','paralytics_flag',\n",
    "                    'min_map', 'max_map','max_sbp', 'min_sbp',\n",
    "                   'min_heart_rate','max_heart_rate', 'min_respiratory_rate','min_spo2', \n",
    "                    'max_respiratory_rate','patel_map_flag','patel_sbp_flag','patel_pulse_flag', \n",
    "                    'patel_resp_rate_flag' , 'patel_spo2_flag', 'patel_resp_flag', 'patel_cardio_flag' ]\n",
    "no_elig_blocks_patel = df_patel_competing.loc[df_patel_competing[\"outcome\"] == 3,\n",
    "                                      \"encounter_block\"].unique()\n",
    "df_fail_patel = final_df[final_df[\"encounter_block\"].isin(no_elig_blocks_patel)]\n",
    "df_fail_patel_filtered = df_fail_patel[reqd_patel_fields].copy()\n",
    "df_fail_patel_filtered = df_fail_patel_filtered.merge(all_ids_w_outcome[['discharge_category', 'encounter_block']], \n",
    "                                                    on='encounter_block', how='inner')\n",
    "df_fail_patel_filtered.groupby('discharge_category')['encounter_block'].nunique().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reqd_yellow_fields =[\n",
    "    'encounter_block', 'hospitalization_id','recorded_dttm',\t'recorded_date'\t,'recorded_hour',\n",
    "    'time_from_vent', 'hourly_trach','paralytics_flag',\n",
    "    # Clinical Measurements\n",
    "    'min_spo2', 'min_map', 'max_map', 'ne_calc_max', 'max_sbp',\n",
    "    'max_heart_rate', 'min_heart_rate', 'min_fio2_set',\n",
    "    'max_resp_rate_obs', 'min_peep_set', 'lactate',\n",
    "    \n",
    "    # Administrative/Timing\n",
    "    'hourly_trach', 'paralytics_flag', 'recorded_hour',\n",
    "    'time_from_vent_adjusted', 'red_meds_flag',\n",
    "    # Red Flags\n",
    "    'red_resp_spo2_flag', 'red_map_flag', 'red_high_support_flag',\n",
    "    'red_hypertensive_flag', 'red_pulse_high_flag', 'red_pulse_low_flag',\n",
    "    \n",
    "    # Yellow Flags\n",
    "    'yellow_resp_spo2_flag', 'yellow_fio2_flag', 'yellow_resp_rate_flag',\n",
    "    'yellow_peep_flag', 'yellow_map_flag', 'yellow_pulse_flag',\n",
    "    'yellow_lactate_flag',\n",
    "    \n",
    "    # Green Flags\n",
    "    'green_resp_spo2_flag', 'green_resp_rate_flag', 'green_fio2_flag',\n",
    "    'green_peep_flag', 'green_map_flag', 'green_pulse_flag',\n",
    "    'green_lactate_flag', 'green_hr_flag',\n",
    "    \n",
    "    # Composite Flags\n",
    "    'any_red', 'any_yellow', 'any_green', 'all_green',\n",
    "    'all_green_no_red', 'all_green_no_red_yellow',\n",
    "    'all_yellow_no_red_green', 'any_yellow_no_red_green',\n",
    "    'any_yellow_or_green_no_red', 'yellow_resp_flag',\n",
    "    'yellow_cardio_flag', 'yellow_all_green', 'yellow_not_all_green'\n",
    "]\n",
    "no_elig_blocks_yellow = df_yellow_competing.loc[df_yellow_competing[\"outcome\"] == 3,\n",
    "                                      \"encounter_block\"].unique()\n",
    "df_fail_yellow = final_df[final_df[\"encounter_block\"].isin(no_elig_blocks_yellow)]\n",
    "df_fail_yellow_filtered = df_fail_yellow[reqd_yellow_fields].copy()\n",
    "df_fail_yellow_filtered = df_fail_yellow_filtered.merge(all_ids_w_outcome[['discharge_category', 'encounter_block']], \n",
    "                                                    on='encounter_block', how='inner')\n",
    "df_fail_yellow_filtered.groupby('discharge_category')['encounter_block'].nunique().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reqd_green_fields =[\n",
    "    'encounter_block', 'hospitalization_id','recorded_dttm',\t'recorded_date'\t,'recorded_hour',\n",
    "    'time_from_vent', 'hourly_trach','paralytics_flag',\n",
    "    # Clinical Measurements\n",
    "    'min_spo2', 'min_map', 'max_map', 'ne_calc_max', 'max_sbp',\n",
    "    'max_heart_rate', 'min_heart_rate', 'min_fio2_set',\n",
    "    'max_resp_rate_obs', 'min_peep_set', 'lactate',\n",
    "    \n",
    "    # Administrative/Timing\n",
    "    'hourly_trach', 'paralytics_flag', 'recorded_hour',\n",
    "    'time_from_vent_adjusted', 'red_meds_flag',\n",
    "    # Red Flags\n",
    "    'red_resp_spo2_flag', 'red_map_flag', 'red_high_support_flag',\n",
    "    'red_hypertensive_flag', 'red_pulse_high_flag', 'red_pulse_low_flag',\n",
    "    \n",
    "    # Green Flags\n",
    "    'green_resp_spo2_flag', 'green_resp_rate_flag', 'green_fio2_flag',\n",
    "    'green_peep_flag', 'green_map_flag', 'green_pulse_flag',\n",
    "    'green_lactate_flag', 'green_hr_flag',\n",
    "    'all_green',\n",
    "    'all_green_no_red', \n",
    "]\n",
    "no_elig_blocks_green = df_green_competing.loc[df_green_competing[\"outcome\"] == 3,\n",
    "                                      \"encounter_block\"].unique()\n",
    "df_fail_green = final_df[final_df[\"encounter_block\"].isin(no_elig_blocks_green)]\n",
    "df_fail_green_filtered = df_fail_green[reqd_green_fields].copy()\n",
    "df_fail_green_filtered = df_fail_green_filtered.merge(all_ids_w_outcome[['discharge_category', 'encounter_block']], \n",
    "                                                    on='encounter_block', how='inner')\n",
    "df_fail_green_filtered.groupby('discharge_category')['encounter_block'].nunique().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Competing Risk Business Hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# Helper: build block-level data set for competing-risk analysis\n",
    "##############################################################################\n",
    "def create_competing_risk_dataset(\n",
    "    criteria_df: pd.DataFrame,\n",
    "    all_ids_w_outcome: pd.DataFrame,\n",
    "    *,\n",
    "    flag_col: str = \"patel_flag\",\n",
    "    time_col: str = \"time_from_vent\"          # NEW ▶ choose raw or biz hours\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    One row per encounter_block with\n",
    "      time_eligibility        - first hour where <flag_col> == 1   (using *time_col*)\n",
    "      time_death              - hours from vent start to death     (NaN if alive)\n",
    "      time_discharge_alive    - hours from vent start to discharge (NaN if died)\n",
    "      t_event                 - min of the three times\n",
    "      outcome                 - 1(eligible)/2(death)/3(discharge)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    criteria_df           : row-level signals (one row per time point)\n",
    "    all_ids_w_outcome     : one row per encounter_block with outcome timestamps\n",
    "    flag_col              : name of the 0/1 eligibility flag in criteria_df\n",
    "    time_col              : *name* of the column in criteria_df holding the\n",
    "                            hours-from-ventage you want to use.  Defaults to\n",
    "                            \"time_from_vent\" but you can pass \"time_biz_from_vent\",\n",
    "                            \"time_from_vent_adjusted\", … as needed.\n",
    "\n",
    "    Assumptions\n",
    "    -----------\n",
    "    • The chosen *time_col* already reflects any cool-off or business-hour logic\n",
    "      you intend.  The function never subtracts 4 h internally any more.\n",
    "    \"\"\"\n",
    "\n",
    "    ###################################################################\n",
    "    # 0) Basic column checks\n",
    "    ###################################################################\n",
    "    hrs_col = time_col                  # just a shorter alias\n",
    "\n",
    "    needed = [\"encounter_block\", hrs_col, \"recorded_dttm\", flag_col]\n",
    "    missing = [c for c in needed if c not in criteria_df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"criteria_df is missing columns: {missing}\")\n",
    "\n",
    "    ###################################################################\n",
    "    # 1) FIRST ELIGIBILITY TIME  (earliest hour where flag==1)\n",
    "    ###################################################################\n",
    "    first_elig = (\n",
    "        criteria_df\n",
    "        .loc[criteria_df[flag_col] == 1, [\"encounter_block\", hrs_col]]\n",
    "        .groupby(\"encounter_block\", as_index=False)\n",
    "        .min()\n",
    "        .rename(columns={hrs_col: \"time_eligibility\"})\n",
    "    )\n",
    "\n",
    "    ###################################################################\n",
    "    # 2) BLOCK-LEVEL death / discharge times\n",
    "    ###################################################################\n",
    "    block_cols = [\n",
    "        \"encounter_block\",\n",
    "        \"block_vent_start_dttm\",\n",
    "        \"final_outcome_dttm\",\n",
    "        \"is_dead\"\n",
    "    ]\n",
    "    block_level = (\n",
    "        all_ids_w_outcome\n",
    "        .loc[all_ids_w_outcome[\"encounter_block\"].isin(criteria_df[\"encounter_block\"]),\n",
    "             block_cols]\n",
    "        .copy()\n",
    "    )\n",
    "\n",
    "    block_level[\"block_vent_start_dttm\"] = pd.to_datetime(\n",
    "        block_level[\"block_vent_start_dttm\"], errors=\"coerce\"\n",
    "    )\n",
    "    block_level[\"final_outcome_dttm\"] = pd.to_datetime(\n",
    "        block_level[\"final_outcome_dttm\"], errors=\"coerce\"\n",
    "    )\n",
    "\n",
    "    hrs_from_vent = (\n",
    "        (block_level[\"final_outcome_dttm\"] - block_level[\"block_vent_start_dttm\"])\n",
    "        .dt.total_seconds() / 3600\n",
    "    )\n",
    "\n",
    "    block_level[\"time_death\"] = np.where(block_level[\"is_dead\"] == 1,\n",
    "                                         hrs_from_vent, np.nan)\n",
    "    block_level[\"time_discharge_alive\"] = np.where(block_level[\"is_dead\"] == 0,\n",
    "                                                   hrs_from_vent, np.nan)\n",
    "\n",
    "    ###################################################################\n",
    "    # 3) MERGE and decide which event happened first\n",
    "    ###################################################################\n",
    "    final_df = (\n",
    "        block_level[[\"encounter_block\", \"time_death\", \"time_discharge_alive\"]]\n",
    "        .merge(first_elig, on=\"encounter_block\", how=\"left\")\n",
    "    )\n",
    "\n",
    "    # earliest non-NaN time\n",
    "    final_df[\"t_event\"] = final_df[\n",
    "        [\"time_eligibility\", \"time_death\", \"time_discharge_alive\"]\n",
    "    ].min(axis=1, skipna=True)\n",
    "\n",
    "    # outcome code\n",
    "    def pick_outcome(r):\n",
    "        if np.isfinite(r[\"time_eligibility\"]) and r[\"t_event\"] == r[\"time_eligibility\"]:\n",
    "            return 1\n",
    "        if np.isfinite(r[\"time_death\"]) and r[\"t_event\"] == r[\"time_death\"]:\n",
    "            return 2\n",
    "        return 3\n",
    "\n",
    "    final_df[\"outcome\"] = final_df.apply(pick_outcome, axis=1)\n",
    "\n",
    "    return final_df[[\n",
    "        \"encounter_block\",\n",
    "        \"time_eligibility\",\n",
    "        \"time_death\",\n",
    "        \"time_discharge_alive\",\n",
    "        \"t_event\",\n",
    "        \"outcome\"\n",
    "    ]].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.merge(\n",
    "        final_df,\n",
    "        all_ids_w_outcome[['encounter_block',\n",
    "       'block_vent_start_dttm', 'block_vent_end_dttm',\n",
    "       'block_first_vital_dttm', 'block_last_vital_dttm', 'discharge_dttm',\n",
    "       'discharge_category', 'death_dttm', 'final_outcome_dttm', 'is_dead']],\n",
    "        on=  'encounter_block',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "df_merged = df_merged[(df_merged['recorded_hour'] >= 8) & (df_merged['recorded_hour'] < 17)].copy()\n",
    "\n",
    "df_merged_team   = df_merged.copy()\n",
    "df_merged_yellow = df_merged.copy()\n",
    "df_merged_patel  = df_merged.copy()\n",
    "df_merged_green  = df_merged.copy()\n",
    "\n",
    "df_patel_competing_biz = create_competing_risk_dataset(\n",
    "    criteria_df        = df_merged_patel,\n",
    "    all_ids_w_outcome  = all_ids_w_outcome,\n",
    "    flag_col           = \"patel_flag\", \n",
    "    time_col=\"time_biz_from_vent\"\n",
    ")\n",
    "df_patel_competing_biz.to_parquet(\"../output/intermediate/competing_risk_patel_final_biz.parquet\")\n",
    "\n",
    "df_patel_competing_biz = create_competing_risk_dataset(\n",
    "    criteria_df        = df_merged_green,\n",
    "    all_ids_w_outcome  = all_ids_w_outcome,\n",
    "    flag_col           = \"all_green\", \n",
    "    time_col=\"time_biz_from_vent\"\n",
    ")\n",
    "df_patel_competing_biz.to_parquet(\"../output/intermediate/competing_risk_patel_final_biz.parquet\")\n",
    "\n",
    "df_team_competing_biz = create_competing_risk_dataset(\n",
    "    criteria_df        = df_merged_team,\n",
    "    all_ids_w_outcome  = all_ids_w_outcome,\n",
    "    flag_col           = \"team_flag\", \n",
    "    time_col=\"time_biz_from_vent\"\n",
    ")\n",
    "df_team_competing_biz.to_parquet(\"../output/intermediate/competing_risk_team_final_biz.parquet\")\n",
    "\n",
    "df_yellow_competing_biz = create_competing_risk_dataset(\n",
    "    criteria_df        = df_merged_yellow,\n",
    "    all_ids_w_outcome  = all_ids_w_outcome,\n",
    "    flag_col           = \"any_yellow_or_green_no_red\", \n",
    "    time_col=\"time_biz_from_vent\"\n",
    ")\n",
    "df_yellow_competing_biz.to_parquet(\"../output/intermediate/competing_risk_yellow_final_biz.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary DFs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reqd_patel_fields = ['encounter_block', 'hospitalization_id','recorded_dttm',\t'recorded_date'\t,'recorded_hour',\n",
    "                    'time_from_vent', 'hourly_trach','paralytics_flag',\n",
    "                    'min_map', 'max_map','max_sbp', 'min_sbp',\n",
    "                   'min_heart_rate','max_heart_rate', 'min_respiratory_rate','min_spo2', \n",
    "                    'max_respiratory_rate','patel_map_flag','patel_sbp_flag','patel_pulse_flag', \n",
    "                    'patel_resp_rate_flag' , 'patel_spo2_flag', 'patel_resp_flag', 'patel_cardio_flag', 'patel_flag' ]\n",
    "final_df_patel = final_df[reqd_patel_fields]\n",
    "final_df_patel = pd.merge(final_df_patel, df_patel_competing, on='encounter_block', how='inner')\n",
    "final_df_patel.to_csv('../output/intermediate/final_df_patel.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reqd_yellow_fields =[\n",
    "    'encounter_block', 'hospitalization_id','recorded_dttm',\t'recorded_date'\t,'recorded_hour',\n",
    "    'time_from_vent', 'hourly_trach','paralytics_flag',\n",
    "    # Clinical Measurements\n",
    "    'min_spo2', 'min_map', 'max_map', 'ne_calc_max', 'max_sbp',\n",
    "    'max_heart_rate', 'min_heart_rate', 'min_fio2_set',\n",
    "    'max_resp_rate_obs', 'min_peep_set', 'lactate',\n",
    "    \n",
    "    # Administrative/Timing\n",
    "    'hourly_trach', 'paralytics_flag', 'recorded_hour',\n",
    "    'time_from_vent_adjusted', 'red_meds_flag',\n",
    "    # Red Flags\n",
    "    'red_resp_spo2_flag', 'red_map_flag', 'red_high_support_flag',\n",
    "    'red_hypertensive_flag', 'red_pulse_high_flag', 'red_pulse_low_flag',\n",
    "    \n",
    "    # Yellow Flags\n",
    "    'yellow_resp_spo2_flag', 'yellow_fio2_flag', 'yellow_resp_rate_flag',\n",
    "    'yellow_peep_flag', 'yellow_map_flag', 'yellow_pulse_flag',\n",
    "    'yellow_lactate_flag',\n",
    "    \n",
    "    # Green Flags\n",
    "    'green_resp_spo2_flag', 'green_resp_rate_flag', 'green_fio2_flag',\n",
    "    'green_peep_flag', 'green_map_flag', 'green_pulse_flag',\n",
    "    'green_lactate_flag', 'green_hr_flag',\n",
    "    \n",
    "    # Composite Flags\n",
    "    'any_red', 'any_yellow', 'any_green', 'all_green',\n",
    "    'all_green_no_red', 'all_green_no_red_yellow',\n",
    "    'all_yellow_no_red_green', 'any_yellow_no_red_green',\n",
    "    'any_yellow_or_green_no_red', 'yellow_resp_flag',\n",
    "    'yellow_cardio_flag', 'yellow_all_green', 'yellow_not_all_green'\n",
    "]\n",
    "final_df_yellow = final_df[reqd_yellow_fields]\n",
    "final_df_yellow = pd.merge(final_df_yellow, df_yellow_competing, on='encounter_block', how='inner')\n",
    "final_df_yellow.to_csv('../output/intermediate/final_df_yellow.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reqd_team_fields = ['encounter_block','hospitalization_id', 'recorded_dttm',\t'recorded_date'\t,'recorded_hour',\n",
    "                    'time_from_vent','hourly_trach','paralytics_flag',\n",
    "                    'lactate', 'max_heart_rate', 'ne_calc_last', 'last_ne_dose_last_6_hours' ,'min_fio2_set', 'max_peep_set', \n",
    "                    'max_resp_rate_obs', \"team_pulse_flag\", \"team_lactate_flag\", \"team_ne_flag\",\n",
    "                    \"team_fio2_flag\",  \"team_peep_flag\",    \"team_resp_rate_flag\", 'team_flag']\n",
    "final_df_team = final_df[reqd_team_fields]\n",
    "final_df_team = pd.merge(final_df_team, df_team_competing, on='encounter_block', how='inner')\n",
    "final_df_team.to_csv('../output/intermediate/final_df_team.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mortality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_death_without_eligibility(df, criteria_name):\n",
    "    # Total number of blocks\n",
    "    total_blocks = len(df)\n",
    "    \n",
    "    # Blocks that died without eligibility (outcome=2)\n",
    "    died_without_elig = df[df['outcome'] == 2].shape[0]\n",
    "    \n",
    "    # Calculate percentage\n",
    "    percent = (died_without_elig / total_blocks) * 100\n",
    "    \n",
    "    return {\n",
    "        'criteria': criteria_name,\n",
    "        'total_blocks': total_blocks,\n",
    "        'died_without_eligibility': died_without_elig,\n",
    "        'percentage': percent\n",
    "    }\n",
    "\n",
    "# Analyze each dataset\n",
    "results = [\n",
    "    analyze_death_without_eligibility(df_team_competing, 'TEAM'),\n",
    "    analyze_death_without_eligibility(df_yellow_competing, 'Yellow'),\n",
    "    analyze_death_without_eligibility(df_patel_competing, 'Patel')\n",
    "]\n",
    "\n",
    "# Convert to DataFrame for easier plotting\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Create the visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Create grouped bar plot\n",
    "x = range(len(results_df['criteria']))\n",
    "width = 0.35\n",
    "\n",
    "# Plot bars\n",
    "bars = plt.bar(x, results_df['percentage'], width, label='Percentage')\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('Percentage of Blocks that Died Without Becoming Eligible by Criteria', pad=20)\n",
    "plt.xlabel('Criteria')\n",
    "plt.ylabel('Percentage (%)')\n",
    "plt.xticks(x, results_df['criteria'])\n",
    "\n",
    "# Add value labels on top of bars\n",
    "for i, bar in enumerate(bars):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:.1f}%\\n({results_df[\"died_without_eligibility\"][i]:,}/{results_df[\"total_blocks\"][i]:,})',\n",
    "             ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed summary\n",
    "print(\"\\nDetailed Summary:\")\n",
    "print(\"=\" * 80)\n",
    "for result in results:\n",
    "    print(f\"\\n{result['criteria']} Criteria:\")\n",
    "    print(f\"Total blocks: {result['total_blocks']:,}\")\n",
    "    print(f\"Died without eligibility: {result['died_without_eligibility']:,}\")\n",
    "    print(f\"Percentage: {result['percentage']:.1f}%\")\n",
    "\n",
    "pd.DataFrame(results).to_csv('../output/final/death_without_eligibility_summary.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final figures and tables\n",
    "\n",
    "1. Figure 1: Percentage of encounter satisfying Patel, TEAM, and any yellow or GREEN criteria\n",
    "2. Figure 2: Percentage of business hours each encounter was eligible for different criteria\n",
    "3. Figure 3: Percentage of business hours not eligible for each criteria broken down by subcomponent failure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregates for comparison across sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria_list = [\n",
    "    ('patel_flag', 'Patel'),\n",
    "    ('team_flag', 'TEAM'),\n",
    "    ('any_yellow_or_green_no_red', 'Yellow'),\n",
    "    ('all_green', 'Green')\n",
    "]\n",
    "\n",
    "aggregate_data = []\n",
    "\n",
    "# Total number of patients\n",
    "total_patients = final_df['encounter_block'].nunique()\n",
    "\n",
    "# Total observed hours\n",
    "total_observed_hours = final_df.shape[0]\n",
    "\n",
    "# Total business hours (8AM-5PM)\n",
    "total_business_hours = final_df[\n",
    "    (final_df['recorded_hour'] >= 8) & \n",
    "    (final_df['recorded_hour'] < 17)\n",
    "].shape[0]\n",
    "\n",
    "for flag_column, criterion_name in criteria_list:\n",
    "    # Filter the DataFrame for rows where the criterion is met\n",
    "    eligible_df = final_df[final_df[flag_column] == 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    eligible_hours = eligible_df.shape[0]\n",
    "    eligible_patients = eligible_df['encounter_block'].nunique()\n",
    "    \n",
    "    # Calculate business hours where criterion is met\n",
    "    eligible_business_hours = eligible_df[\n",
    "        (eligible_df['recorded_hour'] >= 8) & \n",
    "        (eligible_df['recorded_hour'] < 17)\n",
    "    ].shape[0]\n",
    "    \n",
    "    # Append to aggregate data list\n",
    "    aggregate_data.append({\n",
    "        'Criteria': criterion_name,\n",
    "        'Total Patients': total_patients,\n",
    "        'Eligible Patients': eligible_patients,\n",
    "        'Total Observed Hours': total_observed_hours,\n",
    "        'Eligible Hours': eligible_hours,\n",
    "        'Total Business Hours': total_business_hours,\n",
    "        'Eligible Business Hours': eligible_business_hours,\n",
    "        'Proportion of Eligible Hours': (eligible_hours / total_observed_hours)*100,\n",
    "        'Proportion of Eligible Business Hours': (eligible_business_hours / total_business_hours)*100,\n",
    "        'Proportion of Eligible Patients': (eligible_patients / total_patients)*100\n",
    "    })\n",
    "    \n",
    "# Create a DataFrame for aggregate data\n",
    "aggregate_df = pd.DataFrame(aggregate_data)\n",
    "\n",
    "# Save the aggregate data to CSV\n",
    "timestamp = datetime.now().date()\n",
    "aggregate_df.to_csv(f'../output/final/aggregates_{pyCLIF.helper[\"site_name\"]}_{timestamp}.csv', index=False)\n",
    "\n",
    "# Print the aggregate data\n",
    "aggregate_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eligibility by encounter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom colors\n",
    "custom_colors = ['#983232', '#003f5c', '#fdfd96', '#98FB98']  # Maroon, Dark Blue, Pastel Yellow, Pastel Green\n",
    "\n",
    "# Plot the bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "barplot = sns.barplot(x='Criteria', y='Proportion of Eligible Patients', data=aggregate_df, palette=custom_colors)\n",
    "\n",
    "# Rotate the x-axis labels\n",
    "# plt.xticks(rotation=90)\n",
    "\n",
    "# Add percentages on top of the bars\n",
    "for index, row in aggregate_df.iterrows():\n",
    "    barplot.text(index, row['Proportion of Eligible Patients'] , f\"{row['Proportion of Eligible Patients']:.1f}%\", \n",
    "                 color='black', ha=\"center\")\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Criteria')\n",
    "plt.ylabel('Percentage of Encounters')\n",
    "\n",
    "# Save the graph\n",
    "plt.savefig(f'../output/final/graphs/eligibility_by_hosp_{pyCLIF.helper[\"site_name\"]}_{datetime.now().date()}.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eligibility by business hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom colors\n",
    "custom_colors = ['#983232', '#003f5c', '#fdfd96', '#98FB98']  # Maroon, Dark Blue, Pastel Yellow, Pastel Green\n",
    "\n",
    "# Plot the bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "barplot = sns.barplot(x='Criteria', y='Proportion of Eligible Business Hours', data=aggregate_df, palette=custom_colors)\n",
    "\n",
    "# Rotate the x-axis labels\n",
    "# plt.xticks(rotation=90)\n",
    "\n",
    "# Add percentages on top of the bars\n",
    "for index, row in aggregate_df.iterrows():\n",
    "    barplot.text(index, row['Proportion of Eligible Business Hours'] , f\"{row['Proportion of Eligible Business Hours']:.1f}%\", \n",
    "                 color='black', ha=\"center\")\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Criteria')\n",
    "plt.ylabel('Percentage of Eligible Business Hours')\n",
    "\n",
    "# Save the graph\n",
    "plt.savefig(f'../output/final/graphs/eligibility_by_hour_{pyCLIF.helper[\"site_name\"]}_{datetime.now().date()}.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Failure by subcomponents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your criteria and corresponding subcomponent flags\n",
    "criteria_info = {\n",
    "    'patel_flag': {'resp_flag': 'patel_resp_flag', 'cardio_flag': 'patel_cardio_flag'},\n",
    "    'team_flag': {'resp_flag': 'team_resp_flag', 'cardio_flag': 'team_cardio_flag'},\n",
    "    'any_yellow_or_green_no_red': {'resp_flag': 'yellow_resp_flag', 'cardio_flag': 'yellow_cardio_flag'}\n",
    "}\n",
    "\n",
    "# Initialize an empty list to store results\n",
    "results = []\n",
    "\n",
    "# Loop over each criterion\n",
    "for criterion, flags in criteria_info.items():\n",
    "    resp_flag = flags['resp_flag']\n",
    "    cardio_flag = flags['cardio_flag']\n",
    "    \n",
    "    # Calculate total hours per hospitalization_id\n",
    "    total_hours = final_df.groupby('encounter_block').size().rename('total_hours')\n",
    "    \n",
    "    # Create failure indicators\n",
    "    df_failure = final_df.copy()\n",
    "    df_failure['resp_only_failure'] = ((df_failure[resp_flag] == 0) & (df_failure[cardio_flag] == 1)).astype(int)\n",
    "    df_failure['cardio_only_failure'] = ((df_failure[resp_flag] == 1) & (df_failure[cardio_flag] == 0)).astype(int)\n",
    "    df_failure['both_failures'] = ((df_failure[resp_flag] == 0) & (df_failure[cardio_flag] == 0)).astype(int)\n",
    "    \n",
    "    # Aggregate the counts per hospitalization_id\n",
    "    failure_counts = df_failure.groupby('encounter_block')[['resp_only_failure', 'cardio_only_failure', 'both_failures']].sum()\n",
    "    \n",
    "    # Merge with total hours\n",
    "    failure_counts = failure_counts.merge(total_hours, left_index=True, right_index=True)\n",
    "    \n",
    "    # Calculate percentages\n",
    "    failure_counts['resp_only_failure_perc'] = (failure_counts['resp_only_failure'] * 100 / failure_counts['total_hours']).round(3)\n",
    "    failure_counts['cardio_only_failure_perc'] = (failure_counts['cardio_only_failure'] * 100 / failure_counts['total_hours']).round(3)\n",
    "    failure_counts['both_failures_perc'] = (failure_counts['both_failures'] * 100 / failure_counts['total_hours']).round(3)\n",
    "    \n",
    "    # Calculate total failure percentage\n",
    "    failure_counts['total_failure_perc'] = (\n",
    "        failure_counts['resp_only_failure'] + failure_counts['cardio_only_failure'] + failure_counts['both_failures']\n",
    "    ) * 100 / failure_counts['total_hours']\n",
    "    \n",
    "    # Calculate criterion met percentage\n",
    "    criterion_met = final_df.groupby('encounter_block')[criterion].sum().rename('criterion_met_hours')\n",
    "    failure_counts = failure_counts.merge(criterion_met, left_index=True, right_index=True)\n",
    "    failure_counts['criterion_met_perc'] = (failure_counts['criterion_met_hours'] * 100 / failure_counts['total_hours']).round(3)\n",
    "    \n",
    "    # Add criterion name to the DataFrame\n",
    "    failure_counts['Criteria'] = criterion\n",
    "    \n",
    "    # Append to results\n",
    "    results.append(failure_counts.reset_index())\n",
    "\n",
    "# Concatenate results for all criteria\n",
    "all_failure_counts = pd.concat(results, ignore_index=True)\n",
    "\n",
    "# Now, calculate the average percentages across all hospitalizations for each criterion\n",
    "avg_failure_percentages = all_failure_counts.groupby('Criteria').agg({\n",
    "    'resp_only_failure_perc': 'mean',\n",
    "    'cardio_only_failure_perc': 'mean',\n",
    "    'both_failures_perc': 'mean',\n",
    "    'total_failure_perc': 'mean',\n",
    "    'criterion_met_perc': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "# Rename columns for clarity\n",
    "avg_failure_percentages = avg_failure_percentages.rename(columns={\n",
    "    'resp_only_failure_perc': 'Resp Failure Only',\n",
    "    'cardio_only_failure_perc': 'Cardio Failure Only',\n",
    "    'both_failures_perc': 'Both Failures',\n",
    "    'total_failure_perc': 'Total Failure',\n",
    "    'criterion_met_perc': 'Criterion Met'\n",
    "})\n",
    "\n",
    "# Display the average failure percentages\n",
    "criteria_mapping = {\n",
    "    'patel_flag': 'Patel',\n",
    "    'team_flag': 'TEAM',\n",
    "    'any_yellow_or_green_no_red': 'Yellow'\n",
    "}\n",
    "\n",
    "avg_failure_percentages['Criteria'] = avg_failure_percentages['Criteria'].replace(criteria_mapping)\n",
    "avg_failure_percentages['site_name'] = pyCLIF.helper[\"site_name\"]\n",
    "pd.DataFrame(avg_failure_percentages).to_csv(f'../output/final/avg_failure_percentages_{pyCLIF.helper[\"site_name\"]}_{datetime.now().date()}.csv',index=False)\n",
    "avg_failure_percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U kaleido\n",
    "# !pip install plotly\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import kaleido\n",
    "import plotly.graph_objects as go\n",
    "# Create a stacked bar plot using Plotly\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add bars for Cardio Failure Only\n",
    "fig.add_trace(go.Bar(\n",
    "    x=avg_failure_percentages['Criteria'],\n",
    "    y=avg_failure_percentages['Cardio Failure Only'],\n",
    "    name='Cardio Failure Only',\n",
    "    marker_color='#003366'  # Dark Blue\n",
    "))\n",
    "\n",
    "# Add bars for Resp Failure Only\n",
    "fig.add_trace(go.Bar(\n",
    "    x=avg_failure_percentages['Criteria'],\n",
    "    y=avg_failure_percentages['Resp Failure Only'],\n",
    "    name='Resp Failure Only',\n",
    "    marker_color='#983232'  # Maroon\n",
    "))\n",
    "\n",
    "# Add bars for Both Failures\n",
    "fig.add_trace(go.Bar(\n",
    "    x=avg_failure_percentages['Criteria'],\n",
    "    y=avg_failure_percentages['Both Failures'],\n",
    "    name='Both Failures',\n",
    "    marker_color='#fdfd96'  # Pastel Yellow\n",
    "))\n",
    "\n",
    "# Customize the layout\n",
    "fig.update_layout(\n",
    "    barmode='stack',\n",
    "    xaxis_title='Criteria',\n",
    "    yaxis_title='Average Percentage of Business Hours Not Met (%)',\n",
    "    yaxis=dict(range=[0, 100]),\n",
    "    template='plotly_white',\n",
    "    legend_title='Failure Type'\n",
    ")\n",
    "# Save the plot\n",
    "fig.write_image(f'../output/final/graphs/avg_failure_components_{pyCLIF.helper[\"site_name\"]}_{datetime.now().date()}.png')\n",
    "# Show the plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ────────────────────────────────────────────────────────────\n",
    "#  analyse_criterion.py  – plug‑and‑play helper\n",
    "# ────────────────────────────────────────────────────────────\n",
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt, seaborn as sns\n",
    "from upsetplot import from_indicators, UpSet\n",
    "from pathlib   import Path\n",
    "\n",
    "def analyse_criterion(\n",
    "    df: pd.DataFrame,\n",
    "    crit_name: str,\n",
    "    *,\n",
    "    flag_cols: list,             # list of sub‑criterion flags (0/1)\n",
    "    master_flag: str,            # overall eligibility flag (0/1)\n",
    "    id_col: str        = \"encounter_block\",\n",
    "    time_col: str      = \"time_from_vent\",\n",
    "    out_dir           = \"../output/final\",\n",
    "    save_fig_data: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    • Find blocks that *never* satisfy `master_flag`\n",
    "    • For those blocks …\n",
    "         – how often is each sub‑flag FALSE?\n",
    "         – which sub‑flag is the *latest* to turn TRUE       (primary blocker)\n",
    "         – which combinations of sub‑flags ever fail (UpSet)\n",
    "         – optionally: a distribution plot of a dose / lab   (pass in yourself)\n",
    "\n",
    "    Everything (plots + .csv helpers) is written to `out_dir/crit_name/`.\n",
    "    \"\"\"\n",
    "    out_dir = Path(out_dir, crit_name.lower())\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # ── 1 · which blocks NEVER became eligible? ─────────────────────────\n",
    "    never = (df.groupby(id_col)[master_flag].max()\n",
    "               .reset_index(name=\"ever\")[lambda d: d[\"ever\"] == 0]\n",
    "               .drop(columns=\"ever\"))\n",
    "    fail  = never.merge(df, on=id_col, how=\"inner\")\n",
    "    print(f\"[{crit_name}] {fail[id_col].nunique()} blocks never became eligible\")\n",
    "\n",
    "    # ── 2 · proportion of hours each sub‑flag is FALSE ──────────────────\n",
    "    long = fail.melt(id_vars=[id_col], value_vars=flag_cols,\n",
    "                     var_name=\"criterion\", value_name=\"flag\")\n",
    "    summary = (long.groupby(\"criterion\")[\"flag\"]\n",
    "                     .apply(lambda s: (s == 0).mean())\n",
    "                     .rename(\"prop_hours_failed\")\n",
    "                     .reset_index()\n",
    "                     .sort_values(\"prop_hours_failed\", ascending=False))\n",
    "\n",
    "    if save_fig_data:\n",
    "        summary.to_csv(out_dir/\"subflag_failure_rates.csv\", index=False)\n",
    "\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.barh(summary[\"criterion\"], summary[\"prop_hours_failed\"])\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.xlabel(\"Proportion of hours NOT satisfied\")\n",
    "    plt.title(f\"{crit_name}: which sub‑criteria blocked eligibility?\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_dir/\"subflag_bar.png\", dpi=300);  plt.close()\n",
    "\n",
    "    # ── 3 · “primary blocker” per block (latest first‑TRUE) ─────────────\n",
    "    def first_true(g, col):\n",
    "        hit = g.loc[g[col] == 1, time_col]\n",
    "        return hit.min() if not hit.empty else np.inf\n",
    "\n",
    "    prim = []\n",
    "    for blk, g in fail.groupby(id_col):\n",
    "        lags = {c: first_true(g, c) for c in flag_cols}\n",
    "        prim_blk = max(lags, key=lags.get)\n",
    "        prim.append([blk, prim_blk])\n",
    "\n",
    "    prim_df = pd.DataFrame(prim, columns=[id_col, \"primary_blocker\"])\n",
    "    # prim_df.to_csv(out_dir/\"primary_blocker_per_block.csv\", index=False)\n",
    "\n",
    "    # ── 4 · UpSet plot of sub‑flag combinations that *ever* fail ────────\n",
    "    failed = fail[flag_cols].eq(0)                   # True => criterion failed\n",
    "    block_fail = failed.groupby(fail[id_col]).max()  # at any hour in block\n",
    "    upset_data = from_indicators(block_fail.columns, block_fail)\n",
    "\n",
    "    UpSet(upset_data, show_counts=True,\n",
    "          sort_by=\"cardinality\").plot()\n",
    "    plt.suptitle(f\"{crit_name}: combinations of failed sub‑criteria\")\n",
    "    plt.savefig(out_dir/\"upset.png\", dpi=300);  plt.close()\n",
    "\n",
    "    # ── 5 · quick dose distribution (example: NE)  ----------------------\n",
    "    if \"ne_calc_max\" in fail.columns:\n",
    "        sns.histplot(fail[\"ne_calc_max\"], bins=40, kde=False)\n",
    "        plt.axvline(0.2, color=\"red\", ls=\"--\")\n",
    "        plt.title(f\"{crit_name}: hourly max NE dose (only failed blocks)\")\n",
    "        plt.xlabel(\"µg / kg / min\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(out_dir/\"ne_hist.png\", dpi=300);  plt.close()\n",
    "\n",
    "    print(f\"[{crit_name}] figures + CSV written to {out_dir}\\n\")\n",
    "    return summary, prim_df\n",
    "# ────────────────────────────────────────────────────────────\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- define once --------------------------------------------------------\n",
    "team_flags = [\"team_pulse_flag\", \"team_lactate_flag\", \"team_ne_flag\",\n",
    "              \"team_fio2_flag\",  \"team_peep_flag\",    \"team_resp_rate_flag\"]\n",
    "\n",
    "# --- call helper --------------------------------------------------------\n",
    "summary_team, primary_team = analyse_criterion(\n",
    "    final_df,                      # your long hourly dataframe\n",
    "    crit_name  = \"TEAM\",\n",
    "    flag_cols  = team_flags,\n",
    "    master_flag= \"team_flag\",      # overall TEAM eligibility flag\n",
    "    out_dir    = \"../output/final\"   # everything will live in …/figures/team/\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reqd_yellow_fields =[\n",
    "    # Red Flags\n",
    "    'red_resp_spo2_flag', 'red_map_flag', 'red_high_support_flag',\n",
    "    'red_hypertensive_flag', 'red_pulse_high_flag', 'red_pulse_low_flag',\n",
    "    \n",
    "    # Yellow Flags\n",
    "    'yellow_resp_spo2_flag', 'yellow_fio2_flag', 'yellow_resp_rate_flag',\n",
    "    'yellow_peep_flag', 'yellow_map_flag', 'yellow_pulse_flag',\n",
    "    'yellow_lactate_flag',\n",
    "    \n",
    "    # Green Flags\n",
    "    'green_resp_spo2_flag', 'green_resp_rate_flag', 'green_fio2_flag',\n",
    "    'green_peep_flag', 'green_map_flag', 'green_pulse_flag',\n",
    "    'green_lactate_flag', 'green_hr_flag',\n",
    "    \n",
    "    # Composite Flags\n",
    "    'any_red', 'any_yellow', 'any_green', 'all_green',\n",
    "    'all_green_no_red', 'all_green_no_red_yellow',\n",
    "    'all_yellow_no_red_green', 'any_yellow_no_red_green',\n",
    "    'any_yellow_or_green_no_red', 'yellow_resp_flag',\n",
    "    'yellow_cardio_flag', 'yellow_all_green', 'yellow_not_all_green'\n",
    "]\n",
    "\n",
    "summary_yel, primary_yel = analyse_criterion(\n",
    "    final_df,\n",
    "    crit_name  = \"Yellow\",\n",
    "    flag_cols  = reqd_yellow_fields,\n",
    "    master_flag= \"any_yellow_or_green_no_red\"          # <- whatever your overall column is called\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reqd_patel_fields = [\n",
    "                    'min_map', 'max_map','max_sbp', 'min_sbp',\n",
    "                   'min_heart_rate','max_heart_rate', 'min_respiratory_rate','min_spo2', \n",
    "                    'max_respiratory_rate','patel_map_flag','patel_sbp_flag','patel_pulse_flag', \n",
    "                    'patel_resp_rate_flag' , 'patel_spo2_flag', 'patel_resp_flag', 'patel_cardio_flag', 'patel_flag' ]\n",
    "\n",
    "summary_patel, primary_patel = analyse_criterion(\n",
    "    final_df,\n",
    "    crit_name  = \"Patel\",\n",
    "    flag_cols  = reqd_patel_fields,\n",
    "    master_flag= \"patel_flag\"          \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reqd_green_fields =[\n",
    "    # Clinical Measurements\n",
    "    'min_spo2', 'min_map', 'max_map', 'ne_calc_last', 'max_sbp',\n",
    "    'max_heart_rate', 'min_heart_rate', 'min_fio2_set',\n",
    "    'max_resp_rate_obs', 'min_peep_set', 'lactate',\n",
    "\n",
    "    \n",
    "    # Green Flags\n",
    "    'green_resp_spo2_flag', 'green_resp_rate_flag', 'green_fio2_flag',\n",
    "    'green_peep_flag', 'green_map_flag', 'green_pulse_flag',\n",
    "    'green_lactate_flag', 'green_hr_flag',\n",
    "    \n",
    "    # Composite Flags\n",
    "     'all_green',\n",
    "    'all_green_no_red', \n",
    "]\n",
    "\n",
    "summary_green, primary_green = analyse_criterion(\n",
    "    final_df,\n",
    "    crit_name  = \"Green\",\n",
    "    flag_cols  = reqd_green_fields,\n",
    "    master_flag= \"all_green\"          \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KM Survival Analysis: Time to eligibility from intubation\n",
    "\n",
    "This analysis includes a 4 hour \"cool off\" period\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Filter the DataFrame for rows where the Patel criteria are met\n",
    "patel_eligible_df = business_hours_df[business_hours_df['patel_flag'] == 1]\n",
    "\n",
    "# Find the first time each patient meets the Patel criteria\n",
    "first_eligibility_times_patel = patel_eligible_df.groupby('hospitalization_id')['time_from_vent'].min().reset_index()\n",
    "\n",
    "# Create the survival analysis dataset\n",
    "survival_analysis_df_patel = business_hours_df[['hospitalization_id']].drop_duplicates().copy()\n",
    "\n",
    "# Merge with the first eligibility times\n",
    "survival_analysis_df_patel = pd.merge(survival_analysis_df_patel, first_eligibility_times_patel, on='hospitalization_id', how='left')\n",
    "\n",
    "# Fill NaN values with 27 for patients who were never eligible\n",
    "survival_analysis_df_patel['time_from_vent'] = survival_analysis_df_patel['time_from_vent'].fillna(27)\n",
    "\n",
    "# Create the 'eligible' column\n",
    "survival_analysis_df_patel['eligible'] = (survival_analysis_df_patel['time_from_vent'] != 27).astype(int)\n",
    "\n",
    "# Rename columns\n",
    "survival_analysis_df_patel.rename(columns={'time_from_vent': 'time_to_first_eligibility'}, inplace=True)\n",
    "\n",
    "# Add +1 to time_to_first_eligibility\n",
    "survival_analysis_df_patel['time_to_first_eligibility'] = survival_analysis_df_patel['time_to_first_eligibility'] + 1\n",
    "\n",
    "# Display the final dataset\n",
    "print(survival_analysis_df_patel.head())\n",
    "\n",
    "# Repeat the process for TEAM criteria\n",
    "team_eligible_df = business_hours_df[business_hours_df['team_flag'] == 1]\n",
    "first_eligibility_times_team = team_eligible_df.groupby('hospitalization_id')['time_from_vent'].min().reset_index()\n",
    "survival_analysis_df_team = business_hours_df[['hospitalization_id']].drop_duplicates().copy()\n",
    "survival_analysis_df_team = pd.merge(survival_analysis_df_team, first_eligibility_times_team, on='hospitalization_id', how='left')\n",
    "survival_analysis_df_team['time_from_vent'] = survival_analysis_df_team['time_from_vent'].fillna(27)\n",
    "survival_analysis_df_team['eligible'] = (survival_analysis_df_team['time_from_vent'] != 27).astype(int)\n",
    "survival_analysis_df_team.rename(columns={'time_from_vent': 'time_to_first_eligibility'}, inplace=True)\n",
    "survival_analysis_df_team['time_to_first_eligibility'] = survival_analysis_df_team['time_to_first_eligibility'] + 1\n",
    "\n",
    "# Repeat the process for Yellow criteria\n",
    "yellow_eligible_df = business_hours_df[business_hours_df['any_yellow_or_green_no_red'] == 1]\n",
    "first_eligibility_times_yellow = yellow_eligible_df.groupby('hospitalization_id')['time_from_vent'].min().reset_index()\n",
    "survival_analysis_df_yellow = business_hours_df[['hospitalization_id']].drop_duplicates().copy()\n",
    "survival_analysis_df_yellow = pd.merge(survival_analysis_df_yellow, first_eligibility_times_yellow, on='hospitalization_id', how='left')\n",
    "survival_analysis_df_yellow['time_from_vent'] = survival_analysis_df_yellow['time_from_vent'].fillna(27)\n",
    "survival_analysis_df_yellow['eligible'] = (survival_analysis_df_yellow['time_from_vent'] != 27).astype(int)\n",
    "survival_analysis_df_yellow.rename(columns={'time_from_vent': 'time_to_first_eligibility'}, inplace=True)\n",
    "survival_analysis_df_yellow['time_to_first_eligibility'] = survival_analysis_df_yellow['time_to_first_eligibility'] + 1\n",
    "\n",
    "# Display the final datasets\n",
    "print(survival_analysis_df_team.head())\n",
    "print(survival_analysis_df_yellow.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the KaplanMeierFitter for Patel, TEAM and Yellow criteria\n",
    "kmf_patel = KaplanMeierFitter()\n",
    "kmf_team = KaplanMeierFitter()\n",
    "kmf_yellow = KaplanMeierFitter()\n",
    "\n",
    "# fit the data for Patel criteria\n",
    "kmf_patel.fit(durations=survival_analysis_df_patel['time_to_first_eligibility'], event_observed=survival_analysis_df_patel['eligible'], label='Patel Criteria')\n",
    "\n",
    "# Fit the data for TEAM criteria\n",
    "kmf_team.fit(durations=survival_analysis_df_team['time_to_first_eligibility'], event_observed=survival_analysis_df_team['eligible'], label='TEAM Criteria')\n",
    "\n",
    "# Fit the data for Yellow criteria\n",
    "kmf_yellow.fit(durations=survival_analysis_df_yellow['time_to_first_eligibility'], event_observed=survival_analysis_df_yellow['eligible'], label='Yellow Criteria')\n",
    "\n",
    "# Plot the cumulative incidence function for all criteria\n",
    "ax = kmf_patel.plot_cumulative_density()\n",
    "kmf_team.plot_cumulative_density(ax=ax)\n",
    "kmf_yellow.plot_cumulative_density(ax=ax)\n",
    "\n",
    "plt.title('Cumulative Incidence Function for Time to First Eligibility')\n",
    "plt.xlabel('Time to First Eligibility from Intubation (hours)')\n",
    "plt.ylabel('Cumulative Incidence Probability')\n",
    "plt.legend()\n",
    "\n",
    "# Save the plot\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "plt.savefig(f'../output/final/graphs/cif_{pyCLIF.helper[\"site_name\"]}_{timestamp}.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Estimate the median time to first eligibility for all criteria\n",
    "median_time_to_first_eligibility_patel = kmf_patel.median_survival_time_\n",
    "median_time_to_first_eligibility_team = kmf_team.median_survival_time_\n",
    "median_time_to_first_eligibility_yellow = kmf_yellow.median_survival_time_\n",
    "\n",
    "print(f\"Median time to first eligibility (Patel): {median_time_to_first_eligibility_patel} hours\")\n",
    "print(f\"Median time to first eligibility (TEAM): {median_time_to_first_eligibility_team} hours\")\n",
    "print(f\"Median time to first eligibility (Yellow): {median_time_to_first_eligibility_yellow} hours\")\n",
    "\n",
    "# Calculate the cumulative incidence function value at time = 1 for all criteria\n",
    "cif_value_at_1_patel = 1 - kmf_patel.predict(1)\n",
    "cif_value_at_1_team = 1 - kmf_team.predict(1)\n",
    "cif_value_at_1_yellow = 1 - kmf_yellow.predict(1)\n",
    "\n",
    "print(f\"Cumulative Incidence Function value at time = 1 (Patel): {cif_value_at_1_patel:.0%}\")\n",
    "print(f\"Cumulative Incidence Function value at time = 1 (TEAM): {cif_value_at_1_team:.0%}\")\n",
    "print(f\"Cumulative Incidence Function value at time = 1 (Yellow): {cif_value_at_1_yellow:.0%}\")\n",
    "\n",
    "\n",
    "# Save the cumulative incidence function data to CSV files\n",
    "cif_patel = kmf_patel.cumulative_density_.reset_index()\n",
    "cif_team = kmf_team.cumulative_density_.reset_index()\n",
    "cif_yellow = kmf_yellow.cumulative_density_.reset_index()\n",
    "\n",
    "# Add a column to identify the criteria\n",
    "cif_patel['Criteria'] = 'Patel'\n",
    "cif_team['Criteria'] = 'TEAM'\n",
    "cif_yellow['Criteria'] = 'Yellow'\n",
    "\n",
    "# Get the site name from pyCLIF helper\n",
    "site_name = pyCLIF.helper[\"site_name\"]\n",
    "\n",
    "cif_patel['Site'] = site_name\n",
    "cif_team['Site'] = site_name\n",
    "cif_yellow['Site'] = site_name\n",
    "\n",
    "# Save to CSV files with site name in the file names\n",
    "timestamp = datetime.now().date()\n",
    "cif_patel.to_csv(f'../output/final/cif_patel_{pyCLIF.helper[\"site_name\"]}_{timestamp}.csv', index=False)\n",
    "cif_team.to_csv(f'../output/final/cif_team_{pyCLIF.helper[\"site_name\"]}_{timestamp}.csv', index=False)\n",
    "cif_yellow.to_csv(f'../output/final/cif_yellow_{pyCLIF.helper[\"site_name\"]}_{timestamp}.csv', index=False)\n",
    "\n",
    "# Calculate 95% CI for each criteria\n",
    "ci_patel_ci = kmf_patel.confidence_interval_cumulative_density_.reset_index()\n",
    "ci_team_ci = kmf_team.confidence_interval_cumulative_density_.reset_index()\n",
    "ci_yellow_ci = kmf_yellow.confidence_interval_cumulative_density_.reset_index()\n",
    "\n",
    "# Add a column to identify the criteria\n",
    "ci_patel_ci['Criteria'] = 'Patel'\n",
    "ci_team_ci['Criteria'] = 'TEAM'\n",
    "ci_yellow_ci['Criteria'] = 'Yellow'\n",
    "ci_patel_ci['Site'] = site_name\n",
    "ci_team_ci['Site'] = site_name\n",
    "ci_yellow_ci['Site'] = site_name\n",
    "# Save to CSV files with site name in the file names\n",
    "timestamp = datetime.now().date()\n",
    "ci_patel_ci.to_csv(f'../output/final/cif_patel_ci_{pyCLIF.helper[\"site_name\"]}_{timestamp}.csv', index=False)\n",
    "ci_team_ci.to_csv(f'../output/final/cif_team_ci_{pyCLIF.helper[\"site_name\"]}_{timestamp}.csv', index=False)\n",
    "ci_yellow_ci.to_csv(f'../output/final/cif_yellow_ci_{pyCLIF.helper[\"site_name\"]}_{timestamp}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Filter the DataFrame for rows where the Patel criteria are met\n",
    "# patel_eligible_df = business_hours_df[business_hours_df['patel_flag'] == 1]\n",
    "\n",
    "# # Find the first time each patient meets the Patel criteria\n",
    "# first_eligibility_times_patel = patel_eligible_df.groupby('hospitalization_id')['time_biz'].min().reset_index()\n",
    "\n",
    "# # Create the survival analysis dataset\n",
    "# survival_analysis_df_patel = business_hours_df[['hospitalization_id']].drop_duplicates().copy()\n",
    "\n",
    "# # Merge with the first eligibility times\n",
    "# survival_analysis_df_patel = pd.merge(survival_analysis_df_patel, first_eligibility_times_patel, on='hospitalization_id', how='left')\n",
    "\n",
    "# # Fill NaN values with 27 for patients who were never eligible\n",
    "# survival_analysis_df_patel['time_biz'] = survival_analysis_df_patel['time_biz'].fillna(27)\n",
    "\n",
    "# # Create the 'eligible' column\n",
    "# survival_analysis_df_patel['eligible'] = (survival_analysis_df_patel['time_biz'] != 27).astype(int)\n",
    "\n",
    "# # Rename columns\n",
    "# survival_analysis_df_patel.rename(columns={'time_biz': 'time_to_first_eligibility'}, inplace=True)\n",
    "\n",
    "# # Add +1 to time_to_first_eligibility\n",
    "# survival_analysis_df_patel['time_to_first_eligibility'] = survival_analysis_df_patel['time_to_first_eligibility'] + 1\n",
    "\n",
    "# # Display the final dataset\n",
    "# print(survival_analysis_df_patel.head())\n",
    "\n",
    "# # Repeat the process for TEAM criteria\n",
    "# team_eligible_df = business_hours_df[business_hours_df['team_flag'] == 1]\n",
    "# first_eligibility_times_team = team_eligible_df.groupby('hospitalization_id')['time_biz'].min().reset_index()\n",
    "# survival_analysis_df_team = business_hours_df[['hospitalization_id']].drop_duplicates().copy()\n",
    "# survival_analysis_df_team = pd.merge(survival_analysis_df_team, first_eligibility_times_team, on='hospitalization_id', how='left')\n",
    "# survival_analysis_df_team['time_biz'] = survival_analysis_df_team['time_biz'].fillna(27)\n",
    "# survival_analysis_df_team['eligible'] = (survival_analysis_df_team['time_biz'] != 27).astype(int)\n",
    "# survival_analysis_df_team.rename(columns={'time_biz': 'time_to_first_eligibility'}, inplace=True)\n",
    "# survival_analysis_df_team['time_to_first_eligibility'] = survival_analysis_df_team['time_to_first_eligibility'] + 1\n",
    "\n",
    "# # Repeat the process for Yellow criteria\n",
    "# yellow_eligible_df = business_hours_df[business_hours_df['any_yellow_or_green_no_red'] == 1]\n",
    "# first_eligibility_times_yellow = yellow_eligible_df.groupby('hospitalization_id')['time_biz'].min().reset_index()\n",
    "# survival_analysis_df_yellow = business_hours_df[['hospitalization_id']].drop_duplicates().copy()\n",
    "# survival_analysis_df_yellow = pd.merge(survival_analysis_df_yellow, first_eligibility_times_yellow, on='hospitalization_id', how='left')\n",
    "# survival_analysis_df_yellow['time_biz'] = survival_analysis_df_yellow['time_biz'].fillna(27)\n",
    "# survival_analysis_df_yellow['eligible'] = (survival_analysis_df_yellow['time_biz'] != 27).astype(int)\n",
    "# survival_analysis_df_yellow.rename(columns={'time_biz': 'time_to_first_eligibility'}, inplace=True)\n",
    "# survival_analysis_df_yellow['time_to_first_eligibility'] = survival_analysis_df_yellow['time_to_first_eligibility'] + 1\n",
    "\n",
    "# # Display the final datasets\n",
    "# print(survival_analysis_df_team.head())\n",
    "# print(survival_analysis_df_yellow.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the KaplanMeierFitter for Patel, TEAM and Yellow criteria\n",
    "kmf_patel = KaplanMeierFitter()\n",
    "kmf_team = KaplanMeierFitter()\n",
    "kmf_yellow = KaplanMeierFitter()\n",
    "\n",
    "# fit the data for Patel criteria\n",
    "kmf_patel.fit(durations=survival_analysis_df_patel['time_to_first_eligibility'], event_observed=survival_analysis_df_patel['eligible'], label='Patel Criteria')\n",
    "\n",
    "# Fit the data for TEAM criteria\n",
    "kmf_team.fit(durations=survival_analysis_df_team['time_to_first_eligibility'], event_observed=survival_analysis_df_team['eligible'], label='TEAM Criteria')\n",
    "\n",
    "# Fit the data for Yellow criteria\n",
    "kmf_yellow.fit(durations=survival_analysis_df_yellow['time_to_first_eligibility'], event_observed=survival_analysis_df_yellow['eligible'], label='Yellow Criteria')\n",
    "\n",
    "# Plot the cumulative incidence function for all criteria\n",
    "ax = kmf_patel.plot_cumulative_density()\n",
    "kmf_team.plot_cumulative_density(ax=ax)\n",
    "kmf_yellow.plot_cumulative_density(ax=ax)\n",
    "\n",
    "plt.title('Cumulative Incidence Function for Time to First Eligibility')\n",
    "plt.xlabel('Time to First Eligibility (business hours)')\n",
    "plt.ylabel('Cumulative Incidence Probability')\n",
    "# Save the plot\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "plt.savefig(f'../output/final/graphs/cif_b_hours_{pyCLIF.helper[\"site_name\"]}_{timestamp}.png')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Estimate the median time to first eligibility for all criteria\n",
    "median_time_to_first_eligibility_patel = kmf_patel.median_survival_time_\n",
    "median_time_to_first_eligibility_team = kmf_team.median_survival_time_\n",
    "median_time_to_first_eligibility_yellow = kmf_yellow.median_survival_time_\n",
    "\n",
    "print(f\"Median time to first eligibility (Patel): {median_time_to_first_eligibility_patel} hours\")\n",
    "print(f\"Median time to first eligibility (TEAM): {median_time_to_first_eligibility_team} hours\")\n",
    "print(f\"Median time to first eligibility (Yellow): {median_time_to_first_eligibility_yellow} hours\")\n",
    "\n",
    "# Calculate the cumulative incidence function value at time = 1 for all criteria\n",
    "cif_value_at_1_patel = 1 - kmf_patel.predict(1)\n",
    "cif_value_at_1_team = 1 - kmf_team.predict(1)\n",
    "cif_value_at_1_yellow = 1 - kmf_yellow.predict(1)\n",
    "\n",
    "print(f\"Cumulative Incidence Function value at time = 1 (Patel): {cif_value_at_1_patel:.0%}\")\n",
    "print(f\"Cumulative Incidence Function value at time = 1 (TEAM): {cif_value_at_1_team:.0%}\")\n",
    "print(f\"Cumulative Incidence Function value at time = 1 (Yellow): {cif_value_at_1_yellow:.0%}\")\n",
    "\n",
    "\n",
    "# Save the cumulative incidence function data to CSV files\n",
    "cif_patel = kmf_patel.cumulative_density_.reset_index()\n",
    "cif_team = kmf_team.cumulative_density_.reset_index()\n",
    "cif_yellow = kmf_yellow.cumulative_density_.reset_index()\n",
    "\n",
    "# Add a column to identify the criteria\n",
    "cif_patel['Criteria'] = 'Patel'\n",
    "cif_team['Criteria'] = 'TEAM'\n",
    "cif_yellow['Criteria'] = 'Yellow'\n",
    "\n",
    "\n",
    "# Get the site name from pyCLIF helper\n",
    "site_name = pyCLIF.helper[\"site_name\"]\n",
    "\n",
    "cif_patel['Site'] = site_name\n",
    "cif_team['Site'] = site_name\n",
    "cif_yellow['Site'] = site_name\n",
    "\n",
    "# Save to CSV files with site name in the file names\n",
    "timestamp = datetime.now().date()\n",
    "cif_patel.to_csv(f'../output/final/cif_b_hours_patel_{pyCLIF.helper[\"site_name\"]}_{timestamp}.csv', index=False)\n",
    "cif_team.to_csv(f'../output/final/cif_b_hours_team_{pyCLIF.helper[\"site_name\"]}_{timestamp}.csv', index=False)\n",
    "cif_yellow.to_csv(f'../output/final/cif_b_hours_yellow_{pyCLIF.helper[\"site_name\"]}_{timestamp}.csv', index=False)\n",
    "\n",
    "# Calculate 95% CI for each criteria\n",
    "ci_patel_ci = kmf_patel.confidence_interval_cumulative_density_.reset_index()\n",
    "ci_team_ci = kmf_team.confidence_interval_cumulative_density_.reset_index()\n",
    "ci_yellow_ci = kmf_yellow.confidence_interval_cumulative_density_.reset_index()\n",
    "\n",
    "# Add a column to identify the criteria\n",
    "ci_patel_ci['Criteria'] = 'Patel'\n",
    "ci_team_ci['Criteria'] = 'TEAM'\n",
    "ci_yellow_ci['Criteria'] = 'Yellow'\n",
    "ci_patel_ci['Site'] = site_name\n",
    "ci_team_ci['Site'] = site_name\n",
    "ci_yellow_ci['Site'] = site_name\n",
    "# Save to CSV files with site name in the file names\n",
    "timestamp = datetime.now().date()\n",
    "ci_patel_ci.to_csv(f'../output/final/cif_b_hours_patel_ci_{pyCLIF.helper[\"site_name\"]}_{timestamp}.csv', index=False)\n",
    "ci_team_ci.to_csv(f'../output/final/cif_b_hours_team_ci_{pyCLIF.helper[\"site_name\"]}_{timestamp}.csv', index=False)\n",
    "ci_yellow_ci.to_csv(f'../output/final/cif_b_hours_yellow_ci_{pyCLIF.helper[\"site_name\"]}_{timestamp}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note: `time_from_vent_adjusted` is -1 until hour 4, then it counts up from 0. This builds in the 4-hour cool off period.\n",
    "business_hours_df = final_df[(final_df['time_from_vent_adjusted'] >= 0)]\n",
    "#business_hours_df = final_df[(final_df['time_from_vent'] >= 0) & (final_df['time_from_vent'] < 72)]\n",
    "\n",
    "# recorded_hour is the hour of the day (0-23), so business hours are 8 (8 AM) - 17 (5 PM).\n",
    "business_hours_df = business_hours_df[(business_hours_df['recorded_hour'] >= 8) & (business_hours_df['recorded_hour'] < 17)].copy()\n",
    "business_hours_df['time_biz'] = business_hours_df.groupby('hospitalization_id').cumcount()\n",
    "\n",
    "# Filter the DataFrame for rows where the Patel criteria are met\n",
    "patel_eligible_df = business_hours_df[business_hours_df['patel_flag'] == 1]\n",
    "\n",
    "# Find the first time each patient meets the Patel criteria\n",
    "first_eligibility_times_patel = patel_eligible_df.groupby('hospitalization_id')['time_biz'].min().reset_index()\n",
    "\n",
    "# Create the survival analysis dataset\n",
    "survival_analysis_df_patel = business_hours_df[['hospitalization_id']].drop_duplicates().copy()\n",
    "\n",
    "# Merge with the first eligibility times\n",
    "survival_analysis_df_patel = pd.merge(survival_analysis_df_patel, first_eligibility_times_patel, on='hospitalization_id', how='left')\n",
    "\n",
    "# Fill NaN values with 27 for patients who were never eligible\n",
    "survival_analysis_df_patel['time_biz'] = survival_analysis_df_patel['time_biz'].fillna(27)\n",
    "\n",
    "# Create the 'eligible' column\n",
    "survival_analysis_df_patel['eligible'] = (survival_analysis_df_patel['time_biz'] != 27).astype(int)\n",
    "\n",
    "# Rename columns\n",
    "survival_analysis_df_patel.rename(columns={'time_biz': 'time_to_first_eligibility'}, inplace=True)\n",
    "\n",
    "# Add +1 to time_to_first_eligibility\n",
    "survival_analysis_df_patel['time_to_first_eligibility'] = survival_analysis_df_patel['time_to_first_eligibility'] + 1\n",
    "\n",
    "# Display the final dataset\n",
    "print(survival_analysis_df_patel.head())\n",
    "\n",
    "# Repeat the process for TEAM criteria\n",
    "team_eligible_df = business_hours_df[business_hours_df['team_flag'] == 1]\n",
    "first_eligibility_times_team = team_eligible_df.groupby('hospitalization_id')['time_biz'].min().reset_index()\n",
    "survival_analysis_df_team = business_hours_df[['hospitalization_id']].drop_duplicates().copy()\n",
    "survival_analysis_df_team = pd.merge(survival_analysis_df_team, first_eligibility_times_team, on='hospitalization_id', how='left')\n",
    "survival_analysis_df_team['time_biz'] = survival_analysis_df_team['time_biz'].fillna(27)\n",
    "survival_analysis_df_team['eligible'] = (survival_analysis_df_team['time_biz'] != 27).astype(int)\n",
    "survival_analysis_df_team.rename(columns={'time_biz': 'time_to_first_eligibility'}, inplace=True)\n",
    "survival_analysis_df_team['time_to_first_eligibility'] = survival_analysis_df_team['time_to_first_eligibility'] + 1\n",
    "\n",
    "# Repeat the process for Yellow criteria\n",
    "yellow_eligible_df = business_hours_df[business_hours_df['any_yellow_or_green_no_red'] == 1]\n",
    "first_eligibility_times_yellow = yellow_eligible_df.groupby('hospitalization_id')['time_biz'].min().reset_index()\n",
    "survival_analysis_df_yellow = business_hours_df[['hospitalization_id']].drop_duplicates().copy()\n",
    "survival_analysis_df_yellow = pd.merge(survival_analysis_df_yellow, first_eligibility_times_yellow, on='hospitalization_id', how='left')\n",
    "survival_analysis_df_yellow['time_biz'] = survival_analysis_df_yellow['time_biz'].fillna(27)\n",
    "survival_analysis_df_yellow['eligible'] = (survival_analysis_df_yellow['time_biz'] != 27).astype(int)\n",
    "survival_analysis_df_yellow.rename(columns={'time_biz': 'time_to_first_eligibility'}, inplace=True)\n",
    "survival_analysis_df_yellow['time_to_first_eligibility'] = survival_analysis_df_yellow['time_to_first_eligibility'] + 1\n",
    "\n",
    "# Display the final datasets\n",
    "print(survival_analysis_df_team.head())\n",
    "print(survival_analysis_df_yellow.head())\n",
    "\n",
    "# Initialize the KaplanMeierFitter for Patel, TEAM and Yellow criteria\n",
    "kmf_patel = KaplanMeierFitter()\n",
    "kmf_team = KaplanMeierFitter()\n",
    "kmf_yellow = KaplanMeierFitter()\n",
    "\n",
    "# fit the data for Patel criteria\n",
    "kmf_patel.fit(durations=survival_analysis_df_patel['time_to_first_eligibility'], event_observed=survival_analysis_df_patel['eligible'], label='Patel Criteria')\n",
    "\n",
    "# Fit the data for TEAM criteria\n",
    "kmf_team.fit(durations=survival_analysis_df_team['time_to_first_eligibility'], event_observed=survival_analysis_df_team['eligible'], label='TEAM Criteria')\n",
    "\n",
    "# Fit the data for Yellow criteria\n",
    "kmf_yellow.fit(durations=survival_analysis_df_yellow['time_to_first_eligibility'], event_observed=survival_analysis_df_yellow['eligible'], label='Yellow Criteria')\n",
    "\n",
    "# Plot the cumulative incidence function for all criteria\n",
    "ax = kmf_patel.plot_cumulative_density()\n",
    "kmf_team.plot_cumulative_density(ax=ax)\n",
    "kmf_yellow.plot_cumulative_density(ax=ax)\n",
    "\n",
    "plt.title('Cumulative Incidence Function for Time to First Eligibility')\n",
    "plt.xlabel('Time to First Eligibility (business hours)')\n",
    "plt.ylabel('Cumulative Incidence Probability')\n",
    "# Save the plot\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "plt.savefig(f'../output/final/graphs/cif_b_hours_{pyCLIF.helper[\"site_name\"]}_{timestamp}.png')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Estimate the median time to first eligibility for all criteria\n",
    "median_time_to_first_eligibility_patel = kmf_patel.median_survival_time_\n",
    "median_time_to_first_eligibility_team = kmf_team.median_survival_time_\n",
    "median_time_to_first_eligibility_yellow = kmf_yellow.median_survival_time_\n",
    "\n",
    "print(f\"Median time to first eligibility (Patel): {median_time_to_first_eligibility_patel} hours\")\n",
    "print(f\"Median time to first eligibility (TEAM): {median_time_to_first_eligibility_team} hours\")\n",
    "print(f\"Median time to first eligibility (Yellow): {median_time_to_first_eligibility_yellow} hours\")\n",
    "\n",
    "# Calculate the cumulative incidence function value at time = 1 for all criteria\n",
    "cif_value_at_1_patel = 1 - kmf_patel.predict(1)\n",
    "cif_value_at_1_team = 1 - kmf_team.predict(1)\n",
    "cif_value_at_1_yellow = 1 - kmf_yellow.predict(1)\n",
    "\n",
    "print(f\"Cumulative Incidence Function value at time = 1 (Patel): {cif_value_at_1_patel:.0%}\")\n",
    "print(f\"Cumulative Incidence Function value at time = 1 (TEAM): {cif_value_at_1_team:.0%}\")\n",
    "print(f\"Cumulative Incidence Function value at time = 1 (Yellow): {cif_value_at_1_yellow:.0%}\")\n",
    "\n",
    "\n",
    "# Save the cumulative incidence function data to CSV files\n",
    "cif_patel = kmf_patel.cumulative_density_.reset_index()\n",
    "cif_team = kmf_team.cumulative_density_.reset_index()\n",
    "cif_yellow = kmf_yellow.cumulative_density_.reset_index()\n",
    "\n",
    "# Add a column to identify the criteria\n",
    "cif_patel['Criteria'] = 'Patel'\n",
    "cif_team['Criteria'] = 'TEAM'\n",
    "cif_yellow['Criteria'] = 'Yellow'\n",
    "\n",
    "\n",
    "# Get the site name from pyCLIF helper\n",
    "site_name = pyCLIF.helper[\"site_name\"]\n",
    "\n",
    "cif_patel['Site'] = site_name\n",
    "cif_team['Site'] = site_name\n",
    "cif_yellow['Site'] = site_name\n",
    "\n",
    "# Save to CSV files with site name in the file names\n",
    "timestamp = datetime.now().date()\n",
    "cif_patel.to_csv(f'../output/final/cif_b_hours_patel_{pyCLIF.helper[\"site_name\"]}_{timestamp}.csv', index=False)\n",
    "cif_team.to_csv(f'../output/final/cif_b_hours_team_{pyCLIF.helper[\"site_name\"]}_{timestamp}.csv', index=False)\n",
    "cif_yellow.to_csv(f'../output/final/cif_b_hours_yellow_{pyCLIF.helper[\"site_name\"]}_{timestamp}.csv', index=False)\n",
    "\n",
    "# Calculate 95% CI for each criteria\n",
    "ci_patel_ci = kmf_patel.confidence_interval_cumulative_density_.reset_index()\n",
    "ci_team_ci = kmf_team.confidence_interval_cumulative_density_.reset_index()\n",
    "ci_yellow_ci = kmf_yellow.confidence_interval_cumulative_density_.reset_index()\n",
    "\n",
    "# Add a column to identify the criteria\n",
    "ci_patel_ci['Criteria'] = 'Patel'\n",
    "ci_team_ci['Criteria'] = 'TEAM'\n",
    "ci_yellow_ci['Criteria'] = 'Yellow'\n",
    "ci_patel_ci['Site'] = site_name\n",
    "ci_team_ci['Site'] = site_name\n",
    "ci_yellow_ci['Site'] = site_name\n",
    "# Save to CSV files with site name in the file names\n",
    "timestamp = datetime.now().date()\n",
    "ci_patel_ci.to_csv(f'../output/final/cif_b_hours_patel_ci_{pyCLIF.helper[\"site_name\"]}_{timestamp}.csv', index=False)\n",
    "ci_team_ci.to_csv(f'../output/final/cif_b_hours_team_ci_{pyCLIF.helper[\"site_name\"]}_{timestamp}.csv', index=False)\n",
    "ci_yellow_ci.to_csv(f'../output/final/cif_b_hours_yellow_ci_{pyCLIF.helper[\"site_name\"]}_{timestamp}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Updated for blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame for rows where the Patel criteria are met\n",
    "patel_eligible_df = final_df[final_df['patel_flag'] == 1]\n",
    "\n",
    "# Find the first time each patient meets the Patel criteria\n",
    "first_eligibility_times_patel = patel_eligible_df.groupby('encounter_block')['time_from_vent'].min().reset_index()\n",
    "\n",
    "# Create the survival analysis dataset\n",
    "survival_analysis_df_patel = final_df[['encounter_block']].drop_duplicates().copy()\n",
    "\n",
    "# Merge with the first eligibility times\n",
    "survival_analysis_df_patel = pd.merge(survival_analysis_df_patel, first_eligibility_times_patel, on='encounter_block', how='left')\n",
    "\n",
    "# Fill NaN values with 27 for patients who were never eligible\n",
    "survival_analysis_df_patel['time_from_vent'] = survival_analysis_df_patel['time_from_vent'].fillna(27)\n",
    "\n",
    "# Create the 'eligible' column\n",
    "survival_analysis_df_patel['eligible'] = (survival_analysis_df_patel['time_from_vent'] != 27).astype(int)\n",
    "\n",
    "# Rename columns\n",
    "survival_analysis_df_patel.rename(columns={'time_from_vent': 'time_to_first_eligibility'}, inplace=True)\n",
    "\n",
    "# Add +1 to time_to_first_eligibility\n",
    "survival_analysis_df_patel['time_to_first_eligibility'] = survival_analysis_df_patel['time_to_first_eligibility'] + 1\n",
    "\n",
    "# Display the final dataset\n",
    "print(survival_analysis_df_patel.head())\n",
    "\n",
    "# Repeat the process for TEAM criteria\n",
    "team_eligible_df = final_df[final_df['team_flag'] == 1]\n",
    "first_eligibility_times_team = team_eligible_df.groupby('encounter_block')['time_from_vent'].min().reset_index()\n",
    "survival_analysis_df_team = final_df[['encounter_block']].drop_duplicates().copy()\n",
    "survival_analysis_df_team = pd.merge(survival_analysis_df_team, first_eligibility_times_team, on='encounter_block', how='left')\n",
    "survival_analysis_df_team['time_from_vent'] = survival_analysis_df_team['time_from_vent'].fillna(27)\n",
    "survival_analysis_df_team['eligible'] = (survival_analysis_df_team['time_from_vent'] != 27).astype(int)\n",
    "survival_analysis_df_team.rename(columns={'time_from_vent': 'time_to_first_eligibility'}, inplace=True)\n",
    "survival_analysis_df_team['time_to_first_eligibility'] = survival_analysis_df_team['time_to_first_eligibility'] + 1\n",
    "\n",
    "# Repeat the process for Yellow criteria\n",
    "yellow_eligible_df = final_df[final_df['any_yellow_or_green_no_red'] == 1]\n",
    "first_eligibility_times_yellow = yellow_eligible_df.groupby('encounter_block')['time_from_vent'].min().reset_index()\n",
    "survival_analysis_df_yellow = final_df[['encounter_block']].drop_duplicates().copy()\n",
    "survival_analysis_df_yellow = pd.merge(survival_analysis_df_yellow, first_eligibility_times_yellow, on='encounter_block', how='left')\n",
    "survival_analysis_df_yellow['time_from_vent'] = survival_analysis_df_yellow['time_from_vent'].fillna(27)\n",
    "survival_analysis_df_yellow['eligible'] = (survival_analysis_df_yellow['time_from_vent'] != 27).astype(int)\n",
    "survival_analysis_df_yellow.rename(columns={'time_from_vent': 'time_to_first_eligibility'}, inplace=True)\n",
    "survival_analysis_df_yellow['time_to_first_eligibility'] = survival_analysis_df_yellow['time_to_first_eligibility'] + 1\n",
    "\n",
    "# Display the final datasets\n",
    "print(survival_analysis_df_team.head())\n",
    "print(survival_analysis_df_yellow.head())\n",
    "\n",
    "# Initialize the KaplanMeierFitter for Patel, TEAM and Yellow criteria\n",
    "kmf_patel = KaplanMeierFitter()\n",
    "kmf_team = KaplanMeierFitter()\n",
    "kmf_yellow = KaplanMeierFitter()\n",
    "\n",
    "# fit the data for Patel criteria\n",
    "kmf_patel.fit(durations=survival_analysis_df_patel['time_to_first_eligibility'], event_observed=survival_analysis_df_patel['eligible'], label='Patel Criteria')\n",
    "\n",
    "# Fit the data for TEAM criteria\n",
    "kmf_team.fit(durations=survival_analysis_df_team['time_to_first_eligibility'], event_observed=survival_analysis_df_team['eligible'], label='TEAM Criteria')\n",
    "\n",
    "# Fit the data for Yellow criteria\n",
    "kmf_yellow.fit(durations=survival_analysis_df_yellow['time_to_first_eligibility'], event_observed=survival_analysis_df_yellow['eligible'], label='Yellow Criteria')\n",
    "\n",
    "# Plot the cumulative incidence function for all criteria\n",
    "ax = kmf_patel.plot_cumulative_density()\n",
    "kmf_team.plot_cumulative_density(ax=ax)\n",
    "kmf_yellow.plot_cumulative_density(ax=ax)\n",
    "\n",
    "plt.title('Cumulative Incidence Function for Time to First Eligibility')\n",
    "plt.xlabel('Time to First Eligibility from Intubation (hours)')\n",
    "plt.ylabel('Cumulative Incidence Probability')\n",
    "plt.legend()\n",
    "\n",
    "# Save the plot\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "plt.savefig(f'../output/final/graphs/cif_{pyCLIF.helper[\"site_name\"]}_{timestamp}.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Estimate the median time to first eligibility for all criteria\n",
    "median_time_to_first_eligibility_patel = kmf_patel.median_survival_time_\n",
    "median_time_to_first_eligibility_team = kmf_team.median_survival_time_\n",
    "median_time_to_first_eligibility_yellow = kmf_yellow.median_survival_time_\n",
    "\n",
    "print(f\"Median time to first eligibility (Patel): {median_time_to_first_eligibility_patel} hours\")\n",
    "print(f\"Median time to first eligibility (TEAM): {median_time_to_first_eligibility_team} hours\")\n",
    "print(f\"Median time to first eligibility (Yellow): {median_time_to_first_eligibility_yellow} hours\")\n",
    "\n",
    "# Calculate the cumulative incidence function value at time = 1 for all criteria\n",
    "cif_value_at_1_patel = 1 - kmf_patel.predict(1)\n",
    "cif_value_at_1_team = 1 - kmf_team.predict(1)\n",
    "cif_value_at_1_yellow = 1 - kmf_yellow.predict(1)\n",
    "\n",
    "print(f\"Cumulative Incidence Function value at time = 1 (Patel): {cif_value_at_1_patel:.0%}\")\n",
    "print(f\"Cumulative Incidence Function value at time = 1 (TEAM): {cif_value_at_1_team:.0%}\")\n",
    "print(f\"Cumulative Incidence Function value at time = 1 (Yellow): {cif_value_at_1_yellow:.0%}\")\n",
    "\n",
    "\n",
    "# Save the cumulative incidence function data to CSV files\n",
    "cif_patel = kmf_patel.cumulative_density_.reset_index()\n",
    "cif_team = kmf_team.cumulative_density_.reset_index()\n",
    "cif_yellow = kmf_yellow.cumulative_density_.reset_index()\n",
    "\n",
    "# Add a column to identify the criteria\n",
    "cif_patel['Criteria'] = 'Patel'\n",
    "cif_team['Criteria'] = 'TEAM'\n",
    "cif_yellow['Criteria'] = 'Yellow'\n",
    "\n",
    "# Get the site name from pyCLIF helper\n",
    "site_name = pyCLIF.helper[\"site_name\"]\n",
    "\n",
    "cif_patel['Site'] = site_name\n",
    "cif_team['Site'] = site_name\n",
    "cif_yellow['Site'] = site_name\n",
    "\n",
    "# Save to CSV files with site name in the file names\n",
    "timestamp = datetime.now().date()\n",
    "cif_patel.to_csv(f'../output/final/cif_patel_{pyCLIF.helper[\"site_name\"]}_{timestamp}.csv', index=False)\n",
    "cif_team.to_csv(f'../output/final/cif_team_{pyCLIF.helper[\"site_name\"]}_{timestamp}.csv', index=False)\n",
    "cif_yellow.to_csv(f'../output/final/cif_yellow_{pyCLIF.helper[\"site_name\"]}_{timestamp}.csv', index=False)\n",
    "\n",
    "# Calculate 95% CI for each criteria\n",
    "ci_patel_ci = kmf_patel.confidence_interval_cumulative_density_.reset_index()\n",
    "ci_team_ci = kmf_team.confidence_interval_cumulative_density_.reset_index()\n",
    "ci_yellow_ci = kmf_yellow.confidence_interval_cumulative_density_.reset_index()\n",
    "\n",
    "# Add a column to identify the criteria\n",
    "ci_patel_ci['Criteria'] = 'Patel'\n",
    "ci_team_ci['Criteria'] = 'TEAM'\n",
    "ci_yellow_ci['Criteria'] = 'Yellow'\n",
    "ci_patel_ci['Site'] = site_name\n",
    "ci_team_ci['Site'] = site_name\n",
    "ci_yellow_ci['Site'] = site_name\n",
    "# Save to CSV files with site name in the file names\n",
    "timestamp = datetime.now().date()\n",
    "ci_patel_ci.to_csv(f'../output/final/cif_patel_ci_{pyCLIF.helper[\"site_name\"]}_{timestamp}.csv', index=False)\n",
    "ci_team_ci.to_csv(f'../output/final/cif_team_ci_{pyCLIF.helper[\"site_name\"]}_{timestamp}.csv', index=False)\n",
    "ci_yellow_ci.to_csv(f'../output/final/cif_yellow_ci_{pyCLIF.helper[\"site_name\"]}_{timestamp}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame for rows where the Patel criteria are met\n",
    "patel_eligible_df = final_df[final_df['patel_flag'] == 1]\n",
    "\n",
    "# Find the first time each patient meets the Patel criteria\n",
    "first_eligibility_times_patel = patel_eligible_df.groupby('encounter_block')['time_biz_from_vent'].min().reset_index()\n",
    "\n",
    "# Create the survival analysis dataset\n",
    "survival_analysis_df_patel = final_df[['encounter_block']].drop_duplicates().copy()\n",
    "\n",
    "# Merge with the first eligibility times\n",
    "survival_analysis_df_patel = pd.merge(survival_analysis_df_patel, first_eligibility_times_patel, on='encounter_block', how='left')\n",
    "\n",
    "# Fill NaN values with 27 for patients who were never eligible\n",
    "survival_analysis_df_patel['time_biz_from_vent'] = survival_analysis_df_patel['time_biz_from_vent'].fillna(27)\n",
    "\n",
    "# Create the 'eligible' column\n",
    "survival_analysis_df_patel['eligible'] = (survival_analysis_df_patel['time_biz_from_vent'] != 27).astype(int)\n",
    "\n",
    "# Rename columns\n",
    "survival_analysis_df_patel.rename(columns={'time_biz_from_vent': 'time_to_first_eligibility'}, inplace=True)\n",
    "\n",
    "# Add +1 to time_to_first_eligibility\n",
    "survival_analysis_df_patel['time_to_first_eligibility'] = survival_analysis_df_patel['time_to_first_eligibility'] + 1\n",
    "\n",
    "# Display the final dataset\n",
    "print(survival_analysis_df_patel.head())\n",
    "\n",
    "# Repeat the process for TEAM criteria\n",
    "team_eligible_df = final_df[final_df['team_flag'] == 1]\n",
    "first_eligibility_times_team = team_eligible_df.groupby('encounter_block')['time_biz_from_vent'].min().reset_index()\n",
    "survival_analysis_df_team = final_df[['encounter_block']].drop_duplicates().copy()\n",
    "survival_analysis_df_team = pd.merge(survival_analysis_df_team, first_eligibility_times_team, on='encounter_block', how='left')\n",
    "survival_analysis_df_team['time_biz_from_vent'] = survival_analysis_df_team['time_biz_from_vent'].fillna(27)\n",
    "survival_analysis_df_team['eligible'] = (survival_analysis_df_team['time_biz_from_vent'] != 27).astype(int)\n",
    "survival_analysis_df_team.rename(columns={'time_biz_from_vent': 'time_to_first_eligibility'}, inplace=True)\n",
    "survival_analysis_df_team['time_to_first_eligibility'] = survival_analysis_df_team['time_to_first_eligibility'] + 1\n",
    "\n",
    "# Repeat the process for Yellow criteria\n",
    "yellow_eligible_df = final_df[final_df['any_yellow_or_green_no_red'] == 1]\n",
    "first_eligibility_times_yellow = yellow_eligible_df.groupby('encounter_block')['time_biz_from_vent'].min().reset_index()\n",
    "survival_analysis_df_yellow = final_df[['encounter_block']].drop_duplicates().copy()\n",
    "survival_analysis_df_yellow = pd.merge(survival_analysis_df_yellow, first_eligibility_times_yellow, on='encounter_block', how='left')\n",
    "survival_analysis_df_yellow['time_biz_from_vent'] = survival_analysis_df_yellow['time_biz_from_vent'].fillna(27)\n",
    "survival_analysis_df_yellow['eligible'] = (survival_analysis_df_yellow['time_biz_from_vent'] != 27).astype(int)\n",
    "survival_analysis_df_yellow.rename(columns={'time_biz_from_vent': 'time_to_first_eligibility'}, inplace=True)\n",
    "survival_analysis_df_yellow['time_to_first_eligibility'] = survival_analysis_df_yellow['time_to_first_eligibility'] + 1\n",
    "\n",
    "# Display the final datasets\n",
    "print(survival_analysis_df_team.head())\n",
    "print(survival_analysis_df_yellow.head())\n",
    "\n",
    "# Initialize the KaplanMeierFitter for Patel, TEAM and Yellow criteria\n",
    "kmf_patel = KaplanMeierFitter()\n",
    "kmf_team = KaplanMeierFitter()\n",
    "kmf_yellow = KaplanMeierFitter()\n",
    "\n",
    "# fit the data for Patel criteria\n",
    "kmf_patel.fit(durations=survival_analysis_df_patel['time_to_first_eligibility'], event_observed=survival_analysis_df_patel['eligible'], label='Patel Criteria')\n",
    "\n",
    "# Fit the data for TEAM criteria\n",
    "kmf_team.fit(durations=survival_analysis_df_team['time_to_first_eligibility'], event_observed=survival_analysis_df_team['eligible'], label='TEAM Criteria')\n",
    "\n",
    "# Fit the data for Yellow criteria\n",
    "kmf_yellow.fit(durations=survival_analysis_df_yellow['time_to_first_eligibility'], event_observed=survival_analysis_df_yellow['eligible'], label='Yellow Criteria')\n",
    "\n",
    "# Plot the cumulative incidence function for all criteria\n",
    "ax = kmf_patel.plot_cumulative_density()\n",
    "kmf_team.plot_cumulative_density(ax=ax)\n",
    "kmf_yellow.plot_cumulative_density(ax=ax)\n",
    "\n",
    "plt.title('Cumulative Incidence Function for Time to First Eligibility')\n",
    "plt.xlabel('Time to First Eligibility (business hours)')\n",
    "plt.ylabel('Cumulative Incidence Probability')\n",
    "# Save the plot\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "plt.savefig(f'../output/final/graphs/cif_b_hours_{pyCLIF.helper[\"site_name\"]}_{timestamp}.png')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Estimate the median time to first eligibility for all criteria\n",
    "median_time_to_first_eligibility_patel = kmf_patel.median_survival_time_\n",
    "median_time_to_first_eligibility_team = kmf_team.median_survival_time_\n",
    "median_time_to_first_eligibility_yellow = kmf_yellow.median_survival_time_\n",
    "\n",
    "print(f\"Median time to first eligibility (Patel): {median_time_to_first_eligibility_patel} hours\")\n",
    "print(f\"Median time to first eligibility (TEAM): {median_time_to_first_eligibility_team} hours\")\n",
    "print(f\"Median time to first eligibility (Yellow): {median_time_to_first_eligibility_yellow} hours\")\n",
    "\n",
    "# Calculate the cumulative incidence function value at time = 1 for all criteria\n",
    "cif_value_at_1_patel = 1 - kmf_patel.predict(1)\n",
    "cif_value_at_1_team = 1 - kmf_team.predict(1)\n",
    "cif_value_at_1_yellow = 1 - kmf_yellow.predict(1)\n",
    "\n",
    "print(f\"Cumulative Incidence Function value at time = 1 (Patel): {cif_value_at_1_patel:.0%}\")\n",
    "print(f\"Cumulative Incidence Function value at time = 1 (TEAM): {cif_value_at_1_team:.0%}\")\n",
    "print(f\"Cumulative Incidence Function value at time = 1 (Yellow): {cif_value_at_1_yellow:.0%}\")\n",
    "\n",
    "\n",
    "# Save the cumulative incidence function data to CSV files\n",
    "cif_patel = kmf_patel.cumulative_density_.reset_index()\n",
    "cif_team = kmf_team.cumulative_density_.reset_index()\n",
    "cif_yellow = kmf_yellow.cumulative_density_.reset_index()\n",
    "\n",
    "# Add a column to identify the criteria\n",
    "cif_patel['Criteria'] = 'Patel'\n",
    "cif_team['Criteria'] = 'TEAM'\n",
    "cif_yellow['Criteria'] = 'Yellow'\n",
    "\n",
    "\n",
    "# Get the site name from pyCLIF helper\n",
    "site_name = pyCLIF.helper[\"site_name\"]\n",
    "\n",
    "cif_patel['Site'] = site_name\n",
    "cif_team['Site'] = site_name\n",
    "cif_yellow['Site'] = site_name\n",
    "\n",
    "# Save to CSV files with site name in the file names\n",
    "timestamp = datetime.now().date()\n",
    "cif_patel.to_csv(f'../output/final/cif_b_hours_patel_{pyCLIF.helper[\"site_name\"]}_{timestamp}.csv', index=False)\n",
    "cif_team.to_csv(f'../output/final/cif_b_hours_team_{pyCLIF.helper[\"site_name\"]}_{timestamp}.csv', index=False)\n",
    "cif_yellow.to_csv(f'../output/final/cif_b_hours_yellow_{pyCLIF.helper[\"site_name\"]}_{timestamp}.csv', index=False)\n",
    "\n",
    "# Calculate 95% CI for each criteria\n",
    "ci_patel_ci = kmf_patel.confidence_interval_cumulative_density_.reset_index()\n",
    "ci_team_ci = kmf_team.confidence_interval_cumulative_density_.reset_index()\n",
    "ci_yellow_ci = kmf_yellow.confidence_interval_cumulative_density_.reset_index()\n",
    "\n",
    "# Add a column to identify the criteria\n",
    "ci_patel_ci['Criteria'] = 'Patel'\n",
    "ci_team_ci['Criteria'] = 'TEAM'\n",
    "ci_yellow_ci['Criteria'] = 'Yellow'\n",
    "ci_patel_ci['Site'] = site_name\n",
    "ci_team_ci['Site'] = site_name\n",
    "ci_yellow_ci['Site'] = site_name\n",
    "# Save to CSV files with site name in the file names\n",
    "timestamp = datetime.now().date()\n",
    "ci_patel_ci.to_csv(f'../output/final/cif_b_hours_patel_ci_{pyCLIF.helper[\"site_name\"]}_{timestamp}.csv', index=False)\n",
    "ci_team_ci.to_csv(f'../output/final/cif_b_hours_team_ci_{pyCLIF.helper[\"site_name\"]}_{timestamp}.csv', index=False)\n",
    "ci_yellow_ci.to_csv(f'../output/final/cif_b_hours_yellow_ci_{pyCLIF.helper[\"site_name\"]}_{timestamp}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note: `time_from_vent_adjusted` is -1 until hour 4, then it counts up from 0. This builds in the 4-hour cool off period.\n",
    "business_hours_df = final_df[(final_df['time_from_vent_adjusted'] >= 0)]\n",
    "#business_hours_df = final_df[(final_df['time_from_vent'] >= 0) & (final_df['time_from_vent'] < 72)]\n",
    "\n",
    "# recorded_hour is the hour of the day (0-23), so business hours are 8 (8 AM) - 17 (5 PM).\n",
    "business_hours_df = business_hours_df[(business_hours_df['recorded_hour'] >= 8) & (business_hours_df['recorded_hour'] < 17)].copy()\n",
    "business_hours_df['time_biz'] = business_hours_df.groupby('encounter_block').cumcount()\n",
    "\n",
    "# Filter the DataFrame for rows where the Patel criteria are met\n",
    "patel_eligible_df = business_hours_df[business_hours_df['patel_flag'] == 1]\n",
    "\n",
    "# Find the first time each patient meets the Patel criteria\n",
    "first_eligibility_times_patel = patel_eligible_df.groupby('encounter_block')['time_biz'].min().reset_index()\n",
    "\n",
    "# Create the survival analysis dataset\n",
    "survival_analysis_df_patel = business_hours_df[['encounter_block']].drop_duplicates().copy()\n",
    "\n",
    "# Merge with the first eligibility times\n",
    "survival_analysis_df_patel = pd.merge(survival_analysis_df_patel, first_eligibility_times_patel, on='encounter_block', how='left')\n",
    "\n",
    "# Fill NaN values with 27 for patients who were never eligible\n",
    "survival_analysis_df_patel['time_biz'] = survival_analysis_df_patel['time_biz'].fillna(27)\n",
    "\n",
    "# Create the 'eligible' column\n",
    "survival_analysis_df_patel['eligible'] = (survival_analysis_df_patel['time_biz'] != 27).astype(int)\n",
    "\n",
    "# Rename columns\n",
    "survival_analysis_df_patel.rename(columns={'time_biz': 'time_to_first_eligibility'}, inplace=True)\n",
    "\n",
    "# Add +1 to time_to_first_eligibility\n",
    "survival_analysis_df_patel['time_to_first_eligibility'] = survival_analysis_df_patel['time_to_first_eligibility'] + 1\n",
    "\n",
    "# Display the final dataset\n",
    "print(survival_analysis_df_patel.head())\n",
    "\n",
    "# Repeat the process for TEAM criteria\n",
    "team_eligible_df = business_hours_df[business_hours_df['team_flag'] == 1]\n",
    "first_eligibility_times_team = team_eligible_df.groupby('encounter_block')['time_biz'].min().reset_index()\n",
    "survival_analysis_df_team = business_hours_df[['encounter_block']].drop_duplicates().copy()\n",
    "survival_analysis_df_team = pd.merge(survival_analysis_df_team, first_eligibility_times_team, on='encounter_block', how='left')\n",
    "survival_analysis_df_team['time_biz'] = survival_analysis_df_team['time_biz'].fillna(27)\n",
    "survival_analysis_df_team['eligible'] = (survival_analysis_df_team['time_biz'] != 27).astype(int)\n",
    "survival_analysis_df_team.rename(columns={'time_biz': 'time_to_first_eligibility'}, inplace=True)\n",
    "survival_analysis_df_team['time_to_first_eligibility'] = survival_analysis_df_team['time_to_first_eligibility'] + 1\n",
    "\n",
    "# Repeat the process for Yellow criteria\n",
    "yellow_eligible_df = business_hours_df[business_hours_df['any_yellow_or_green_no_red'] == 1]\n",
    "first_eligibility_times_yellow = yellow_eligible_df.groupby('encounter_block')['time_biz'].min().reset_index()\n",
    "survival_analysis_df_yellow = business_hours_df[['encounter_block']].drop_duplicates().copy()\n",
    "survival_analysis_df_yellow = pd.merge(survival_analysis_df_yellow, first_eligibility_times_yellow, on='encounter_block', how='left')\n",
    "survival_analysis_df_yellow['time_biz'] = survival_analysis_df_yellow['time_biz'].fillna(27)\n",
    "survival_analysis_df_yellow['eligible'] = (survival_analysis_df_yellow['time_biz'] != 27).astype(int)\n",
    "survival_analysis_df_yellow.rename(columns={'time_biz': 'time_to_first_eligibility'}, inplace=True)\n",
    "survival_analysis_df_yellow['time_to_first_eligibility'] = survival_analysis_df_yellow['time_to_first_eligibility'] + 1\n",
    "\n",
    "# Display the final datasets\n",
    "print(survival_analysis_df_team.head())\n",
    "print(survival_analysis_df_yellow.head())\n",
    "\n",
    "# Initialize the KaplanMeierFitter for Patel, TEAM and Yellow criteria\n",
    "kmf_patel = KaplanMeierFitter()\n",
    "kmf_team = KaplanMeierFitter()\n",
    "kmf_yellow = KaplanMeierFitter()\n",
    "\n",
    "# fit the data for Patel criteria\n",
    "kmf_patel.fit(durations=survival_analysis_df_patel['time_to_first_eligibility'], event_observed=survival_analysis_df_patel['eligible'], label='Patel Criteria')\n",
    "\n",
    "# Fit the data for TEAM criteria\n",
    "kmf_team.fit(durations=survival_analysis_df_team['time_to_first_eligibility'], event_observed=survival_analysis_df_team['eligible'], label='TEAM Criteria')\n",
    "\n",
    "# Fit the data for Yellow criteria\n",
    "kmf_yellow.fit(durations=survival_analysis_df_yellow['time_to_first_eligibility'], event_observed=survival_analysis_df_yellow['eligible'], label='Yellow Criteria')\n",
    "\n",
    "# Plot the cumulative incidence function for all criteria\n",
    "ax = kmf_patel.plot_cumulative_density()\n",
    "kmf_team.plot_cumulative_density(ax=ax)\n",
    "kmf_yellow.plot_cumulative_density(ax=ax)\n",
    "\n",
    "plt.title('Cumulative Incidence Function for Time to First Eligibility')\n",
    "plt.xlabel('Time to First Eligibility (business hours)')\n",
    "plt.ylabel('Cumulative Incidence Probability')\n",
    "# Save the plot\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "plt.savefig(f'../output/final/graphs/cif_b_hours_{pyCLIF.helper[\"site_name\"]}_{timestamp}.png')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Estimate the median time to first eligibility for all criteria\n",
    "median_time_to_first_eligibility_patel = kmf_patel.median_survival_time_\n",
    "median_time_to_first_eligibility_team = kmf_team.median_survival_time_\n",
    "median_time_to_first_eligibility_yellow = kmf_yellow.median_survival_time_\n",
    "\n",
    "print(f\"Median time to first eligibility (Patel): {median_time_to_first_eligibility_patel} hours\")\n",
    "print(f\"Median time to first eligibility (TEAM): {median_time_to_first_eligibility_team} hours\")\n",
    "print(f\"Median time to first eligibility (Yellow): {median_time_to_first_eligibility_yellow} hours\")\n",
    "\n",
    "# Calculate the cumulative incidence function value at time = 1 for all criteria\n",
    "cif_value_at_1_patel = 1 - kmf_patel.predict(1)\n",
    "cif_value_at_1_team = 1 - kmf_team.predict(1)\n",
    "cif_value_at_1_yellow = 1 - kmf_yellow.predict(1)\n",
    "\n",
    "print(f\"Cumulative Incidence Function value at time = 1 (Patel): {cif_value_at_1_patel:.0%}\")\n",
    "print(f\"Cumulative Incidence Function value at time = 1 (TEAM): {cif_value_at_1_team:.0%}\")\n",
    "print(f\"Cumulative Incidence Function value at time = 1 (Yellow): {cif_value_at_1_yellow:.0%}\")\n",
    "\n",
    "\n",
    "# Save the cumulative incidence function data to CSV files\n",
    "cif_patel = kmf_patel.cumulative_density_.reset_index()\n",
    "cif_team = kmf_team.cumulative_density_.reset_index()\n",
    "cif_yellow = kmf_yellow.cumulative_density_.reset_index()\n",
    "\n",
    "# Add a column to identify the criteria\n",
    "cif_patel['Criteria'] = 'Patel'\n",
    "cif_team['Criteria'] = 'TEAM'\n",
    "cif_yellow['Criteria'] = 'Yellow'\n",
    "\n",
    "\n",
    "# Get the site name from pyCLIF helper\n",
    "site_name = pyCLIF.helper[\"site_name\"]\n",
    "\n",
    "cif_patel['Site'] = site_name\n",
    "cif_team['Site'] = site_name\n",
    "cif_yellow['Site'] = site_name\n",
    "\n",
    "# Save to CSV files with site name in the file names\n",
    "timestamp = datetime.now().date()\n",
    "cif_patel.to_csv(f'../output/final/cif_b_hours_patel_{pyCLIF.helper[\"site_name\"]}_{timestamp}.csv', index=False)\n",
    "cif_team.to_csv(f'../output/final/cif_b_hours_team_{pyCLIF.helper[\"site_name\"]}_{timestamp}.csv', index=False)\n",
    "cif_yellow.to_csv(f'../output/final/cif_b_hours_yellow_{pyCLIF.helper[\"site_name\"]}_{timestamp}.csv', index=False)\n",
    "\n",
    "# Calculate 95% CI for each criteria\n",
    "ci_patel_ci = kmf_patel.confidence_interval_cumulative_density_.reset_index()\n",
    "ci_team_ci = kmf_team.confidence_interval_cumulative_density_.reset_index()\n",
    "ci_yellow_ci = kmf_yellow.confidence_interval_cumulative_density_.reset_index()\n",
    "\n",
    "# Add a column to identify the criteria\n",
    "ci_patel_ci['Criteria'] = 'Patel'\n",
    "ci_team_ci['Criteria'] = 'TEAM'\n",
    "ci_yellow_ci['Criteria'] = 'Yellow'\n",
    "ci_patel_ci['Site'] = site_name\n",
    "ci_team_ci['Site'] = site_name\n",
    "ci_yellow_ci['Site'] = site_name\n",
    "# Save to CSV files with site name in the file names\n",
    "timestamp = datetime.now().date()\n",
    "ci_patel_ci.to_csv(f'../output/final/cif_b_hours_patel_ci_{pyCLIF.helper[\"site_name\"]}_{timestamp}.csv', index=False)\n",
    "ci_team_ci.to_csv(f'../output/final/cif_b_hours_team_ci_{pyCLIF.helper[\"site_name\"]}_{timestamp}.csv', index=False)\n",
    "ci_yellow_ci.to_csv(f'../output/final/cif_b_hours_yellow_ci_{pyCLIF.helper[\"site_name\"]}_{timestamp}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Hours Criteria Met on Days 1, 2, and 3\n",
    "\n",
    "Determine how many hours the criteria are met on specific calendar days (Day 1, Day 2, Day 3 after intubation).\n",
    "\n",
    "1. First, assign a calendar_day column that represents the calendar day relative to intubation.\n",
    "2. Use the recorded_date and recorded hour to calculate the difference from the intubation time, and categorize rows into Day 1, Day 2, Day 3.\n",
    "3. For each encounter, group the data by calendar_day and hospitalization_id and sum the hours that meet each criterion.\n",
    "4. Compute the average number of hours for each criterion per day.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge final_df with vent_start_end to get 'vent_start_time'\n",
    "visualization_df = pd.merge(\n",
    "    final_df,\n",
    "    all_ids_w_outcome[['encounter_block', 'block_vent_start_dttm']],\n",
    "    on='encounter_block',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Ensure 'vent_start_time' and 'recorded_date' are in datetime format\n",
    "visualization_df['block_vent_start_dttm'] = pd.to_datetime(visualization_df['block_vent_start_dttm'])\n",
    "visualization_df['recorded_date'] = pd.to_datetime(visualization_df['recorded_date'])\n",
    "\n",
    "# Combine 'recorded_date' and 'recorded_hour' to create 'recorded_dttm'\n",
    "visualization_df['recorded_dttm'] = visualization_df['recorded_date'] + pd.to_timedelta(visualization_df['recorded_hour'], unit='h')\n",
    "\n",
    "# Verify the data types\n",
    "# print(\"Verify data types\\n\", visualization_df[['vent_start_time', 'recorded_dttm']].dtypes)\n",
    "\n",
    "# Remove timezone information from 'vent_start_time' if it's timezone-aware\n",
    "if visualization_df['block_vent_start_dttm'].dt.tz is not None:\n",
    "    visualization_df['block_vent_start_dttm'] = visualization_df['block_vent_start_dttm'].dt.tz_localize(None)\n",
    "\n",
    "# Similarly, remove timezone information from 'recorded_dttm' if needed\n",
    "if visualization_df['recorded_dttm'].dt.tz is not None:\n",
    "    visualization_df['recorded_dttm'] = visualization_df['recorded_dttm'].dt.tz_localize(None)\n",
    "\n",
    "# print(\"\\nConverted data type if not tz naive\\n\", visualization_df[['vent_start_time', 'recorded_dttm']].dtypes)\n",
    "\n",
    "def assign_calendar_day(df, intubation_col, recorded_col):\n",
    "    # Calculate the difference in days between intubation and recorded time\n",
    "    df['calendar_day'] = (df[recorded_col] - df[intubation_col]).dt.days + 1\n",
    "    return df\n",
    "\n",
    "# Assign calendar day for each encounter\n",
    "visualization_df = assign_calendar_day(visualization_df, 'block_vent_start_dttm', 'recorded_dttm')\n",
    "\n",
    "visualization_df = visualization_df[['encounter_block', 'block_vent_start_dttm', 'recorded_dttm', \n",
    "                  'calendar_day', 'patel_flag', 'team_flag', 'any_yellow_or_green_no_red', 'all_green', 'all_green_no_red',\n",
    "                  'any_green']]\n",
    "\n",
    "def compute_avg_hours_by_day(df, criteria_columns):\n",
    "    # Ensure hospitalization_id is handled as string/object and numeric columns as numbers\n",
    "    hours_per_day = df.groupby(['encounter_block', 'calendar_day']).agg({\n",
    "        'patel_flag': 'sum',\n",
    "        'team_flag': 'sum',\n",
    "        'any_yellow_or_green_no_red': 'sum',\n",
    "        'all_green': 'sum',\n",
    "    }).reset_index()\n",
    "    # Filter for Day 1, Day 2, Day 3\n",
    "    hours_per_day = hours_per_day[hours_per_day['calendar_day'].isin([1, 2, 3])]\n",
    "    \n",
    "    # Calculate the average number of hours for each day\n",
    "    avg_hours_by_day = hours_per_day.groupby('calendar_day').agg({\n",
    "        'patel_flag': 'mean',\n",
    "        'team_flag': 'mean',\n",
    "        'any_yellow_or_green_no_red': 'mean',\n",
    "        'all_green': 'mean',\n",
    "    }).reset_index()\n",
    "    \n",
    "    return avg_hours_by_day\n",
    "\n",
    "# Define your criteria columns\n",
    "criteria_columns = ['patel_flag', 'team_flag', 'any_yellow_or_green_no_red', 'all_green']\n",
    "# Calculate the average number of hours each criterion is met on Day 1, 2, and 3\n",
    "avg_hours_by_day = compute_avg_hours_by_day(visualization_df, criteria_columns)\n",
    "avg_hours_by_day['site_name'] = pyCLIF.helper[\"site_name\"]\n",
    "pd.DataFrame(avg_hours_by_day).to_csv(f'../output/final/avg_hours_by_day_{pyCLIF.helper[\"site_name\"]}_{datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}.csv',index=False)\n",
    "avg_hours_by_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_avg_hours_by_day_bar(avg_hours_by_day, criteria_columns):\n",
    "    # Melt the DataFrame for easier plotting with seaborn\n",
    "    melted_df = avg_hours_by_day.melt(id_vars='calendar_day', value_vars=criteria_columns, var_name='Criteria', value_name='Average Hours Met')\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Create a bar plot\n",
    "    sns.barplot(x='calendar_day', y='Average Hours Met', hue='Criteria', data=melted_df, palette='viridis')\n",
    "    \n",
    "    # Add custom x-axis labels for Day 1, Day 2, Day 3\n",
    "    plt.xticks(ticks=[0, 1, 2], labels=[\"Day 1\", \"Day 2\", \"Day 3\"])\n",
    "    \n",
    "    # Add labels and title\n",
    "    plt.title('Average Hours Criteria Met per Day')\n",
    "    plt.xlabel('Calendar Day')\n",
    "    plt.ylabel('Average Hours Criteria Met')\n",
    "    \n",
    "    # Move the legend to the bottom\n",
    "    plt.legend(title='Criteria', loc='upper center', bbox_to_anchor=(0.5, -0.1), ncol=3)\n",
    "    \n",
    "    # Show plot\n",
    "    plt.tight_layout()\n",
    "    # Save the plot\n",
    "    plt.savefig(f'../output/final/graphs/avg_hours_by_day_{pyCLIF.helper[\"site_name\"]}.png')\n",
    "    plt.show()\n",
    "\n",
    "# Plot the average hours by day using a bar plot\n",
    "plot_avg_hours_by_day_bar(avg_hours_by_day, criteria_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel categories plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Useful for EDA \n",
    "# Create a DataFrame for parallel categories plot\n",
    "parallel_df = final_df[['patel_flag', 'team_flag', 'any_yellow_or_green_no_red', 'all_green']].copy()\n",
    "parallel_df['patel_flag'] = parallel_df['patel_flag'].apply(lambda x: 1 if x else 0)\n",
    "parallel_df['team_flag'] = parallel_df['team_flag'].apply(lambda x: 1 if x else 0)\n",
    "parallel_df['any_yellow_or_green_no_red'] = parallel_df['any_yellow_or_green_no_red'].apply(lambda x: 1 if x else 0)\n",
    "parallel_df['all_green'] = parallel_df['all_green'].apply(lambda x: 1 if x else 0)\n",
    "\n",
    "# Create parallel categories plot\n",
    "fig = px.parallel_categories(parallel_df, dimensions=['patel_flag', 'team_flag', 'any_yellow_or_green_no_red', 'all_green'],\n",
    "                             color=\"patel_flag\",\n",
    "                             labels={'patel_flag': 'Patel Met', 'team_flag': 'TEAM Met', 'any_yellow_or_green_no_red': 'Yellow Flag', 'all_green': 'Green Flag'},\n",
    "                             color_continuous_scale=px.colors.sequential.Inferno)\n",
    "\n",
    "fig.update_layout(title=\"Parallel Categories Plot: Comparison of Criteria Satisfaction\")\n",
    "fig.show()\n",
    "\n",
    "# Save the final figure\n",
    "fig.write_image(f'../output/final/graphs/parallel_categories_{pyCLIF.helper[\"site_name\"]}.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at encounters when Patel flag is not met but team flag is met\n",
    "## sanity check\n",
    "patel_fail_team_pass = final_df[(final_df['patel_flag'] == 0) & (final_df['team_flag'] == 1)]\n",
    "# Verify the filter\n",
    "print(f\"\\nTotal number of hours where Patel failed and Team passed: {len(patel_fail_team_pass)}\\n\")\n",
    "\n",
    "if len(patel_fail_team_pass) > 0:\n",
    "    # Dictionary to store our failure counts\n",
    "    print(\"Primary cause of Patel Criteria non-compliance\")\n",
    "    failure_counts = {\n",
    "            'MAP': sum(patel_fail_team_pass['patel_map_flag'] == 0),\n",
    "            'SBP': sum(patel_fail_team_pass['patel_sbp_flag'] == 0),\n",
    "            'Pulse': sum(patel_fail_team_pass['patel_pulse_flag'] == 0),\n",
    "            'Respiratory Rate': sum(patel_fail_team_pass['patel_resp_rate_flag'] == 0),\n",
    "            'SpO2': sum(patel_fail_team_pass['patel_spo2_flag'] == 0)\n",
    "        }\n",
    "    failure_df = pd.DataFrame(list(failure_counts.items()),columns = ['Criteria','Count'])\n",
    "    failure_df.to_csv(f'../output/final/patel_fail_team_pass_subcomponents_{pyCLIF.helper[\"site_name\"]}_{datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}.csv',index=False)\n",
    "    print(failure_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yellow-Green spectrum criteria distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which sub‑criteria are green / yellow\n",
    "green_cols  = [c for c in final_df.columns if c.startswith(\"green_\") and c.endswith(\"_flag\")]\n",
    "yellow_cols = [c for c in final_df.columns if c.startswith(\"yellow_\") and c.endswith(\"_flag\")]\n",
    "\n",
    "# one row per block at the moment it first became eligible\n",
    "first_hit = (\n",
    "    final_df.loc[final_df[\"any_yellow_or_green_no_red\"] == 1]\n",
    "             .sort_values([\"encounter_block\", \"time_from_vent\"])\n",
    "             .groupby(\"encounter_block\")\n",
    "             .first()\n",
    ")\n",
    "\n",
    "# count how many green / yellow sub‑criteria were satisfied at that hour\n",
    "first_hit[\"n_green\"]  = first_hit[green_cols].sum(axis=1)\n",
    "first_hit[\"n_yellow\"] = first_hit[yellow_cols].sum(axis=1)\n",
    "\n",
    "# yellow‑fraction: 0 = all satisfied criteria were green, 1 = all yellow\n",
    "first_hit[\"yellow_frac\"] = (\n",
    "    first_hit[\"n_yellow\"] /\n",
    "    (first_hit[\"n_green\"] + first_hit[\"n_yellow\"])\n",
    ").fillna(0)              # guard against division by zero\n",
    "\n",
    "# yellow‑fraction: 0 = all satisfied criteria were green, 1 = all yellow\n",
    "first_hit[\"green_frac\"] = (\n",
    "    first_hit[\"n_green\"] /\n",
    "    (first_hit[\"n_green\"] + first_hit[\"n_yellow\"])\n",
    ").fillna(0)              # guard against division by zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x = np.random.normal(0, 0.002, size=len(first_hit))   # tiny jitter for visibility\n",
    "y = first_hit[\"green_frac\"].values\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.scatter(x, y, s=12, alpha=0.6, c=y, cmap=\"RdYlGn_r\", vmin=0, vmax=1)\n",
    "plt.gca().set_xlim(-0.02, 0.02)\n",
    "plt.gca().set_xticks([])\n",
    "plt.ylabel(\"Green fraction  (1=pure green  |  0=pure yellow)\")\n",
    "plt.title(\"Eligibility colour spectrum per encounter block\")\n",
    "plt.colorbar(label=\"Green fraction\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1.  Build the jittered scatter data  (unchanged)\n",
    "# ------------------------------------------------------------\n",
    "x = np.random.normal(0, 0.002, size=len(first_hit))   # tiny horizontal jitter\n",
    "y = first_hit[\"green_frac\"].values                    # 1 = pure green, 0 = pure yellow\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2.  Custom colormap: pure‑green  →  pure‑yellow\n",
    "# ------------------------------------------------------------\n",
    "green_yellow = mcolors.LinearSegmentedColormap.from_list(\n",
    "    \"YellowGreen\", [\"#ffeb3b\", \"#2ca02c\"]   #   0 (yellow)   →   1 (green)\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3.  Plot\n",
    "# ------------------------------------------------------------\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "sc = ax.scatter(x, y,\n",
    "                s=14, alpha=0.7,\n",
    "                c=y, cmap=green_yellow, vmin=0, vmax=1)\n",
    "\n",
    "ax.set_xlim(-0.02, 0.02)\n",
    "ax.set_xticks([])\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_ylabel(\"Green fraction  (1=pure green  |  0=pure yellow)\")\n",
    "ax.set_title(\"Eligibility colour spectrum per encounter block\", pad=12)\n",
    "\n",
    "cbar = fig.colorbar(sc, ax=ax, pad=0.02, shrink=0.8)\n",
    "cbar.set_label(\"Green fraction\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4.  Caption (automatic wrap)\n",
    "# ------------------------------------------------------------\n",
    "caption = (\n",
    "    \"Each dot = first eligible hour of an encounter block. \"\n",
    "    \"Vertical position/colour show the fraction of satisfied criteria that were \"\n",
    "    \"GREEN (physiologically safer) versus YELLOW (less conservative).\" \n",
    "    \"Horizontal spread is tiny random jitter to avoid over plotting; x -axis has no meaning \"\n",
    ")\n",
    "fig.text(0.01, -0.10, caption, ha=\"left\", va=\"top\", wrap=True, fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.savefig(f'../output/final/graphs/yellow_eligibility_colour_spectrum_{pyCLIF.helper[\"site_name\"]}.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".mobilization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
