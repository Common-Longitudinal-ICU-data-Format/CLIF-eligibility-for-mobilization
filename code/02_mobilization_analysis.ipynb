{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eligibility for mobilization - Analysis\n",
    "\n",
    "Run this script after running the [cohort_identification.ipynb](cohort_identification.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install pandas numpy duckdb seaborn matplotlib tableone\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from datetime import datetime\n",
    "import pyCLIF\n",
    "\n",
    "from lifelines import KaplanMeierFitter\n",
    "import matplotlib.pyplot as plt\n",
    "from lifelines import KaplanMeierFitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.read_parquet('../output/intermediate/final_df.parquet')\n",
    "vent_start_end = pd.read_parquet('../output/intermediate/final_block_vent_start_end_dttms.parquet')\n",
    "all_ids_w_outcome = pd.read_csv('../output/intermediate/cohort_all_ids_w_outcome.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward and Backward fill the final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Define which columns are flags vs. which are “excluded” vs. which need numeric ffill\n",
    "flag_columns = [\n",
    "    'hourly_trach','hourly_on_vent','nicardipine_flag','nitroprusside_flag',\n",
    "    'clevidipine_flag','red_meds_flag','cisatracurium_flag','vecuronium_flag',\n",
    "    'rocuronium_flag','paralytics_flag'\n",
    "]\n",
    "exclude_columns = [\n",
    "    'patient_id','hospitalization_id','encounter_block',\n",
    "    'recorded_dttm','recorded_date','recorded_hour',\n",
    "    'time_from_vent','time_from_vent_adjusted'\n",
    "]\n",
    "all_cols = set(final_df.columns)\n",
    "potential_fill_cols = all_cols - set(flag_columns) - set(exclude_columns)\n",
    "# Keep only columns that are numeric\n",
    "continuous_columns = [c for c in potential_fill_cols if pd.api.types.is_numeric_dtype(final_df[c])]\n",
    "\n",
    "# 2) Fill flags with 0\n",
    "for col in flag_columns:\n",
    "    final_df[col] = final_df[col].fillna(0).astype(int)\n",
    "\n",
    "# 3) Forward/Backward fill continuous columns\n",
    "final_df[continuous_columns] = (\n",
    "    final_df.groupby('encounter_block')[continuous_columns]\n",
    "    .transform(lambda s: s.ffill().bfill())\n",
    ")\n",
    "\n",
    "# 4 Handle trach- once on trach, assume they will continue to be on trach for the rest of their stay\n",
    "final_df['hourly_trach'] = (\n",
    "    final_df.groupby('encounter_block')['hourly_trach']\n",
    "    .transform(lambda s: s.cummax())\n",
    "    .astype(int)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint- useful to compare to the original df and check filling logic\n",
    "final_df.to_parquet(f'../output/intermediate/final_df_filled_{datetime.now().date()}.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Criteria Flags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patel et al. Criteria:\n",
    "\n",
    "Cardio\n",
    "* Mean arterial blood pressure: 65-110 mm Hg\n",
    "* Systolic blood pressure: ≤ 200 mm Hg\n",
    "* Heart rate: 40-130 beats per minute\n",
    "\n",
    "Respiratory\n",
    "* Respiratory rate: 5-40 breaths per minute\n",
    "* Pulse oximetry: ≥ 88%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Patel et al. Criteria\n",
    "\n",
    "# 1. Mean arterial blood pressure: 65-110 mm Hg\n",
    "final_df['patel_map_flag'] = (\n",
    "    (final_df['min_map'] >= 65) & (final_df['max_map'] <= 110)\n",
    ").astype(int)\n",
    "\n",
    "# 2. Systolic blood pressure: ≤ 200 mm Hg\n",
    "final_df['patel_sbp_flag'] = (\n",
    "    final_df['max_sbp'] <= 200\n",
    ").astype(int)\n",
    "\n",
    "# 3. Heart rate (Pulse): 40-130 beats per minute\n",
    "final_df['patel_pulse_flag'] = (\n",
    "    (final_df['min_heart_rate'] >= 40) & (final_df['max_heart_rate'] <= 130)\n",
    ").astype(int)\n",
    "\n",
    "# 4. Respiratory rate: 5-40 breaths per minute\n",
    "final_df['patel_resp_rate_flag'] = (\n",
    "    (final_df['min_respiratory_rate'] >= 5) & (final_df['max_respiratory_rate'] <= 40)\n",
    ").astype(int)\n",
    "\n",
    "# 5. Pulse oximetry (SpO2): ≥ 88%\n",
    "final_df['patel_spo2_flag'] = (\n",
    "    final_df['min_spo2'] >= 88\n",
    ").astype(int)\n",
    "\n",
    "# Resp flag: Combines respiratory rate and SpO2 criteria\n",
    "final_df['patel_resp_flag'] = (\n",
    "    final_df['patel_resp_rate_flag'] &\n",
    "    final_df['patel_spo2_flag'] &\n",
    "    (final_df['hourly_trach'] == 0) &\n",
    "    (final_df['paralytics_flag'] == 0)\n",
    ").astype(int)\n",
    "\n",
    "# Cardio flag: Combines MAP, SBP, and Pulse criteria\n",
    "final_df['patel_cardio_flag'] = (\n",
    "    final_df['patel_map_flag'] &\n",
    "    final_df['patel_sbp_flag'] &\n",
    "    final_df['patel_pulse_flag'] &\n",
    "    (final_df['hourly_trach'] == 0) &\n",
    "    (final_df['paralytics_flag'] == 0)\n",
    ").astype(int)\n",
    "\n",
    "# Step 2: Create the overall Patel flag\n",
    "final_df['patel_flag'] = (\n",
    "    final_df['patel_map_flag'] &\n",
    "    final_df['patel_sbp_flag'] &\n",
    "    final_df['patel_pulse_flag'] &\n",
    "    final_df['patel_resp_rate_flag'] &\n",
    "    final_df['patel_spo2_flag'] &\n",
    "    (final_df['hourly_trach'] == 0) &\n",
    "    (final_df['paralytics_flag'] == 0) &\n",
    "    (final_df['recorded_hour'] >= 8) &\n",
    "    (final_df['recorded_hour'] < 17)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEAM criteria\n",
    "\n",
    "Cardio\n",
    "* Heart rate: ≤ 150 bpm\n",
    "* Most recent lactate: ≤ 4.0 mmol/L\n",
    "* Noradrenaline infusion rate: <0.2 mcg/kg/min or if infusion rate has increased by more than 25% in the last 6 hours, dose must be <0.1 mcg/kg/min.\n",
    "Respiratory\n",
    "* Sufficient respiratory stability:\n",
    "    *  FiO2: ≤ 0.6\n",
    "    *  PEEP: ≤ 16 cm H2O (use peep_observed)\n",
    "* Current respiratory rate: ≤ 45 (use resp_rate_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Heart rate: ≤ 150 bpm\n",
    "final_df['team_pulse_flag'] = (\n",
    "    final_df['max_heart_rate'] <= 150\n",
    ").astype(int)\n",
    "\n",
    "# 2. Most recent lactate: ≤ 4.0 mmol/L\n",
    "final_df['team_lactate_flag'] = (\n",
    "    final_df['lactate'] <= 4.0\n",
    ").astype(int)\n",
    "\n",
    "# 3. Noradrenaline infusion rate: <0.2 mcg/kg/min \n",
    "final_df['team_ne_flag'] = (\n",
    "    # (final_df['ne_calc_min'] >= 0.1) & (final_df['ne_calc_max'] <= 0.2)\n",
    "    final_df['ne_calc_max'] <= 0.2\n",
    ").astype(int)\n",
    "\n",
    "# print the number of team_ne_flag == 1\n",
    "print(\"TEAM NE flag counts when ne < 0.2\\n\", final_df['team_ne_flag'].value_counts(), \"\\n\")\n",
    " \n",
    "#3b. set the flag to 0 if infusion rate has increased by more than 25% in the last 6 hours and the dose is >0.1 mcg/kg/min.\n",
    "final_df['team_ne_flag'] = np.where(\n",
    "    (final_df['ne_calc_max'] > 1.25 * final_df['min_ne_dose_last_6_hours']) & (final_df['ne_calc_max'] > 0.1),\n",
    "    0,\n",
    "    final_df['team_ne_flag']\n",
    ")\n",
    "print(\"TEAM NE flag counts adjusting for change in the last 6 hrs\\n\", final_df['team_ne_flag'].value_counts(), \"\\n\")\n",
    "\n",
    "# 4. Sufficient respiratory stability:\n",
    "#    a. FiO2: ≤ 0.6\n",
    "final_df['team_fio2_flag'] = (\n",
    "    final_df['min_fio2_set'] <= 0.6\n",
    ").astype(int)\n",
    "\n",
    "#    b. PEEP: ≤ 16 cm H2O\n",
    "final_df['team_peep_flag'] = (\n",
    "    final_df['max_peep_set'] <= 16\n",
    ").astype(int)\n",
    "\n",
    "# 5. Current respiratory rate: ≤ 45\n",
    "final_df['team_resp_rate_flag'] = (\n",
    "    final_df['max_resp_rate_obs'] <= 45 \n",
    ").astype(int)\n",
    "\n",
    "# Cardio flag: Combines heart rate, lactate, and norepinephrine criteria\n",
    "final_df['team_cardio_flag'] = (\n",
    "    final_df['team_pulse_flag'] &\n",
    "    final_df['team_lactate_flag'] &\n",
    "    final_df['team_ne_flag'] &\n",
    "    (final_df['hourly_trach'] == 0) &\n",
    "    (final_df['paralytics_flag'] == 0)\n",
    ").astype(int)\n",
    "\n",
    "# Resp flag: Combines FiO2, PEEP, and respiratory rate criteria\n",
    "final_df['team_resp_flag'] = (\n",
    "    final_df['team_fio2_flag'] &\n",
    "    final_df['team_peep_flag'] &\n",
    "    final_df['team_resp_rate_flag'] &\n",
    "    (final_df['hourly_trach'] == 0) &\n",
    "    (final_df['paralytics_flag'] == 0)\n",
    ").astype(int)\n",
    "\n",
    "\n",
    "# Create the overall TEAM flag\n",
    "final_df['team_flag'] = (\n",
    "    final_df['team_pulse_flag'] &\n",
    "    final_df['team_lactate_flag'] &\n",
    "    final_df['team_ne_flag'] &\n",
    "    final_df['team_fio2_flag'] &\n",
    "    final_df['team_peep_flag'] &\n",
    "    final_df['team_resp_rate_flag'] &\n",
    "    (final_df['hourly_trach'] == 0) &\n",
    "    (final_df['paralytics_flag'] == 0) & \n",
    "    (final_df['recorded_hour'] >= 8) &\n",
    "    (final_df['recorded_hour'] < 17)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consensus criteria\n",
    "\n",
    "* Green Criteria\n",
    "    * Respiratory\n",
    "        * Saturation  90% and\n",
    "        * Respiratory rate ≤ 30 breaths/min\n",
    "        * Current FiO2 ≤ 0.6 and\n",
    "        * PEEP≤ 10cm H20\n",
    "    * Cardiovascular:\n",
    "        * Blood pressure greater than lower limit of target range (MAP 65+) while on no or low level of support (low support- define as <0.1 μg/kg/min of Norepi equivalents)\n",
    "        * Heart rate <120 beats/min\n",
    "        * lactate < 4mmol/L\n",
    "        * HR > 40\n",
    "* Yellow Criteria\n",
    "    * Respiratory\n",
    "        * Sat >= 90%\n",
    "        * Current FiO2 >0.6\n",
    "        * Respiratory rate >30breaths/min\n",
    "        * PEEP >10cm H20\n",
    "    * Cardiovascular\n",
    "        * Blood pressure greater than lower limit of target range (MAP 65+) while receiving moderate level of support (medium-define as 0.1–0.3 μg/kg/min of Norepi equivalents)\n",
    "        * Heart rate 120-150 beats/min\n",
    "        * Shock of any cause with lactate >4mmol/L\n",
    "        * HR > 40\n",
    "* Red Criteria\n",
    "    * Respiratory\n",
    "        * Sat <90%\n",
    "    * Cardiovascular\n",
    "        * Below target MAP despite support (MAP <65) or\n",
    "        * greater than lower limit MAP (MAP 65+) but on high level support (high defined as >0.3 μg/kg/min of Norepi equivalents)\n",
    "        * IV therapy for hypertensive emergency (SBP >200mmHg or MAP >110 and on nicardipine, nitroprusside, or clevidipine gtt)\n",
    "        * HR >150 bpm\n",
    "        * Bradycardia <40\n",
    "\n",
    "\n",
    "### Consensus criteria - redefined \n",
    "\n",
    "* all_red: All red subcomponents must be met.\n",
    "* all_green: All green subcomponents must be met, and no red subcomponents are met.\n",
    "* all_yellow: All yellow subcomponents must be met, no red subcomponents are met, and all green subcomponents are not met.\n",
    "* any_yellow: Any yellow subcomponent is met, no green subcomponents are fully met, and no red subcomponents are met.\n",
    "* any_yellow_or_green_no_red: Any yellow or green subcomponents are met, but no red subcomponents are met."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Red Cardiovascular Criteria\n",
    "final_df['red_resp_spo2_flag'] = (final_df['min_spo2'] < 90).astype(int)\n",
    "final_df['red_map_flag'] = (final_df['min_map'] < 65).astype(int)\n",
    "# High support (Norepinephrine equivalents > 0.3 μg/kg/min)\n",
    "final_df['red_high_support_flag'] = (final_df['ne_calc_max'] > 0.3).astype(int)\n",
    "# Hypertensive emergency criteria (SBP > 200 mmHg or MAP > 110 mmHg and on certain medications)\n",
    "final_df['red_hypertensive_flag'] = (\n",
    "    ((final_df['max_sbp'] > 200) | (final_df['max_map'] > 110)) &\n",
    "    (final_df['red_meds_flag'] == 1)\n",
    ").astype(int)\n",
    "# High heart rate criteria (HR > 150 bpm)\n",
    "final_df['red_pulse_high_flag'] = (final_df['max_heart_rate'] > 150).astype(int)\n",
    "# Low heart rate criteria (HR < 40 bpm)\n",
    "final_df['red_pulse_low_flag'] = (final_df['min_heart_rate'] < 40).astype(int)\n",
    "\n",
    "# Yellow Respiratory Criteria\n",
    "final_df['yellow_resp_spo2_flag'] = (final_df['min_spo2'] >= 90).astype(int)\n",
    "final_df['yellow_fio2_flag'] = (final_df['min_fio2_set'] > 0.6).astype(int)\n",
    "final_df['yellow_resp_rate_flag'] = (final_df['max_resp_rate_obs'] > 30).astype(int)\n",
    "final_df['yellow_peep_flag'] = (final_df['min_peep_set'] > 10).astype(int)\n",
    "\n",
    "# Yellow Cardiovascular Criteria\n",
    "final_df['yellow_map_flag'] = ((final_df['min_map'] >= 65) & (final_df['ne_calc_max'].between(0.1, 0.3))).astype(int)\n",
    "final_df['yellow_pulse_flag'] = (final_df['min_heart_rate'].between(120, 150)).astype(int)\n",
    "final_df['yellow_lactate_flag'] = (final_df['lactate'] > 4).astype(int)\n",
    "\n",
    "# Step 3: Implement Green Criteria\n",
    "final_df['green_resp_spo2_flag'] = (final_df['min_spo2'] >= 90).astype(int)\n",
    "final_df['green_resp_rate_flag'] = (final_df['max_resp_rate_obs'] <= 30).astype(int)\n",
    "final_df['green_fio2_flag'] = (final_df['min_fio2_set'] <= 0.6).astype(int)\n",
    "final_df['green_peep_flag'] = (final_df['min_peep_set'] <= 10).astype(int)\n",
    "\n",
    "# Green Cardiovascular Criteria\n",
    "final_df['green_map_flag'] = ((final_df['min_map'] >= 65) & (final_df['ne_calc_max'] < 0.1)).astype(int)\n",
    "final_df['green_pulse_flag'] = (final_df['min_heart_rate'] < 120).astype(int)\n",
    "final_df['green_lactate_flag'] = (final_df['lactate'] < 4).astype(int)\n",
    "final_df['green_hr_flag'] = (final_df['min_heart_rate'] > 40).astype(int)\n",
    "\n",
    "final_df['any_red'] = (\n",
    "    (final_df['red_resp_spo2_flag'] |\n",
    "    final_df['red_map_flag'] |\n",
    "    final_df['red_high_support_flag'] |\n",
    "    final_df['red_hypertensive_flag'] |\n",
    "    final_df['red_pulse_high_flag'] |\n",
    "    final_df['red_pulse_low_flag']) &\n",
    "    (final_df['hourly_trach'] == 0) &\n",
    "    (final_df['paralytics_flag'] == 0) &\n",
    "    (final_df['recorded_hour'] >= 8) &\n",
    "    (final_df['recorded_hour'] < 17)\n",
    ").astype(int)\n",
    "\n",
    "final_df['any_yellow'] = (\n",
    "    (final_df['yellow_resp_spo2_flag'] |\n",
    "    final_df['yellow_fio2_flag'] |\n",
    "    final_df['yellow_resp_rate_flag'] |\n",
    "    final_df['yellow_peep_flag'] |\n",
    "    final_df['yellow_map_flag'] |\n",
    "    final_df['yellow_pulse_flag'] |\n",
    "    final_df['yellow_lactate_flag']) &\n",
    "    (final_df['hourly_trach'] == 0) &\n",
    "    (final_df['paralytics_flag'] == 0) &\n",
    "    (final_df['recorded_hour'] >= 8) &\n",
    "    (final_df['recorded_hour'] < 17)\n",
    ").astype(int)\n",
    "\n",
    "final_df['any_green'] = (\n",
    "    (final_df['green_resp_spo2_flag'] |\n",
    "    final_df['green_resp_rate_flag'] |\n",
    "    final_df['green_fio2_flag'] |\n",
    "    final_df['green_peep_flag'] |\n",
    "    final_df['green_map_flag'] |\n",
    "    final_df['green_pulse_flag'] |\n",
    "    final_df['green_lactate_flag'] |\n",
    "    final_df['green_hr_flag']) &\n",
    "    (final_df['hourly_trach'] == 0) &\n",
    "    (final_df['paralytics_flag'] == 0) &\n",
    "    (final_df['recorded_hour'] >= 8) &\n",
    "    (final_df['recorded_hour'] < 17)\n",
    ").astype(int)\n",
    "\n",
    "final_df['all_green'] = (\n",
    "    final_df['green_resp_spo2_flag'] &\n",
    "    final_df['green_resp_rate_flag'] &\n",
    "    final_df['green_fio2_flag'] &\n",
    "    final_df['green_peep_flag'] &\n",
    "    final_df['green_map_flag'] &\n",
    "    final_df['green_pulse_flag'] &\n",
    "    final_df['green_lactate_flag'] &\n",
    "    final_df['green_hr_flag'] &\n",
    "    (final_df['hourly_trach'] == 0) &\n",
    "    (final_df['paralytics_flag'] == 0) &\n",
    "    (final_df['recorded_hour'] >= 8) &\n",
    "    (final_df['recorded_hour'] < 17)\n",
    ").astype(int)\n",
    "\n",
    "final_df['all_green_no_red'] = (\n",
    "    final_df['green_resp_spo2_flag'] &\n",
    "    final_df['green_resp_rate_flag'] &\n",
    "    final_df['green_fio2_flag'] &\n",
    "    final_df['green_peep_flag'] &\n",
    "    final_df['green_map_flag'] &\n",
    "    final_df['green_pulse_flag'] &\n",
    "    final_df['green_lactate_flag'] &\n",
    "    final_df['green_hr_flag'] &\n",
    "    (final_df['any_red'] == 0) &\n",
    "    (final_df['hourly_trach'] == 0) &\n",
    "    (final_df['paralytics_flag'] == 0) &\n",
    "    (final_df['recorded_hour'] >= 8) &\n",
    "    (final_df['recorded_hour'] < 17)\n",
    ").astype(int)\n",
    "\n",
    "final_df['all_green_no_red_yellow'] = (\n",
    "    final_df['green_resp_spo2_flag'] &\n",
    "    final_df['green_resp_rate_flag'] &\n",
    "    final_df['green_fio2_flag'] &\n",
    "    final_df['green_peep_flag'] &\n",
    "    final_df['green_map_flag'] &\n",
    "    final_df['green_pulse_flag'] &\n",
    "    final_df['green_lactate_flag'] &\n",
    "    final_df['green_hr_flag'] &\n",
    "    (final_df['any_red'] == 0) &\n",
    "    (final_df['any_yellow'] == 0) &\n",
    "    (final_df['hourly_trach'] == 0) &\n",
    "    (final_df['paralytics_flag'] == 0) &\n",
    "    (final_df['recorded_hour'] >= 8) &\n",
    "    (final_df['recorded_hour'] < 17)\n",
    ").astype(int)\n",
    "\n",
    "final_df['all_yellow_no_red_green'] = (\n",
    "    final_df['yellow_resp_spo2_flag'] &\n",
    "    final_df['yellow_fio2_flag'] &\n",
    "    final_df['yellow_resp_rate_flag'] &\n",
    "    final_df['yellow_peep_flag'] &\n",
    "    final_df['yellow_map_flag'] &\n",
    "    final_df['yellow_pulse_flag'] &\n",
    "    final_df['yellow_lactate_flag'] &\n",
    "    (final_df['any_red'] == 0) &\n",
    "    (final_df['any_green'] == 0) &\n",
    "    (final_df['hourly_trach'] == 0) &\n",
    "    (final_df['paralytics_flag'] == 0) &\n",
    "    (final_df['recorded_hour'] >= 8) &\n",
    "    (final_df['recorded_hour'] < 17)\n",
    ").astype(int)\n",
    "\n",
    "final_df['any_yellow_no_red_green'] = (\n",
    "    (final_df['yellow_resp_spo2_flag'] |\n",
    "    final_df['yellow_fio2_flag'] |\n",
    "    final_df['yellow_resp_rate_flag'] |\n",
    "    final_df['yellow_peep_flag'] |\n",
    "    final_df['yellow_map_flag'] |\n",
    "    final_df['yellow_pulse_flag'] |\n",
    "    final_df['yellow_lactate_flag']) &\n",
    "    (final_df['any_red'] == 0) &\n",
    "    (final_df['any_green'] == 0) &\n",
    "    (final_df['hourly_trach'] == 0) &\n",
    "    (final_df['paralytics_flag'] == 0) &\n",
    "    (final_df['recorded_hour'] >= 8) &\n",
    "    (final_df['recorded_hour'] < 17)\n",
    ").astype(int)\n",
    "\n",
    "final_df['any_yellow_or_green_no_red'] = (\n",
    "    (final_df['yellow_resp_spo2_flag'] |\n",
    "    final_df['yellow_fio2_flag'] |\n",
    "    final_df['yellow_resp_rate_flag'] |\n",
    "    final_df['yellow_peep_flag'] |\n",
    "    final_df['yellow_map_flag'] |\n",
    "    final_df['yellow_pulse_flag'] |\n",
    "    final_df['yellow_lactate_flag'] |\n",
    "    final_df['green_resp_spo2_flag'] |\n",
    "    final_df['green_resp_rate_flag'] |\n",
    "    final_df['green_fio2_flag'] |\n",
    "    final_df['green_peep_flag'] |\n",
    "    final_df['green_map_flag'] |\n",
    "    final_df['green_pulse_flag'] |\n",
    "    final_df['green_lactate_flag'] |\n",
    "    final_df['green_hr_flag']) &\n",
    "    (final_df['any_red'] == 0) &\n",
    "    (final_df['hourly_trach'] == 0) &\n",
    "    (final_df['paralytics_flag'] == 0) &\n",
    "    (final_df['recorded_hour'] >= 8) &\n",
    "    (final_df['recorded_hour'] < 17)\n",
    ").astype(int)\n",
    "\n",
    "final_df['yellow_resp_flag'] = (\n",
    "    (final_df['yellow_resp_spo2_flag'] |\n",
    "    final_df['yellow_fio2_flag'] |\n",
    "    final_df['yellow_resp_rate_flag'] |\n",
    "    final_df['yellow_peep_flag'] |\n",
    "    final_df['green_resp_spo2_flag'] |\n",
    "    final_df['green_resp_rate_flag'] |\n",
    "    final_df['green_fio2_flag'] |\n",
    "    final_df['green_peep_flag']) &\n",
    "    (final_df['any_red'] == 0) &\n",
    "    (final_df['hourly_trach'] == 0) &\n",
    "    (final_df['paralytics_flag'] == 0) &\n",
    "    (final_df['recorded_hour'] >= 8) &\n",
    "    (final_df['recorded_hour'] < 17)\n",
    ").astype(int)\n",
    "\n",
    "final_df['yellow_cardio_flag'] = (\n",
    "    (final_df['yellow_map_flag'] |\n",
    "    final_df['yellow_pulse_flag'] |\n",
    "    final_df['yellow_lactate_flag'] |\n",
    "    final_df['green_map_flag'] |\n",
    "    final_df['green_pulse_flag'] |\n",
    "    final_df['green_lactate_flag'] |\n",
    "    final_df['green_hr_flag']) &\n",
    "    (final_df['any_red'] == 0) &\n",
    "    (final_df['hourly_trach'] == 0) &\n",
    "    (final_df['paralytics_flag'] == 0) &\n",
    "    (final_df['recorded_hour'] >= 8) &\n",
    "    (final_df['recorded_hour'] < 17)\n",
    ").astype(int)\n",
    "\n",
    "final_df['yellow_all_green'] = (\n",
    "    final_df['all_green_no_red'] &\n",
    "    (final_df['any_yellow'] == 0)\n",
    ").astype(int)\n",
    "\n",
    "final_df['yellow_not_all_green'] = (\n",
    "    final_df['any_yellow_or_green_no_red'] &\n",
    "    (final_df['all_green_no_red'] == 0)\n",
    ").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print value counts for each flag\n",
    "print(final_df[['any_red', 'any_yellow', 'any_green' ,  'all_green',\n",
    "                'all_green_no_red', 'all_green_no_red_yellow', 'all_yellow_no_red_green', \n",
    "                'any_yellow_no_red_green','any_yellow_or_green_no_red','yellow_all_green',\n",
    "                 'yellow_not_all_green' ]].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TableOne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table1_overall = pyCLIF.generate_table_one(final_df, filename=\"table1_overall\")\n",
    "table1_overall\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restrict to business hours in the first 72 hours after intubation*\n",
    "\n",
    "* 4-hour cool off period after first intubation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note: `time_from_vent_adjusted` is -1 until hour 4, then it counts up from 0. This builds in the 4-hour cool off period.\n",
    "business_hours_df = final_df[(final_df['time_from_vent_adjusted'] >= 0) & (final_df['time_from_vent_adjusted'] < 72)]\n",
    "#business_hours_df = final_df[(final_df['time_from_vent'] >= 0) & (final_df['time_from_vent'] < 72)]\n",
    "\n",
    "# recorded_hour is the hour of the day (0-23), so business hours are 8 (8 AM) - 17 (5 PM).\n",
    "business_hours_df = business_hours_df[(business_hours_df['recorded_hour'] >= 8) & (business_hours_df['recorded_hour'] < 17)].copy()\n",
    "business_hours_df['time_biz'] = business_hours_df.groupby('hospitalization_id').cumcount()\n",
    "\n",
    "business_hours_df.head(n = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Some encounters are dropped when there are no business hours are availble\n",
    "pyCLIF.count_unique_encounters(business_hours_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table1_business_hours = pyCLIF.generate_table_one(business_hours_df, filename=\"table1_business_hours\")\n",
    "table1_business_hours\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Competing Risk Analysis Setup\n",
    "\n",
    "Create a dataframe for each criteria with the following columns \n",
    "\n",
    "1. encounter_block: identify the patient encounter\n",
    "2. time_eligibility: earliest eligibility time from first intubation episode per encounter block\n",
    "3. time_death: time from ventilation start to death, if applicable. Missing if not dead\n",
    "4. time_discharge_alive: time from ventilation start to discharge. If not dead, assumed discharged and the last recorded vital time is discharge time.\n",
    "5. t_event: earliest of the above three times\n",
    "6. outcome: 1(eligibility), 2(death), 3(discharge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_hours_all_df = final_df[(final_df['time_from_vent_adjusted'] >= 0)].copy()\n",
    "business_hours_all_df= business_hours_all_df[['patient_id', 'hospitalization_id', 'encounter_block', \n",
    "                                              'recorded_dttm', 'recorded_date', 'recorded_hour', 'time_from_vent',\n",
    "                                              'time_from_vent_adjusted', 'patel_flag', 'team_flag', 'any_yellow_or_green_no_red']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_competing_risk_dataset(\n",
    "    criteria_df,\n",
    "    all_ids_w_outcome,\n",
    "    flag_col='patel_flag',\n",
    "    cool_off_hrs=4\n",
    "):\n",
    "    \"\"\"\n",
    "    Build a competing-risk dataset for the time to first eligibility (flag==1), \n",
    "    versus death, versus discharge alive.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    criteria_df : pd.DataFrame\n",
    "        Row-level DataFrame with columns:\n",
    "          - encounter_block (unique block ID)\n",
    "          - recorded_dttm (datetime)\n",
    "          - block_vent_start_dttm (optional) or we merge later\n",
    "          - time_from_vent_adjusted : The number of hours since intubation \n",
    "            (with a 4-hour cool-off). We'll rely on this if present,\n",
    "            or compute from block_vent_start_dttm if needed.\n",
    "          - <flag_col> (e.g. 'patel_flag', 'team_flag', 'any_yellow_or_green_no_red'): 0/1 per row\n",
    "    all_ids_w_outcome : pd.DataFrame\n",
    "        Block-level DataFrame with columns:\n",
    "          - encounter_block\n",
    "          - block_vent_start_dttm\n",
    "          - final_outcome_dttm  (the final time = death, discharge, or last vital)\n",
    "          - death_dttm, discharge_dttm (optional)\n",
    "          - is_dead (0 or 1)\n",
    "    flag_col : str\n",
    "        The column name in criteria_df that indicates \"eligibility\" (1=eligible).\n",
    "    cool_off_hrs : int\n",
    "        The cool-off period used (e.g. 4). 'time_from_vent_adjusted' \n",
    "        has already subtracted it, this should remain consistent.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A block-level DataFrame with columns:\n",
    "          [ encounter_block, time_eligibility, time_death, time_discharge_alive,\n",
    "            t_event, outcome ]\n",
    "        where outcome = 1(eligibility), 2(death), 3(discharge),\n",
    "        and t_event is the earliest of those times.\n",
    "    \"\"\"\n",
    "\n",
    "    ###################################################################\n",
    "    # 1) Merge the row-level data with block-outcome data on encounter_block\n",
    "    ###################################################################\n",
    "    needed_cols = ['encounter_block', 'time_from_vent_adjusted', 'recorded_dttm', flag_col]\n",
    "    \n",
    "    for col in needed_cols:\n",
    "        if col not in criteria_df.columns:\n",
    "            raise ValueError(f\"Missing required column '{col}' in criteria_df\")\n",
    "\n",
    "    ###################################################################\n",
    "    # 2) Find the earliest eligibility time per block\n",
    "    ###################################################################\n",
    "    def earliest_eligibility(group):\n",
    "        eligible_rows = group[group[flag_col] == 1]\n",
    "        return eligible_rows['time_from_vent_adjusted'].min() if not eligible_rows.empty else np.nan\n",
    "\n",
    "    first_elig = criteria_df.groupby('encounter_block').apply(earliest_eligibility).reset_index()\n",
    "    first_elig.columns = ['encounter_block', 'time_eligibility']\n",
    "\n",
    "    ###################################################################\n",
    "    # 3) Compute time from ventilation start for death & discharge\n",
    "    ###################################################################\n",
    "    def calc_time_from_vent_adjusted(row, event_dttm):\n",
    "        \"\"\"\n",
    "        Calculate time (in hours) from ventilation start to a given event.\n",
    "        \"\"\"\n",
    "        # Convert event_dttm and block_vent_start_dttm to datetime if not already\n",
    "        if isinstance(event_dttm, str):\n",
    "            event_dttm = pd.to_datetime(event_dttm, errors='coerce')\n",
    "\n",
    "        if isinstance(row['block_vent_start_dttm'], str):\n",
    "            row['block_vent_start_dttm'] = pd.to_datetime(row['block_vent_start_dttm'], errors='coerce')\n",
    "\n",
    "        # If event_dttm is NaT (null), return NaN\n",
    "        if pd.isna(event_dttm) or pd.isna(row['block_vent_start_dttm']):\n",
    "            return np.nan\n",
    "\n",
    "        # Compute time difference in hours\n",
    "        delta_hrs = (event_dttm - row['block_vent_start_dttm']).total_seconds() / 3600\n",
    "\n",
    "        # Subtract the cool-off period\n",
    "        return delta_hrs - cool_off_hrs\n",
    "\n",
    "    block_level = all_ids_w_outcome[\n",
    "        all_ids_w_outcome['encounter_block'].isin(criteria_df['encounter_block'])\n",
    "    ][['encounter_block', 'block_vent_start_dttm', 'death_dttm', 'discharge_dttm', 'final_outcome_dttm', 'is_dead']].copy()\n",
    "    block_level['block_vent_start_dttm'] = pd.to_datetime(block_level['block_vent_start_dttm'], errors='coerce')\n",
    "    # block_level['time_death'] = block_level.apply(lambda row: calc_time_from_vent_adjusted(row, row['death_dttm']), axis=1)\n",
    "    # block_level['time_discharge_alive'] = block_level.apply(lambda row: calc_time_from_vent_adjusted(row, row['discharge_dttm']), axis=1)\n",
    "    block_level['time_death'] = block_level.apply(lambda row: calc_time_from_vent_adjusted(row, row['final_outcome_dttm']) if row['is_dead'] == 1 else np.nan, axis=1)\n",
    "    block_level['time_discharge_alive'] = block_level.apply(lambda row: calc_time_from_vent_adjusted(row, row['final_outcome_dttm']) if row['is_dead'] == 0 else np.nan, axis=1)\n",
    "\n",
    "    ###################################################################\n",
    "    # 4) Merge eligibility times with block-level events\n",
    "    ###################################################################\n",
    "    final_df = pd.merge(\n",
    "        block_level[['encounter_block', 'time_death', 'time_discharge_alive', 'is_dead']],\n",
    "        first_elig,\n",
    "        on='encounter_block',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    ###################################################################\n",
    "    # 5) Determine the earliest event time and its outcome category\n",
    "    ###################################################################\n",
    "    final_df[['time_eligibility', 'time_death', 'time_discharge_alive']] = final_df[\n",
    "        ['time_eligibility', 'time_death', 'time_discharge_alive']\n",
    "    ]\n",
    "    # ].fillna(1e9)  # Assign a large value to missing times so they won't be the minimum\n",
    "\n",
    "    def which_event(row):\n",
    "        \"\"\"Determine the earliest event and assign outcome codes (1=eligibility, 2=death, 3=discharge).\"\"\"\n",
    "        times = {1: row['time_eligibility'], 2: row['time_death'], 3: row['time_discharge_alive']}\n",
    "        min_event = min(times, key=times.get)\n",
    "        return min_event\n",
    "\n",
    "    final_df['t_event'] = final_df[['time_eligibility', 'time_death', 'time_discharge_alive']].min(axis=1)\n",
    "    final_df['outcome'] = final_df.apply(which_event, axis=1)\n",
    "\n",
    "    ###################################################################\n",
    "    # 6) Exclude censored cases and handle negative event times\n",
    "    ###################################################################\n",
    "    # final_df = final_df[final_df['outcome'] != 0].copy()\n",
    "\n",
    "    # Handle cases where t_event < 0 (event within the cool-off period)\n",
    "    # If needed, these rows can be removed or set to zero.\n",
    "    # Uncomment to remove:\n",
    "    # final_df = final_df[final_df['t_event'] >= 0]\n",
    "\n",
    "    # Return only relevant columns\n",
    "    final_cols = ['encounter_block', 'time_eligibility', 'time_death', 'time_discharge_alive', 't_event', 'outcome']\n",
    "    return final_df[final_cols].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.merge(\n",
    "        business_hours_all_df,\n",
    "        all_ids_w_outcome[['encounter_block',\n",
    "       'block_vent_start_dttm', 'block_vent_end_dttm',\n",
    "       'block_first_vital_dttm', 'block_last_vital_dttm', 'discharge_dttm',\n",
    "       'discharge_category', 'death_dttm', 'final_outcome_dttm', 'is_dead']],\n",
    "        on=  'encounter_block',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "df_merged_patel = df_merged[df_merged['patel_flag'] == 1]\n",
    "df_merged_team = df_merged[df_merged['team_flag'] == 1]\n",
    "df_merged_yellow= df_merged[df_merged['any_yellow_or_green_no_red'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patel_competing = create_competing_risk_dataset(\n",
    "    criteria_df=df_merged_patel,\n",
    "    all_ids_w_outcome=all_ids_w_outcome,\n",
    "    flag_col='patel_flag',\n",
    "    cool_off_hrs=4\n",
    ")\n",
    "df_patel_competing.to_parquet('../output/intermediate/competing_risk_patel.parquet')\n",
    "\n",
    "df_team_competing = create_competing_risk_dataset(\n",
    "    criteria_df=df_merged_team,\n",
    "    all_ids_w_outcome=all_ids_w_outcome,\n",
    "    flag_col='team_flag',\n",
    "    cool_off_hrs=4\n",
    ")\n",
    "df_team_competing.to_parquet('../output/intermediate/competing_risk_team.parquet')\n",
    "\n",
    "df_yellow_competing = create_competing_risk_dataset(\n",
    "    criteria_df=df_merged_yellow,\n",
    "    all_ids_w_outcome=all_ids_w_outcome,\n",
    "    flag_col='any_yellow_or_green_no_red',\n",
    "    cool_off_hrs=4\n",
    ")\n",
    "df_yellow_competing.to_parquet('../output/intermediate/competing_risk_yellow.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lifelines import AalenJohansenFitter\n",
    "\n",
    "def fit_aalen_johansen_cif(\n",
    "    df,\n",
    "    duration_col='t_event',\n",
    "    event_col='outcome',\n",
    "    cause_of_interest=1,\n",
    "    calculate_variance=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Fits an Aalen–Johansen estimator to compute the cumulative incidence\n",
    "    of a given cause (cause_of_interest) in a multi-state/competing-risks scenario.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        A DataFrame with at least [duration_col, event_col].\n",
    "        event_col should be integer-coded:\n",
    "          1 => cause #1\n",
    "          2 => cause #2\n",
    "          3 => cause #3\n",
    "        For your scenario, no censoring => no 0 values.\n",
    "    duration_col : str\n",
    "        The column name holding time to event (e.g., 't_event').\n",
    "    event_col : str\n",
    "        The column name with the integer-coded event type.\n",
    "    cause_of_interest : int\n",
    "        Which cause you want the CIF for (1, 2, or 3).\n",
    "    calculate_variance : bool\n",
    "        Whether to compute variance/confidence intervals \n",
    "        (if large data, set to False to speed things up).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ajf : AalenJohansenFitter\n",
    "        A lifelines AalenJohansenFitter object containing the fitted CIF.\n",
    "        - ajf.cumulative_density_ => the estimated CIF over time\n",
    "        - ajf.plot() => quick plot method\n",
    "    \"\"\"\n",
    "\n",
    "    # Drop any rows missing duration or event\n",
    "    df = df.dropna(subset=[duration_col, event_col]).copy()\n",
    "\n",
    "    # Ensure event_col is int\n",
    "    df[event_col] = df[event_col].astype(int)\n",
    "\n",
    "    # Initialize the fitter\n",
    "    ajf = AalenJohansenFitter(calculate_variance=calculate_variance)\n",
    "\n",
    "    # Fit it\n",
    "    ajf.fit(\n",
    "        durations=df[duration_col],\n",
    "        event_observed=df[event_col],\n",
    "        event_of_interest=cause_of_interest\n",
    "    )\n",
    "\n",
    "    return ajf\n",
    "\n",
    "\n",
    "def main_competing_risk_analysis(df_competing_risk, criteria=None):\n",
    "    \"\"\"\n",
    "    Example usage: \n",
    "    Suppose you have a DataFrame df_patel_competing with columns:\n",
    "      encounter_block, time_eligibility, time_death, time_discharge_alive,\n",
    "      t_event, outcome in {1,2,3}\n",
    "    We'll fit & plot the CIF for each cause (1=elig,2=death,3=discharge).\n",
    "    \"\"\"\n",
    "\n",
    "    # 2) Fit CIF for cause=1\n",
    "    ajf_cause1 = fit_aalen_johansen_cif(\n",
    "        df_competing_risk,\n",
    "        duration_col='t_event',\n",
    "        event_col='outcome',\n",
    "        cause_of_interest=1,  # 1 => \"eligibility\"\n",
    "        calculate_variance=True\n",
    "    )\n",
    "\n",
    "    # 3) Fit CIF for cause=2\n",
    "    ajf_cause2 = fit_aalen_johansen_cif(\n",
    "        df_competing_risk,\n",
    "        duration_col='t_event',\n",
    "        event_col='outcome',\n",
    "        cause_of_interest=2,  # 2 => \"death\"\n",
    "        calculate_variance=True\n",
    "    )\n",
    "\n",
    "    # 4) Fit CIF for cause=3\n",
    "    ajf_cause3 = fit_aalen_johansen_cif(\n",
    "        df_competing_risk,\n",
    "        duration_col='t_event',\n",
    "        event_col='outcome',\n",
    "        cause_of_interest=3,  # 3 => \"discharge\"\n",
    "        calculate_variance=True\n",
    "    )\n",
    "\n",
    "    # 5) Plot them on the same figure\n",
    "    plt.figure(figsize=(8,6))\n",
    "    # cause=1\n",
    "    ajf_cause1.plot(label=\"Cause=1 (Eligible)\")\n",
    "    # cause=2\n",
    "    ajf_cause2.plot(label=\"Cause=2 (Death)\")\n",
    "    # cause=3\n",
    "    ajf_cause3.plot(label=\"Cause=3 (Discharge)\")\n",
    "    \n",
    "    plt.title(f\"Aalen–Johansen CIF for All Causes- {criteria}\")\n",
    "    plt.xlabel(\"Time (hours)\")\n",
    "    plt.ylabel(\"Cumulative Incidence\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Aalen-Johansen estimator cannot handle tied event times.\n",
    "# If a warning states - data is randomly jittered. It means that t_event is randomly changed by a very small number 0.00000001\n",
    "main_competing_risk_analysis(df_patel_competing, \"Patel\")\n",
    "main_competing_risk_analysis(df_team_competing, \"Team\")\n",
    "main_competing_risk_analysis(df_yellow_competing, \"Yellow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lifelines import AalenJohansenFitter\n",
    "\n",
    "#########################\n",
    "# 1) A helper to fit Aalen–Johansen for cause=1\n",
    "#########################\n",
    "def fit_aalen_johansen_cif(\n",
    "    df,\n",
    "    duration_col='t_event',\n",
    "    event_col='outcome',\n",
    "    cause_of_interest=1,\n",
    "    calculate_variance=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Fits an Aalen–Johansen estimator to compute the cumulative incidence\n",
    "    of a given cause (cause_of_interest) in a multi-state/competing-risks scenario.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Must have columns [duration_col, event_col] with event_col in {1,2,3}.\n",
    "        (No 0 => no censoring).\n",
    "    duration_col : str\n",
    "        Name of the column with time to event (e.g., 't_event').\n",
    "    event_col : str\n",
    "        Name of the column with the integer-coded event type.\n",
    "    cause_of_interest : int\n",
    "        Which cause you want the CIF for (1,2,3).\n",
    "    calculate_variance : bool\n",
    "        Whether to compute variance/confidence intervals (slows it down if large).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ajf : AalenJohansenFitter\n",
    "        A lifelines AalenJohansenFitter object containing the fitted CIF.\n",
    "        - ajf.cumulative_density_ => DataFrame with index 'timeline' & column(s) like 'CIF_1'\n",
    "        - ajf.plot(...) => quick plotting method\n",
    "    \"\"\"\n",
    "\n",
    "    # Drop any rows missing time or event\n",
    "    df = df.dropna(subset=[duration_col, event_col]).copy()\n",
    "\n",
    "    # Ensure event_col is int\n",
    "    df[event_col] = df[event_col].astype(int)\n",
    "\n",
    "    # Create the fitter\n",
    "    ajf = AalenJohansenFitter(calculate_variance=calculate_variance)\n",
    "    ajf.fit(\n",
    "        durations=df[duration_col],\n",
    "        event_observed=df[event_col],\n",
    "        event_of_interest=cause_of_interest\n",
    "    )\n",
    "    return ajf\n",
    "\n",
    "\n",
    "#########################\n",
    "# 2) A function to extract CIF data & median from a fitted AalenJohansenFitter\n",
    "#########################\n",
    "def extract_cif_and_median(ajf):\n",
    "    \"\"\"\n",
    "    Given a fitted AalenJohansenFitter, extract:\n",
    "      - A DataFrame with columns [time, cif, lower_ci, upper_ci] (if variance was computed)\n",
    "      - The approximate median time (time at which CIF crosses 0.5), or np.inf if never crosses\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cif_df : pd.DataFrame\n",
    "        Columns: time, cif, lower_ci, upper_ci\n",
    "    median_time : float or np.inf\n",
    "        The time at which CIF ~ 0.5\n",
    "    \"\"\"\n",
    "\n",
    "    # The lifelines object stores the CIF in .cumulative_density_\n",
    "    # The index is typically named 'timeline', and the main column is 'CIF_1'.\n",
    "    df_cif = ajf.cumulative_density_.copy()  # a DataFrame indexed by timeline\n",
    "\n",
    "    # Move the 'timeline' index to a 'time' column\n",
    "    df_cif['time'] = df_cif.index\n",
    "    df_cif.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Rename \"CIF_1\" -> \"cif\"\n",
    "    if 'CIF_1' not in df_cif.columns:\n",
    "        raise KeyError(\"Expected column 'CIF_1' in cumulative_density_, but not found. Check column names.\")\n",
    "    df_cif.rename(columns={'CIF_1': 'cif'}, inplace=True)\n",
    "\n",
    "    # If user turned on variance, we may see columns like 'CIF_1_lower_0.95', 'CIF_1_upper_0.95'\n",
    "    possible_lower = [col for col in df_cif.columns if '_lower_' in col]\n",
    "    possible_upper = [col for col in df_cif.columns if '_upper_' in col]\n",
    "\n",
    "    if possible_lower and possible_upper:\n",
    "        df_cif.rename(columns={\n",
    "            possible_lower[0]: 'lower_ci',\n",
    "            possible_upper[0]: 'upper_ci'\n",
    "        }, inplace=True)\n",
    "    else:\n",
    "        # No variance columns found => fill with NaN\n",
    "        df_cif['lower_ci'] = np.nan\n",
    "        df_cif['upper_ci'] = np.nan\n",
    "\n",
    "    # Approximate median time => time at which CIF >= 0.5\n",
    "    cif_vals = df_cif['cif'].values\n",
    "    time_vals= df_cif['time'].values\n",
    "\n",
    "    crossing_idx = np.where(cif_vals >= 0.5)[0]\n",
    "    if len(crossing_idx) == 0:\n",
    "        median_time = np.inf\n",
    "    else:\n",
    "        median_time = time_vals[crossing_idx[0]]\n",
    "\n",
    "    return df_cif[['time','cif','lower_ci','upper_ci']], median_time\n",
    "\n",
    "\n",
    "#########################\n",
    "# 3) The main function\n",
    "#########################\n",
    "def main_competing_risk_analysis(\n",
    "    df_patel_competing,\n",
    "    df_team_competing,\n",
    "    df_yellow_competing\n",
    "):\n",
    "    \"\"\"\n",
    "    1) Fit cause=1 (eligibility) Aalen–Johansen for each of the three data sets\n",
    "    2) Plot them all on one figure\n",
    "    3) Print & store the median times\n",
    "    4) Save the CIF data (including variance if available) to CSV\n",
    "       so you can share it w/o patient-level data\n",
    "    \"\"\"\n",
    "\n",
    "    # 3a) Fit each with variance\n",
    "    ajf_patel  = fit_aalen_johansen_cif(df_patel_competing, cause_of_interest=1, calculate_variance=True)\n",
    "    ajf_team   = fit_aalen_johansen_cif(df_team_competing, cause_of_interest=1, calculate_variance=True)\n",
    "    ajf_yellow = fit_aalen_johansen_cif(df_yellow_competing, cause_of_interest=1, calculate_variance=True)\n",
    "\n",
    "    # 3b) Extract CIF data & median\n",
    "    patel_cif_df,  patel_median  = extract_cif_and_median(ajf_patel)\n",
    "    team_cif_df,   team_median   = extract_cif_and_median(ajf_team)\n",
    "    yellow_cif_df, yellow_median = extract_cif_and_median(ajf_yellow)\n",
    "\n",
    "    # 3c) Print median times\n",
    "    print(\"=== Median Times to First Eligibility (Aalen–Johansen, cause=1) ===\")\n",
    "    print(f\"Patel:  {patel_median} hours\")\n",
    "    print(f\"TEAM:   {team_median} hours\")\n",
    "    print(f\"Yellow: {yellow_median} hours\")\n",
    "\n",
    "    # 3d) Plot them all on one figure\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.step(patel_cif_df['time'], patel_cif_df['cif'],  where='post', label=\"Patel\")\n",
    "    plt.step(team_cif_df['time'],  team_cif_df['cif'],   where='post', label=\"TEAM\")\n",
    "    plt.step(yellow_cif_df['time'],yellow_cif_df['cif'],where='post', label=\"Yellow\")\n",
    "\n",
    "    plt.title(\"CIF for Time to Eligibility (Cause=1) - Aalen–Johansen\")\n",
    "    plt.xlabel(\"Time (hours)\")\n",
    "    plt.ylabel(\"Cumulative Incidence\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # 3e) Optionally, save the combined CIF data to CSV\n",
    "    patel_cif_df['Criteria']  = 'Patel'\n",
    "    team_cif_df['Criteria']   = 'TEAM'\n",
    "    yellow_cif_df['Criteria'] = 'Yellow'\n",
    "    combined_cif_df = pd.concat([patel_cif_df, team_cif_df, yellow_cif_df], ignore_index=True)\n",
    "\n",
    "    combined_cif_df.to_csv(\"../output/final/aalen_johansen_cif_eligibility.csv\", index=False)\n",
    "    print(\"\\nSaved combined CIF data (with variance if available) to CSV.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_competing_risk_analysis(df_patel_competing, df_team_competing, df_yellow_competing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ajf_patel  = fit_aalen_johansen_cif(df_patel_competing, cause_of_interest=1)\n",
    "ajf_team   = fit_aalen_johansen_cif(df_team_competing, cause_of_interest=1)\n",
    "ajf_yellow = fit_aalen_johansen_cif(df_yellow_competing, cause_of_interest=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ajf_patel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final figures and tables\n",
    "\n",
    "1. Figure 1: Percentage of encounter satisfying Patel, TEAM, and any yellow or GREEN criteria\n",
    "2. Figure 2: Percentage of business hours each encounter was eligible for different criteria\n",
    "3. Figure 3: Percentage of business hours not eligible for each criteria broken down by subcomponent failure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eligibility by encounter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_encounters_meeting_criteria(df):\n",
    "    total_encounters = df['hospitalization_id'].nunique()  # Calculate total unique encounters\n",
    "    # For each encounter, check if they ever met the criteria and sum up\n",
    "    criteria_counts = df.groupby('hospitalization_id').agg(\n",
    "        patel_met=('patel_flag', lambda x: x.max()),\n",
    "        team_met=('team_flag', lambda x: x.max()),\n",
    "        any_yellow_or_green_no_red_met=('any_yellow_or_green_no_red', lambda x: x.max()),\n",
    "        all_green_no_red=('all_green_no_red', lambda x: x.max()),\n",
    "        all_green=('all_green', lambda x: x.max()),\n",
    "    ).sum().reset_index()\n",
    "\n",
    "    criteria_counts.columns = ['Criteria', 'Number of Encounters']\n",
    "    criteria_counts['Percentage'] = (criteria_counts['Number of Encounters'] / total_encounters) * 100\n",
    "    criteria_counts['Total Encounters'] = total_encounters  # Save total_encounters in a column\n",
    "    \n",
    "    return criteria_counts\n",
    "\n",
    "# Generate the criteria comparison table\n",
    "\n",
    "criteria_comparison_table = count_encounters_meeting_criteria(business_hours_df)\n",
    "criteria_comparison_table['site_name'] = pyCLIF.helper[\"site_name\"]\n",
    "pd.DataFrame(criteria_comparison_table).to_csv(f'../output/final/eligibility_by_hosp_{pyCLIF.helper[\"site_name\"]}_{datetime.now().date()}.csv',index=False)\n",
    "criteria_comparison_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the criteria for clarity\n",
    "criteria_comparison_table['Criteria'] = criteria_comparison_table['Criteria'].replace({\n",
    "    'patel_met': 'Patel',\n",
    "    'team_met': 'TEAM',\n",
    "    'any_yellow_or_green_no_red_met': 'Yellow'\n",
    "})\n",
    "\n",
    "# Define custom colors\n",
    "custom_colors = ['#983232', '#003f5c', '#fdfd96']  # Maroon, Dark Blue, Pastel Yellow\n",
    "\n",
    "# Plot the bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "barplot = sns.barplot(x='Criteria', y='Percentage', data=criteria_comparison_table, palette=custom_colors)\n",
    "\n",
    "# Rotate the x-axis labels\n",
    "# plt.xticks(rotation=90)\n",
    "\n",
    "# Add percentages on top of the bars\n",
    "for index, row in criteria_comparison_table.iterrows():\n",
    "    barplot.text(index, row['Percentage'] + 0.5, f\"{row['Percentage']:.1f}%\", \n",
    "                 color='black', ha=\"center\")\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Criteria')\n",
    "plt.ylabel('Percentage of Encounters')\n",
    "# plt.title('Percentage of Encounters Meeting Each Criterion')\n",
    "\n",
    "# Save the graph\n",
    "plt.savefig(f'../output/final/graphs/eligibility_by_hosp_{pyCLIF.helper[\"site_name\"]}_{datetime.now().date()}.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eligibility by business hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute the percentage of business hours\n",
    "def compute_percentage_hours_by_criteria(df, criteria_columns):\n",
    "    \"\"\"\n",
    "    Compute the percentage of business hours each encounter was eligible for different criteria.\n",
    "    \"\"\"\n",
    "    # Calculate the total number of business hours per encounter\n",
    "    total_business_hours = df.groupby('hospitalization_id')['time_biz'].max()\n",
    "    # Filter out encounters with zero business hours to avoid division by zero\n",
    "    total_business_hours = total_business_hours[total_business_hours > 0]\n",
    "    # Sum the number of hours each criterion is met for each encounter\n",
    "    hours_criteria = df.groupby('hospitalization_id').agg({criterion: 'sum' for criterion in criteria_columns})\n",
    "    # Retain only the encounters with non-zero business hours\n",
    "    hours_criteria = hours_criteria.loc[total_business_hours.index]\n",
    "    # Calculate the percentage of business hours met for each criterion\n",
    "    percentage_hours_by_criteria = hours_criteria.divide(total_business_hours, axis=0) * 100\n",
    "    # Calculate the mean percentage of hours met for each criterion across all encounters\n",
    "    avg_percentage_by_criteria = percentage_hours_by_criteria.mean().reset_index()\n",
    "    avg_percentage_by_criteria.columns = ['Criteria', 'Average Percentage of Hours Met']\n",
    "    return avg_percentage_by_criteria\n",
    "\n",
    "# Define the mapping for the criteria\n",
    "criteria_columns = ['patel_flag', 'team_flag', 'any_yellow_or_green_no_red']\n",
    "criteria_mapping = {\n",
    "    'patel_flag': 'Patel',\n",
    "    'team_flag': 'TEAM',\n",
    "    'any_yellow_or_green_no_red': 'Yellow'\n",
    "}\n",
    "\n",
    "# Calculate the percentage of business hours met for each criterion\n",
    "avg_percentage_by_criteria = compute_percentage_hours_by_criteria(business_hours_df, criteria_columns)\n",
    "\n",
    "# Replace the criteria names according to the mapping\n",
    "avg_percentage_by_criteria['Criteria'] = avg_percentage_by_criteria['Criteria'].replace(criteria_mapping)\n",
    "avg_percentage_by_criteria['site_name'] = pyCLIF.helper[\"site_name\"]\n",
    "pd.DataFrame(avg_percentage_by_criteria).to_csv(f'../output/final/eligibility_by_hour_{pyCLIF.helper[\"site_name\"]}_{datetime.now().date()}.csv',index=False)\n",
    "avg_percentage_by_criteria\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Custom colors for each criterion: Patel (Maroon), TEAM (Dark Blue), Yellow (Pastel Yellow)\n",
    "custom_colors = ['#983232', '#003366', '#fdfd96']\n",
    "\n",
    "ax = sns.barplot(x='Criteria', y='Average Percentage of Hours Met', data=avg_percentage_by_criteria, palette=custom_colors)\n",
    "\n",
    "# Add the percentage as labels on top of the bars\n",
    "for i, row in avg_percentage_by_criteria.iterrows():\n",
    "    ax.text(i, row['Average Percentage of Hours Met'] + 0.5, f'{row[\"Average Percentage of Hours Met\"]:.2f}%', \n",
    "            ha='center', color='black', fontsize=12)\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('Percentage of Business Hours Each Encounter Was Eligible for Different Criteria')\n",
    "plt.xlabel('Criteria')\n",
    "plt.ylabel('Average Percentage of Business Hours (%)')\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'../output/final/graphs/eligibility_by_hour_{pyCLIF.helper[\"site_name\"]}_{datetime.now().date()}.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Failure by subcomponents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your criteria and corresponding subcomponent flags\n",
    "criteria_info = {\n",
    "    'patel_flag': {'resp_flag': 'patel_resp_flag', 'cardio_flag': 'patel_cardio_flag'},\n",
    "    'team_flag': {'resp_flag': 'team_resp_flag', 'cardio_flag': 'team_cardio_flag'},\n",
    "    'any_yellow_or_green_no_red': {'resp_flag': 'yellow_resp_flag', 'cardio_flag': 'yellow_cardio_flag'}\n",
    "}\n",
    "\n",
    "# Initialize an empty list to store results\n",
    "results = []\n",
    "\n",
    "# Loop over each criterion\n",
    "for criterion, flags in criteria_info.items():\n",
    "    resp_flag = flags['resp_flag']\n",
    "    cardio_flag = flags['cardio_flag']\n",
    "    \n",
    "    # Calculate total hours per hospitalization_id\n",
    "    total_hours = final_df.groupby('hospitalization_id').size().rename('total_hours')\n",
    "    \n",
    "    # Create failure indicators\n",
    "    df_failure = final_df.copy()\n",
    "    df_failure['resp_only_failure'] = ((df_failure[resp_flag] == 0) & (df_failure[cardio_flag] == 1)).astype(int)\n",
    "    df_failure['cardio_only_failure'] = ((df_failure[resp_flag] == 1) & (df_failure[cardio_flag] == 0)).astype(int)\n",
    "    df_failure['both_failures'] = ((df_failure[resp_flag] == 0) & (df_failure[cardio_flag] == 0)).astype(int)\n",
    "    \n",
    "    # Aggregate the counts per hospitalization_id\n",
    "    failure_counts = df_failure.groupby('hospitalization_id')[['resp_only_failure', 'cardio_only_failure', 'both_failures']].sum()\n",
    "    \n",
    "    # Merge with total hours\n",
    "    failure_counts = failure_counts.merge(total_hours, left_index=True, right_index=True)\n",
    "    \n",
    "    # Calculate percentages\n",
    "    failure_counts['resp_only_failure_perc'] = (failure_counts['resp_only_failure'] * 100 / failure_counts['total_hours']).round(3)\n",
    "    failure_counts['cardio_only_failure_perc'] = (failure_counts['cardio_only_failure'] * 100 / failure_counts['total_hours']).round(3)\n",
    "    failure_counts['both_failures_perc'] = (failure_counts['both_failures'] * 100 / failure_counts['total_hours']).round(3)\n",
    "    \n",
    "    # Calculate total failure percentage\n",
    "    failure_counts['total_failure_perc'] = (\n",
    "        failure_counts['resp_only_failure'] + failure_counts['cardio_only_failure'] + failure_counts['both_failures']\n",
    "    ) * 100 / failure_counts['total_hours']\n",
    "    \n",
    "    # Calculate criterion met percentage\n",
    "    criterion_met = final_df.groupby('hospitalization_id')[criterion].sum().rename('criterion_met_hours')\n",
    "    failure_counts = failure_counts.merge(criterion_met, left_index=True, right_index=True)\n",
    "    failure_counts['criterion_met_perc'] = (failure_counts['criterion_met_hours'] * 100 / failure_counts['total_hours']).round(3)\n",
    "    \n",
    "    # Add criterion name to the DataFrame\n",
    "    failure_counts['Criteria'] = criterion\n",
    "    \n",
    "    # Append to results\n",
    "    results.append(failure_counts.reset_index())\n",
    "\n",
    "# Concatenate results for all criteria\n",
    "all_failure_counts = pd.concat(results, ignore_index=True)\n",
    "\n",
    "# Now, calculate the average percentages across all hospitalizations for each criterion\n",
    "avg_failure_percentages = all_failure_counts.groupby('Criteria').agg({\n",
    "    'resp_only_failure_perc': 'mean',\n",
    "    'cardio_only_failure_perc': 'mean',\n",
    "    'both_failures_perc': 'mean',\n",
    "    'total_failure_perc': 'mean',\n",
    "    'criterion_met_perc': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "# Rename columns for clarity\n",
    "avg_failure_percentages = avg_failure_percentages.rename(columns={\n",
    "    'resp_only_failure_perc': 'Resp Failure Only',\n",
    "    'cardio_only_failure_perc': 'Cardio Failure Only',\n",
    "    'both_failures_perc': 'Both Failures',\n",
    "    'total_failure_perc': 'Total Failure',\n",
    "    'criterion_met_perc': 'Criterion Met'\n",
    "})\n",
    "\n",
    "# Display the average failure percentages\n",
    "criteria_mapping = {\n",
    "    'patel_flag': 'Patel',\n",
    "    'team_flag': 'TEAM',\n",
    "    'any_yellow_or_green_no_red': 'Yellow'\n",
    "}\n",
    "\n",
    "avg_failure_percentages['Criteria'] = avg_failure_percentages['Criteria'].replace(criteria_mapping)\n",
    "avg_failure_percentages['site_name'] = pyCLIF.helper[\"site_name\"]\n",
    "pd.DataFrame(avg_failure_percentages).to_csv(f'../output/final/avg_failure_percentages_{pyCLIF.helper[\"site_name\"]}_{datetime.now().date()}.csv',index=False)\n",
    "avg_failure_percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "# Add bars for each criterion\n",
    "fig.add_trace(go.Bar(\n",
    "    x=avg_failure_percentages['Criteria'],\n",
    "    y=avg_failure_percentages['Criterion Met'],\n",
    "    marker=dict(color=avg_failure_percentages['Criteria'].map({\n",
    "        'Yellow': '#fdfd96',\n",
    "        'Patel': '#983232',\n",
    "        'TEAM': '#003366'\n",
    "    })),  # Custom colors\n",
    "    text=avg_failure_percentages['Criterion Met'].round(2),\n",
    "    textposition='auto'\n",
    "))\n",
    "\n",
    "# Customize the layout\n",
    "fig.update_layout(\n",
    "    # title='Average Percentage of Business Hours Each Criterion Is Met',\n",
    "    xaxis_title='Criteria',\n",
    "    yaxis_title='Average Percentage of Business Hours Met (%)',\n",
    "    yaxis=dict(range=[0, 100]),  # Ensure y-axis range is 0-100%\n",
    "    template='plotly_white'\n",
    ")\n",
    "\n",
    "# Save the plot\n",
    "fig.write_image(f'../output/final/graphs/avg_failure_percentages_{pyCLIF.helper[\"site_name\"]}_{datetime.now().date()}.png')\n",
    "# fig.savefig(f'../output/final/graphs/avg_failure_percentages_{pyCLIF.helper[\"site_name\"]}.png')\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a stacked bar plot using Plotly\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add bars for Cardio Failure Only\n",
    "fig.add_trace(go.Bar(\n",
    "    x=avg_failure_percentages['Criteria'],\n",
    "    y=avg_failure_percentages['Cardio Failure Only'],\n",
    "    name='Cardio Failure Only',\n",
    "    marker_color='#003366'  # Dark Blue\n",
    "))\n",
    "\n",
    "# Add bars for Resp Failure Only\n",
    "fig.add_trace(go.Bar(\n",
    "    x=avg_failure_percentages['Criteria'],\n",
    "    y=avg_failure_percentages['Resp Failure Only'],\n",
    "    name='Resp Failure Only',\n",
    "    marker_color='#983232'  # Maroon\n",
    "))\n",
    "\n",
    "# Add bars for Both Failures\n",
    "fig.add_trace(go.Bar(\n",
    "    x=avg_failure_percentages['Criteria'],\n",
    "    y=avg_failure_percentages['Both Failures'],\n",
    "    name='Both Failures',\n",
    "    marker_color='#fdfd96'  # Pastel Yellow\n",
    "))\n",
    "\n",
    "# Customize the layout\n",
    "fig.update_layout(\n",
    "    barmode='stack',\n",
    "    xaxis_title='Criteria',\n",
    "    yaxis_title='Average Percentage of Business Hours Not Met (%)',\n",
    "    yaxis=dict(range=[0, 100]),\n",
    "    template='plotly_white',\n",
    "    legend_title='Failure Type'\n",
    ")\n",
    "# Save the plot\n",
    "fig.write_image(f'../output/final/graphs/avg_failure_components_{pyCLIF.helper[\"site_name\"]}_{datetime.now().date()}.png')\n",
    "# Show the plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time to eligibility from intubation\n",
    "\n",
    "This analysis includes a 4 hour \"cool off\" period\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Filter the DataFrame for rows where the Patel criteria are met\n",
    "patel_eligible_df = business_hours_df[business_hours_df['patel_flag'] == 1]\n",
    "\n",
    "# Find the first time each patient meets the Patel criteria\n",
    "first_eligibility_times_patel = patel_eligible_df.groupby('hospitalization_id')['time_from_vent'].min().reset_index()\n",
    "\n",
    "# Create the survival analysis dataset\n",
    "survival_analysis_df_patel = business_hours_df[['hospitalization_id']].drop_duplicates().copy()\n",
    "\n",
    "# Merge with the first eligibility times\n",
    "survival_analysis_df_patel = pd.merge(survival_analysis_df_patel, first_eligibility_times_patel, on='hospitalization_id', how='left')\n",
    "\n",
    "# Fill NaN values with 27 for patients who were never eligible\n",
    "survival_analysis_df_patel['time_from_vent'] = survival_analysis_df_patel['time_from_vent'].fillna(27)\n",
    "\n",
    "# Create the 'eligible' column\n",
    "survival_analysis_df_patel['eligible'] = (survival_analysis_df_patel['time_from_vent'] != 27).astype(int)\n",
    "\n",
    "# Rename columns\n",
    "survival_analysis_df_patel.rename(columns={'time_from_vent': 'time_to_first_eligibility'}, inplace=True)\n",
    "\n",
    "# Add +1 to time_to_first_eligibility\n",
    "survival_analysis_df_patel['time_to_first_eligibility'] = survival_analysis_df_patel['time_to_first_eligibility'] + 1\n",
    "\n",
    "# Display the final dataset\n",
    "print(survival_analysis_df_patel.head())\n",
    "\n",
    "# Repeat the process for TEAM criteria\n",
    "team_eligible_df = business_hours_df[business_hours_df['team_flag'] == 1]\n",
    "first_eligibility_times_team = team_eligible_df.groupby('hospitalization_id')['time_from_vent'].min().reset_index()\n",
    "survival_analysis_df_team = business_hours_df[['hospitalization_id']].drop_duplicates().copy()\n",
    "survival_analysis_df_team = pd.merge(survival_analysis_df_team, first_eligibility_times_team, on='hospitalization_id', how='left')\n",
    "survival_analysis_df_team['time_from_vent'] = survival_analysis_df_team['time_from_vent'].fillna(27)\n",
    "survival_analysis_df_team['eligible'] = (survival_analysis_df_team['time_from_vent'] != 27).astype(int)\n",
    "survival_analysis_df_team.rename(columns={'time_from_vent': 'time_to_first_eligibility'}, inplace=True)\n",
    "survival_analysis_df_team['time_to_first_eligibility'] = survival_analysis_df_team['time_to_first_eligibility'] + 1\n",
    "\n",
    "# Repeat the process for Yellow criteria\n",
    "yellow_eligible_df = business_hours_df[business_hours_df['any_yellow_or_green_no_red'] == 1]\n",
    "first_eligibility_times_yellow = yellow_eligible_df.groupby('hospitalization_id')['time_from_vent'].min().reset_index()\n",
    "survival_analysis_df_yellow = business_hours_df[['hospitalization_id']].drop_duplicates().copy()\n",
    "survival_analysis_df_yellow = pd.merge(survival_analysis_df_yellow, first_eligibility_times_yellow, on='hospitalization_id', how='left')\n",
    "survival_analysis_df_yellow['time_from_vent'] = survival_analysis_df_yellow['time_from_vent'].fillna(27)\n",
    "survival_analysis_df_yellow['eligible'] = (survival_analysis_df_yellow['time_from_vent'] != 27).astype(int)\n",
    "survival_analysis_df_yellow.rename(columns={'time_from_vent': 'time_to_first_eligibility'}, inplace=True)\n",
    "survival_analysis_df_yellow['time_to_first_eligibility'] = survival_analysis_df_yellow['time_to_first_eligibility'] + 1\n",
    "\n",
    "# Display the final datasets\n",
    "print(survival_analysis_df_team.head())\n",
    "print(survival_analysis_df_yellow.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the KaplanMeierFitter for Patel, TEAM and Yellow criteria\n",
    "kmf_patel = KaplanMeierFitter()\n",
    "kmf_team = KaplanMeierFitter()\n",
    "kmf_yellow = KaplanMeierFitter()\n",
    "\n",
    "# fit the data for Patel criteria\n",
    "kmf_patel.fit(durations=survival_analysis_df_patel['time_to_first_eligibility'], event_observed=survival_analysis_df_patel['eligible'], label='Patel Criteria')\n",
    "\n",
    "# Fit the data for TEAM criteria\n",
    "kmf_team.fit(durations=survival_analysis_df_team['time_to_first_eligibility'], event_observed=survival_analysis_df_team['eligible'], label='TEAM Criteria')\n",
    "\n",
    "# Fit the data for Yellow criteria\n",
    "kmf_yellow.fit(durations=survival_analysis_df_yellow['time_to_first_eligibility'], event_observed=survival_analysis_df_yellow['eligible'], label='Yellow Criteria')\n",
    "\n",
    "# Plot the cumulative incidence function for all criteria\n",
    "ax = kmf_patel.plot_cumulative_density()\n",
    "kmf_team.plot_cumulative_density(ax=ax)\n",
    "kmf_yellow.plot_cumulative_density(ax=ax)\n",
    "\n",
    "plt.title('Cumulative Incidence Function for Time to First Eligibility')\n",
    "plt.xlabel('Time to First Eligibility from Intubation (hours)')\n",
    "plt.ylabel('Cumulative Incidence Probability')\n",
    "plt.legend()\n",
    "\n",
    "# Save the plot\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "plt.savefig(f'../output/final/graphs/cif_{pyCLIF.helper[\"site_name\"]}_{timestamp}.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Estimate the median time to first eligibility for all criteria\n",
    "median_time_to_first_eligibility_patel = kmf_patel.median_survival_time_\n",
    "median_time_to_first_eligibility_team = kmf_team.median_survival_time_\n",
    "median_time_to_first_eligibility_yellow = kmf_yellow.median_survival_time_\n",
    "\n",
    "print(f\"Median time to first eligibility (Patel): {median_time_to_first_eligibility_patel} hours\")\n",
    "print(f\"Median time to first eligibility (TEAM): {median_time_to_first_eligibility_team} hours\")\n",
    "print(f\"Median time to first eligibility (Yellow): {median_time_to_first_eligibility_yellow} hours\")\n",
    "\n",
    "# Calculate the cumulative incidence function value at time = 1 for all criteria\n",
    "cif_value_at_1_patel = 1 - kmf_patel.predict(1)\n",
    "cif_value_at_1_team = 1 - kmf_team.predict(1)\n",
    "cif_value_at_1_yellow = 1 - kmf_yellow.predict(1)\n",
    "\n",
    "print(f\"Cumulative Incidence Function value at time = 1 (Patel): {cif_value_at_1_patel:.0%}\")\n",
    "print(f\"Cumulative Incidence Function value at time = 1 (TEAM): {cif_value_at_1_team:.0%}\")\n",
    "print(f\"Cumulative Incidence Function value at time = 1 (Yellow): {cif_value_at_1_yellow:.0%}\")\n",
    "\n",
    "\n",
    "# Save the cumulative incidence function data to CSV files\n",
    "cif_patel = kmf_patel.cumulative_density_.reset_index()\n",
    "cif_team = kmf_team.cumulative_density_.reset_index()\n",
    "cif_yellow = kmf_yellow.cumulative_density_.reset_index()\n",
    "\n",
    "# Add a column to identify the criteria\n",
    "cif_patel['Criteria'] = 'Patel'\n",
    "cif_team['Criteria'] = 'TEAM'\n",
    "cif_yellow['Criteria'] = 'Yellow'\n",
    "\n",
    "# Get the site name from pyCLIF helper\n",
    "site_name = pyCLIF.helper[\"site_name\"]\n",
    "\n",
    "cif_patel['Site'] = site_name\n",
    "cif_team['Site'] = site_name\n",
    "cif_yellow['Site'] = site_name\n",
    "\n",
    "# Save to CSV files with site name in the file names\n",
    "timestamp = datetime.now().date()\n",
    "cif_patel.to_csv(f'../output/final/cif_patel_{pyCLIF.helper[\"site_name\"]}_{timestamp}.csv', index=False)\n",
    "cif_team.to_csv(f'../output/final/cif_team_{pyCLIF.helper[\"site_name\"]}_{timestamp}.csv', index=False)\n",
    "cif_yellow.to_csv(f'../output/final/cif_yellow_{pyCLIF.helper[\"site_name\"]}_{timestamp}.csv', index=False)\n",
    "\n",
    "# Calculate 95% CI for each criteria\n",
    "ci_patel_ci = kmf_patel.confidence_interval_cumulative_density_.reset_index()\n",
    "ci_team_ci = kmf_team.confidence_interval_cumulative_density_.reset_index()\n",
    "ci_yellow_ci = kmf_yellow.confidence_interval_cumulative_density_.reset_index()\n",
    "\n",
    "# Add a column to identify the criteria\n",
    "ci_patel_ci['Criteria'] = 'Patel'\n",
    "ci_team_ci['Criteria'] = 'TEAM'\n",
    "ci_yellow_ci['Criteria'] = 'Yellow'\n",
    "ci_patel_ci['Site'] = site_name\n",
    "ci_team_ci['Site'] = site_name\n",
    "ci_yellow_ci['Site'] = site_name\n",
    "# Save to CSV files with site name in the file names\n",
    "timestamp = datetime.now().date()\n",
    "ci_patel_ci.to_csv(f'../output/final/cif_patel_ci_{pyCLIF.helper[\"site_name\"]}_{timestamp}.csv', index=False)\n",
    "ci_team_ci.to_csv(f'../output/final/cif_team_ci_{pyCLIF.helper[\"site_name\"]}_{timestamp}.csv', index=False)\n",
    "ci_yellow_ci.to_csv(f'../output/final/cif_yellow_ci_{pyCLIF.helper[\"site_name\"]}_{timestamp}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame for rows where the Patel criteria are met\n",
    "patel_eligible_df = business_hours_df[business_hours_df['patel_flag'] == 1]\n",
    "\n",
    "# Find the first time each patient meets the Patel criteria\n",
    "first_eligibility_times_patel = patel_eligible_df.groupby('hospitalization_id')['time_biz'].min().reset_index()\n",
    "\n",
    "# Create the survival analysis dataset\n",
    "survival_analysis_df_patel = business_hours_df[['hospitalization_id']].drop_duplicates().copy()\n",
    "\n",
    "# Merge with the first eligibility times\n",
    "survival_analysis_df_patel = pd.merge(survival_analysis_df_patel, first_eligibility_times_patel, on='hospitalization_id', how='left')\n",
    "\n",
    "# Fill NaN values with 27 for patients who were never eligible\n",
    "survival_analysis_df_patel['time_biz'] = survival_analysis_df_patel['time_biz'].fillna(27)\n",
    "\n",
    "# Create the 'eligible' column\n",
    "survival_analysis_df_patel['eligible'] = (survival_analysis_df_patel['time_biz'] != 27).astype(int)\n",
    "\n",
    "# Rename columns\n",
    "survival_analysis_df_patel.rename(columns={'time_biz': 'time_to_first_eligibility'}, inplace=True)\n",
    "\n",
    "# Add +1 to time_to_first_eligibility\n",
    "survival_analysis_df_patel['time_to_first_eligibility'] = survival_analysis_df_patel['time_to_first_eligibility'] + 1\n",
    "\n",
    "# Display the final dataset\n",
    "print(survival_analysis_df_patel.head())\n",
    "\n",
    "# Repeat the process for TEAM criteria\n",
    "team_eligible_df = business_hours_df[business_hours_df['team_flag'] == 1]\n",
    "first_eligibility_times_team = team_eligible_df.groupby('hospitalization_id')['time_biz'].min().reset_index()\n",
    "survival_analysis_df_team = business_hours_df[['hospitalization_id']].drop_duplicates().copy()\n",
    "survival_analysis_df_team = pd.merge(survival_analysis_df_team, first_eligibility_times_team, on='hospitalization_id', how='left')\n",
    "survival_analysis_df_team['time_biz'] = survival_analysis_df_team['time_biz'].fillna(27)\n",
    "survival_analysis_df_team['eligible'] = (survival_analysis_df_team['time_biz'] != 27).astype(int)\n",
    "survival_analysis_df_team.rename(columns={'time_biz': 'time_to_first_eligibility'}, inplace=True)\n",
    "survival_analysis_df_team['time_to_first_eligibility'] = survival_analysis_df_team['time_to_first_eligibility'] + 1\n",
    "\n",
    "# Repeat the process for Yellow criteria\n",
    "yellow_eligible_df = business_hours_df[business_hours_df['any_yellow_or_green_no_red'] == 1]\n",
    "first_eligibility_times_yellow = yellow_eligible_df.groupby('hospitalization_id')['time_biz'].min().reset_index()\n",
    "survival_analysis_df_yellow = business_hours_df[['hospitalization_id']].drop_duplicates().copy()\n",
    "survival_analysis_df_yellow = pd.merge(survival_analysis_df_yellow, first_eligibility_times_yellow, on='hospitalization_id', how='left')\n",
    "survival_analysis_df_yellow['time_biz'] = survival_analysis_df_yellow['time_biz'].fillna(27)\n",
    "survival_analysis_df_yellow['eligible'] = (survival_analysis_df_yellow['time_biz'] != 27).astype(int)\n",
    "survival_analysis_df_yellow.rename(columns={'time_biz': 'time_to_first_eligibility'}, inplace=True)\n",
    "survival_analysis_df_yellow['time_to_first_eligibility'] = survival_analysis_df_yellow['time_to_first_eligibility'] + 1\n",
    "\n",
    "# Display the final datasets\n",
    "print(survival_analysis_df_team.head())\n",
    "print(survival_analysis_df_yellow.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the KaplanMeierFitter for Patel, TEAM and Yellow criteria\n",
    "kmf_patel = KaplanMeierFitter()\n",
    "kmf_team = KaplanMeierFitter()\n",
    "kmf_yellow = KaplanMeierFitter()\n",
    "\n",
    "# fit the data for Patel criteria\n",
    "kmf_patel.fit(durations=survival_analysis_df_patel['time_to_first_eligibility'], event_observed=survival_analysis_df_patel['eligible'], label='Patel Criteria')\n",
    "\n",
    "# Fit the data for TEAM criteria\n",
    "kmf_team.fit(durations=survival_analysis_df_team['time_to_first_eligibility'], event_observed=survival_analysis_df_team['eligible'], label='TEAM Criteria')\n",
    "\n",
    "# Fit the data for Yellow criteria\n",
    "kmf_yellow.fit(durations=survival_analysis_df_yellow['time_to_first_eligibility'], event_observed=survival_analysis_df_yellow['eligible'], label='Yellow Criteria')\n",
    "\n",
    "# Plot the cumulative incidence function for all criteria\n",
    "ax = kmf_patel.plot_cumulative_density()\n",
    "kmf_team.plot_cumulative_density(ax=ax)\n",
    "kmf_yellow.plot_cumulative_density(ax=ax)\n",
    "\n",
    "plt.title('Cumulative Incidence Function for Time to First Eligibility')\n",
    "plt.xlabel('Time to First Eligibility (business hours)')\n",
    "plt.ylabel('Cumulative Incidence Probability')\n",
    "# Save the plot\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "plt.savefig(f'../output/final/graphs/cif_b_hours_{pyCLIF.helper[\"site_name\"]}_{timestamp}.png')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Estimate the median time to first eligibility for all criteria\n",
    "median_time_to_first_eligibility_patel = kmf_patel.median_survival_time_\n",
    "median_time_to_first_eligibility_team = kmf_team.median_survival_time_\n",
    "median_time_to_first_eligibility_yellow = kmf_yellow.median_survival_time_\n",
    "\n",
    "print(f\"Median time to first eligibility (Patel): {median_time_to_first_eligibility_patel} hours\")\n",
    "print(f\"Median time to first eligibility (TEAM): {median_time_to_first_eligibility_team} hours\")\n",
    "print(f\"Median time to first eligibility (Yellow): {median_time_to_first_eligibility_yellow} hours\")\n",
    "\n",
    "# Calculate the cumulative incidence function value at time = 1 for all criteria\n",
    "cif_value_at_1_patel = 1 - kmf_patel.predict(1)\n",
    "cif_value_at_1_team = 1 - kmf_team.predict(1)\n",
    "cif_value_at_1_yellow = 1 - kmf_yellow.predict(1)\n",
    "\n",
    "print(f\"Cumulative Incidence Function value at time = 1 (Patel): {cif_value_at_1_patel:.0%}\")\n",
    "print(f\"Cumulative Incidence Function value at time = 1 (TEAM): {cif_value_at_1_team:.0%}\")\n",
    "print(f\"Cumulative Incidence Function value at time = 1 (Yellow): {cif_value_at_1_yellow:.0%}\")\n",
    "\n",
    "\n",
    "# Save the cumulative incidence function data to CSV files\n",
    "cif_patel = kmf_patel.cumulative_density_.reset_index()\n",
    "cif_team = kmf_team.cumulative_density_.reset_index()\n",
    "cif_yellow = kmf_yellow.cumulative_density_.reset_index()\n",
    "\n",
    "# Add a column to identify the criteria\n",
    "cif_patel['Criteria'] = 'Patel'\n",
    "cif_team['Criteria'] = 'TEAM'\n",
    "cif_yellow['Criteria'] = 'Yellow'\n",
    "\n",
    "\n",
    "# Get the site name from pyCLIF helper\n",
    "site_name = pyCLIF.helper[\"site_name\"]\n",
    "\n",
    "cif_patel['Site'] = site_name\n",
    "cif_team['Site'] = site_name\n",
    "cif_yellow['Site'] = site_name\n",
    "\n",
    "# Save to CSV files with site name in the file names\n",
    "timestamp = datetime.now().date()\n",
    "cif_patel.to_csv(f'../output/final/cif_b_hours_patel_{pyCLIF.helper[\"site_name\"]}_{timestamp}.csv', index=False)\n",
    "cif_team.to_csv(f'../output/final/cif_b_hours_team_{pyCLIF.helper[\"site_name\"]}_{timestamp}.csv', index=False)\n",
    "cif_yellow.to_csv(f'../output/final/cif_b_hours_yellow_{pyCLIF.helper[\"site_name\"]}_{timestamp}.csv', index=False)\n",
    "\n",
    "# Calculate 95% CI for each criteria\n",
    "ci_patel_ci = kmf_patel.confidence_interval_cumulative_density_.reset_index()\n",
    "ci_team_ci = kmf_team.confidence_interval_cumulative_density_.reset_index()\n",
    "ci_yellow_ci = kmf_yellow.confidence_interval_cumulative_density_.reset_index()\n",
    "\n",
    "# Add a column to identify the criteria\n",
    "ci_patel_ci['Criteria'] = 'Patel'\n",
    "ci_team_ci['Criteria'] = 'TEAM'\n",
    "ci_yellow_ci['Criteria'] = 'Yellow'\n",
    "ci_patel_ci['Site'] = site_name\n",
    "ci_team_ci['Site'] = site_name\n",
    "ci_yellow_ci['Site'] = site_name\n",
    "# Save to CSV files with site name in the file names\n",
    "timestamp = datetime.now().date()\n",
    "ci_patel_ci.to_csv(f'../output/final/cif_b_hours_patel_ci_{pyCLIF.helper[\"site_name\"]}_{timestamp}.csv', index=False)\n",
    "ci_team_ci.to_csv(f'../output/final/cif_b_hours_team_ci_{pyCLIF.helper[\"site_name\"]}_{timestamp}.csv', index=False)\n",
    "ci_yellow_ci.to_csv(f'../output/final/cif_b_hours_yellow_ci_{pyCLIF.helper[\"site_name\"]}_{timestamp}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note: `time_from_vent_adjusted` is -1 until hour 4, then it counts up from 0. This builds in the 4-hour cool off period.\n",
    "business_hours_df = final_df[(final_df['time_from_vent_adjusted'] >= 0)]\n",
    "#business_hours_df = final_df[(final_df['time_from_vent'] >= 0) & (final_df['time_from_vent'] < 72)]\n",
    "\n",
    "# recorded_hour is the hour of the day (0-23), so business hours are 8 (8 AM) - 17 (5 PM).\n",
    "business_hours_df = business_hours_df[(business_hours_df['recorded_hour'] >= 8) & (business_hours_df['recorded_hour'] < 17)].copy()\n",
    "business_hours_df['time_biz'] = business_hours_df.groupby('hospitalization_id').cumcount()\n",
    "\n",
    "# Filter the DataFrame for rows where the Patel criteria are met\n",
    "patel_eligible_df = business_hours_df[business_hours_df['patel_flag'] == 1]\n",
    "\n",
    "# Find the first time each patient meets the Patel criteria\n",
    "first_eligibility_times_patel = patel_eligible_df.groupby('hospitalization_id')['time_biz'].min().reset_index()\n",
    "\n",
    "# Create the survival analysis dataset\n",
    "survival_analysis_df_patel = business_hours_df[['hospitalization_id']].drop_duplicates().copy()\n",
    "\n",
    "# Merge with the first eligibility times\n",
    "survival_analysis_df_patel = pd.merge(survival_analysis_df_patel, first_eligibility_times_patel, on='hospitalization_id', how='left')\n",
    "\n",
    "# Fill NaN values with 27 for patients who were never eligible\n",
    "survival_analysis_df_patel['time_biz'] = survival_analysis_df_patel['time_biz'].fillna(27)\n",
    "\n",
    "# Create the 'eligible' column\n",
    "survival_analysis_df_patel['eligible'] = (survival_analysis_df_patel['time_biz'] != 27).astype(int)\n",
    "\n",
    "# Rename columns\n",
    "survival_analysis_df_patel.rename(columns={'time_biz': 'time_to_first_eligibility'}, inplace=True)\n",
    "\n",
    "# Add +1 to time_to_first_eligibility\n",
    "survival_analysis_df_patel['time_to_first_eligibility'] = survival_analysis_df_patel['time_to_first_eligibility'] + 1\n",
    "\n",
    "# Display the final dataset\n",
    "print(survival_analysis_df_patel.head())\n",
    "\n",
    "# Repeat the process for TEAM criteria\n",
    "team_eligible_df = business_hours_df[business_hours_df['team_flag'] == 1]\n",
    "first_eligibility_times_team = team_eligible_df.groupby('hospitalization_id')['time_biz'].min().reset_index()\n",
    "survival_analysis_df_team = business_hours_df[['hospitalization_id']].drop_duplicates().copy()\n",
    "survival_analysis_df_team = pd.merge(survival_analysis_df_team, first_eligibility_times_team, on='hospitalization_id', how='left')\n",
    "survival_analysis_df_team['time_biz'] = survival_analysis_df_team['time_biz'].fillna(27)\n",
    "survival_analysis_df_team['eligible'] = (survival_analysis_df_team['time_biz'] != 27).astype(int)\n",
    "survival_analysis_df_team.rename(columns={'time_biz': 'time_to_first_eligibility'}, inplace=True)\n",
    "survival_analysis_df_team['time_to_first_eligibility'] = survival_analysis_df_team['time_to_first_eligibility'] + 1\n",
    "\n",
    "# Repeat the process for Yellow criteria\n",
    "yellow_eligible_df = business_hours_df[business_hours_df['any_yellow_or_green_no_red'] == 1]\n",
    "first_eligibility_times_yellow = yellow_eligible_df.groupby('hospitalization_id')['time_biz'].min().reset_index()\n",
    "survival_analysis_df_yellow = business_hours_df[['hospitalization_id']].drop_duplicates().copy()\n",
    "survival_analysis_df_yellow = pd.merge(survival_analysis_df_yellow, first_eligibility_times_yellow, on='hospitalization_id', how='left')\n",
    "survival_analysis_df_yellow['time_biz'] = survival_analysis_df_yellow['time_biz'].fillna(27)\n",
    "survival_analysis_df_yellow['eligible'] = (survival_analysis_df_yellow['time_biz'] != 27).astype(int)\n",
    "survival_analysis_df_yellow.rename(columns={'time_biz': 'time_to_first_eligibility'}, inplace=True)\n",
    "survival_analysis_df_yellow['time_to_first_eligibility'] = survival_analysis_df_yellow['time_to_first_eligibility'] + 1\n",
    "\n",
    "# Display the final datasets\n",
    "print(survival_analysis_df_team.head())\n",
    "print(survival_analysis_df_yellow.head())\n",
    "\n",
    "# Initialize the KaplanMeierFitter for Patel, TEAM and Yellow criteria\n",
    "kmf_patel = KaplanMeierFitter()\n",
    "kmf_team = KaplanMeierFitter()\n",
    "kmf_yellow = KaplanMeierFitter()\n",
    "\n",
    "# fit the data for Patel criteria\n",
    "kmf_patel.fit(durations=survival_analysis_df_patel['time_to_first_eligibility'], event_observed=survival_analysis_df_patel['eligible'], label='Patel Criteria')\n",
    "\n",
    "# Fit the data for TEAM criteria\n",
    "kmf_team.fit(durations=survival_analysis_df_team['time_to_first_eligibility'], event_observed=survival_analysis_df_team['eligible'], label='TEAM Criteria')\n",
    "\n",
    "# Fit the data for Yellow criteria\n",
    "kmf_yellow.fit(durations=survival_analysis_df_yellow['time_to_first_eligibility'], event_observed=survival_analysis_df_yellow['eligible'], label='Yellow Criteria')\n",
    "\n",
    "# Plot the cumulative incidence function for all criteria\n",
    "ax = kmf_patel.plot_cumulative_density()\n",
    "kmf_team.plot_cumulative_density(ax=ax)\n",
    "kmf_yellow.plot_cumulative_density(ax=ax)\n",
    "\n",
    "plt.title('Cumulative Incidence Function for Time to First Eligibility')\n",
    "plt.xlabel('Time to First Eligibility (business hours)')\n",
    "plt.ylabel('Cumulative Incidence Probability')\n",
    "# Save the plot\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "plt.savefig(f'../output/final/graphs/cif_b_hours_{pyCLIF.helper[\"site_name\"]}_{timestamp}.png')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Estimate the median time to first eligibility for all criteria\n",
    "median_time_to_first_eligibility_patel = kmf_patel.median_survival_time_\n",
    "median_time_to_first_eligibility_team = kmf_team.median_survival_time_\n",
    "median_time_to_first_eligibility_yellow = kmf_yellow.median_survival_time_\n",
    "\n",
    "print(f\"Median time to first eligibility (Patel): {median_time_to_first_eligibility_patel} hours\")\n",
    "print(f\"Median time to first eligibility (TEAM): {median_time_to_first_eligibility_team} hours\")\n",
    "print(f\"Median time to first eligibility (Yellow): {median_time_to_first_eligibility_yellow} hours\")\n",
    "\n",
    "# Calculate the cumulative incidence function value at time = 1 for all criteria\n",
    "cif_value_at_1_patel = 1 - kmf_patel.predict(1)\n",
    "cif_value_at_1_team = 1 - kmf_team.predict(1)\n",
    "cif_value_at_1_yellow = 1 - kmf_yellow.predict(1)\n",
    "\n",
    "print(f\"Cumulative Incidence Function value at time = 1 (Patel): {cif_value_at_1_patel:.0%}\")\n",
    "print(f\"Cumulative Incidence Function value at time = 1 (TEAM): {cif_value_at_1_team:.0%}\")\n",
    "print(f\"Cumulative Incidence Function value at time = 1 (Yellow): {cif_value_at_1_yellow:.0%}\")\n",
    "\n",
    "\n",
    "# Save the cumulative incidence function data to CSV files\n",
    "cif_patel = kmf_patel.cumulative_density_.reset_index()\n",
    "cif_team = kmf_team.cumulative_density_.reset_index()\n",
    "cif_yellow = kmf_yellow.cumulative_density_.reset_index()\n",
    "\n",
    "# Add a column to identify the criteria\n",
    "cif_patel['Criteria'] = 'Patel'\n",
    "cif_team['Criteria'] = 'TEAM'\n",
    "cif_yellow['Criteria'] = 'Yellow'\n",
    "\n",
    "\n",
    "# Get the site name from pyCLIF helper\n",
    "site_name = pyCLIF.helper[\"site_name\"]\n",
    "\n",
    "cif_patel['Site'] = site_name\n",
    "cif_team['Site'] = site_name\n",
    "cif_yellow['Site'] = site_name\n",
    "\n",
    "# Save to CSV files with site name in the file names\n",
    "timestamp = datetime.now().date()\n",
    "cif_patel.to_csv(f'../output/final/cif_b_hours_patel_{pyCLIF.helper[\"site_name\"]}_{timestamp}.csv', index=False)\n",
    "cif_team.to_csv(f'../output/final/cif_b_hours_team_{pyCLIF.helper[\"site_name\"]}_{timestamp}.csv', index=False)\n",
    "cif_yellow.to_csv(f'../output/final/cif_b_hours_yellow_{pyCLIF.helper[\"site_name\"]}_{timestamp}.csv', index=False)\n",
    "\n",
    "# Calculate 95% CI for each criteria\n",
    "ci_patel_ci = kmf_patel.confidence_interval_cumulative_density_.reset_index()\n",
    "ci_team_ci = kmf_team.confidence_interval_cumulative_density_.reset_index()\n",
    "ci_yellow_ci = kmf_yellow.confidence_interval_cumulative_density_.reset_index()\n",
    "\n",
    "# Add a column to identify the criteria\n",
    "ci_patel_ci['Criteria'] = 'Patel'\n",
    "ci_team_ci['Criteria'] = 'TEAM'\n",
    "ci_yellow_ci['Criteria'] = 'Yellow'\n",
    "ci_patel_ci['Site'] = site_name\n",
    "ci_team_ci['Site'] = site_name\n",
    "ci_yellow_ci['Site'] = site_name\n",
    "# Save to CSV files with site name in the file names\n",
    "timestamp = datetime.now().date()\n",
    "ci_patel_ci.to_csv(f'../output/final/cif_b_hours_patel_ci_{pyCLIF.helper[\"site_name\"]}_{timestamp}.csv', index=False)\n",
    "ci_team_ci.to_csv(f'../output/final/cif_b_hours_team_ci_{pyCLIF.helper[\"site_name\"]}_{timestamp}.csv', index=False)\n",
    "ci_yellow_ci.to_csv(f'../output/final/cif_b_hours_yellow_ci_{pyCLIF.helper[\"site_name\"]}_{timestamp}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregates for comparison across sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total observed business hours\n",
    "total_observed_hours = business_hours_df.shape[0]\n",
    "# Total number of patients\n",
    "total_patients = business_hours_df['hospitalization_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria_list = [\n",
    "    ('patel_flag', 'Patel'),\n",
    "    ('team_flag', 'TEAM'),\n",
    "    ('any_yellow_or_green_no_red', 'Yellow')\n",
    "]\n",
    "\n",
    "aggregate_data = []\n",
    "\n",
    "for flag_column, criterion_name in criteria_list:\n",
    "    # Filter the DataFrame for rows where the criterion is met\n",
    "    eligible_df = business_hours_df[business_hours_df[flag_column] == 1]\n",
    "\n",
    "    # Total eligible hours for the criterion\n",
    "    eligible_hours = eligible_df.shape[0]\n",
    "    \n",
    "    # Find the first time each patient meets the criterion\n",
    "    first_eligibility_times = eligible_df.groupby('hospitalization_id')['time_biz'].min().reset_index()\n",
    "    \n",
    "    # Create the survival analysis dataset\n",
    "    survival_analysis_df = business_hours_df[['hospitalization_id']].drop_duplicates().copy()\n",
    "    \n",
    "    # Merge with the first eligibility times\n",
    "    survival_analysis_df = pd.merge(\n",
    "        survival_analysis_df,\n",
    "        first_eligibility_times,\n",
    "        on='hospitalization_id',\n",
    "        how='left'\n",
    "    )\n",
    "    # Fill NaN values with 27 for patients who were never eligible\n",
    "    survival_analysis_df['time_biz'] = survival_analysis_df['time_biz'].fillna(27)\n",
    "    # Create the 'eligible' column\n",
    "    survival_analysis_df['eligible'] = (survival_analysis_df['time_biz'] != 27).astype(int)\n",
    "    # Rename columns\n",
    "    survival_analysis_df.rename(columns={'time_biz': 'time_to_first_eligibility'}, inplace=True)\n",
    "    # Add +1 to time_to_first_eligibility\n",
    "    survival_analysis_df['time_to_first_eligibility'] = survival_analysis_df_patel['time_to_first_eligibility'] + 1\n",
    "    # Median time to first eligibility\n",
    "    median_time = survival_analysis_df[survival_analysis_df['eligible'] == 1]['time_to_first_eligibility'].median()\n",
    "    # Number of patients who became eligible\n",
    "    eligible_patients = survival_analysis_df['eligible'].sum()\n",
    "    # Confidence intervals for median time (optional)\n",
    "    # Using bootstrapping for confidence intervals\n",
    "    median_times = survival_analysis_df[survival_analysis_df['eligible'] == 1]['time_to_first_eligibility']\n",
    "    ci_lower = median_times.quantile(0.25)\n",
    "    ci_upper = median_times.quantile(0.75)\n",
    "    \n",
    "    # Append to aggregate data list\n",
    "    aggregate_data.append({\n",
    "        'Criteria': criterion_name,\n",
    "        'Total Patients': total_patients,\n",
    "        'Eligible Patients': eligible_patients,\n",
    "        'Total Observed Hours': total_observed_hours,\n",
    "        'Eligible Hours': eligible_hours,\n",
    "        'Median Time': median_time,\n",
    "        'CI Lower Median Time': ci_lower,\n",
    "        'CI Upper Median Time': ci_upper\n",
    "    })\n",
    "    \n",
    "    # Save the cumulative incidence function data if needed\n",
    "    kmf = KaplanMeierFitter()\n",
    "    kmf.fit(\n",
    "        durations=survival_analysis_df['time_to_first_eligibility'],\n",
    "        event_observed=survival_analysis_df['eligible'],\n",
    "        label=f'{criterion_name} Criteria'\n",
    "    )\n",
    "    cif = kmf.cumulative_density_.reset_index()\n",
    "    cif['Criteria'] = criterion_name\n",
    "    \n",
    "# Create a DataFrame for aggregate data\n",
    "aggregate_df = pd.DataFrame(aggregate_data)\n",
    "\n",
    "# Save the aggregate data to CSV\n",
    "aggregate_df.to_csv(f'../output/final/aggregates_{pyCLIF.helper[\"site_name\"]}_{timestamp}.csv', index=False)\n",
    "\n",
    "# Print the aggregate data\n",
    "print(aggregate_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Hours Criteria Met on Days 1, 2, and 3\n",
    "\n",
    "Determine how many hours the criteria are met on specific calendar days (Day 1, Day 2, Day 3 after intubation).\n",
    "\n",
    "1. First, assign a calendar_day column that represents the calendar day relative to intubation.\n",
    "2. Use the recorded_date and recorded hour to calculate the difference from the intubation time, and categorize rows into Day 1, Day 2, Day 3.\n",
    "3. For each encounter, group the data by calendar_day and hospitalization_id and sum the hours that meet each criterion.\n",
    "4. Compute the average number of hours for each criterion per day.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_hours_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vent_start_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge business_hours_df with vent_start_end to get 'vent_start_time'\n",
    "visualization_df = pd.merge(\n",
    "    business_hours_df,\n",
    "    vent_start_end[['encounter_block', 'block_vent_start_dttm']],\n",
    "    on='encounter_block',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Ensure 'vent_start_time' and 'recorded_date' are in datetime format\n",
    "visualization_df['block_vent_start_dttm'] = pd.to_datetime(visualization_df['block_vent_start_dttm'])\n",
    "visualization_df['recorded_date'] = pd.to_datetime(visualization_df['recorded_date'])\n",
    "\n",
    "# Combine 'recorded_date' and 'recorded_hour' to create 'recorded_dttm'\n",
    "visualization_df['recorded_dttm'] = visualization_df['recorded_date'] + pd.to_timedelta(visualization_df['recorded_hour'], unit='h')\n",
    "\n",
    "# Verify the data types\n",
    "# print(\"Verify data types\\n\", visualization_df[['vent_start_time', 'recorded_dttm']].dtypes)\n",
    "\n",
    "# Remove timezone information from 'vent_start_time' if it's timezone-aware\n",
    "if visualization_df['block_vent_start_dttm'].dt.tz is not None:\n",
    "    visualization_df['block_vent_start_dttm'] = visualization_df['block_vent_start_dttm'].dt.tz_localize(None)\n",
    "\n",
    "# Similarly, remove timezone information from 'recorded_dttm' if needed\n",
    "if visualization_df['recorded_dttm'].dt.tz is not None:\n",
    "    visualization_df['recorded_dttm'] = visualization_df['recorded_dttm'].dt.tz_localize(None)\n",
    "\n",
    "# print(\"\\nConverted data type if not tz naive\\n\", visualization_df[['vent_start_time', 'recorded_dttm']].dtypes)\n",
    "\n",
    "def assign_calendar_day(df, intubation_col, recorded_col):\n",
    "    # Calculate the difference in days between intubation and recorded time\n",
    "    df['calendar_day'] = (df[recorded_col] - df[intubation_col]).dt.days + 1\n",
    "    return df\n",
    "\n",
    "# Assign calendar day for each encounter\n",
    "visualization_df = assign_calendar_day(visualization_df, 'block_vent_start_dttm', 'recorded_dttm')\n",
    "\n",
    "visualization_df = visualization_df[['encounter_block', 'block_vent_start_dttm', 'recorded_dttm', \n",
    "                  'calendar_day', 'patel_flag', 'team_flag', 'any_yellow_or_green_no_red', 'all_green', 'all_green_no_red',\n",
    "                  'any_green']]\n",
    "\n",
    "def compute_avg_hours_by_day(df, criteria_columns):\n",
    "    # Ensure hospitalization_id is handled as string/object and numeric columns as numbers\n",
    "    hours_per_day = df.groupby(['encounter_block', 'calendar_day']).agg({\n",
    "        'patel_flag': 'sum',\n",
    "        'team_flag': 'sum',\n",
    "        'any_yellow_or_green_no_red': 'sum',\n",
    "        'all_green': 'sum',\n",
    "        'all_green_no_red':'sum',\n",
    "        'any_green' : 'sum'\n",
    "    }).reset_index()\n",
    "    # Filter for Day 1, Day 2, Day 3\n",
    "    hours_per_day = hours_per_day[hours_per_day['calendar_day'].isin([1, 2, 3])]\n",
    "    \n",
    "    # Calculate the average number of hours for each day\n",
    "    avg_hours_by_day = hours_per_day.groupby('calendar_day').agg({\n",
    "        'patel_flag': 'mean',\n",
    "        'team_flag': 'mean',\n",
    "        'any_yellow_or_green_no_red': 'mean',\n",
    "        'all_green': 'mean',\n",
    "        'all_green_no_red':'mean',\n",
    "        'any_green' : 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    return avg_hours_by_day\n",
    "\n",
    "# Define your criteria columns\n",
    "criteria_columns = ['patel_flag', 'team_flag', 'any_yellow_or_green_no_red', 'all_green', 'all_green_no_red']\n",
    "# Calculate the average number of hours each criterion is met on Day 1, 2, and 3\n",
    "avg_hours_by_day = compute_avg_hours_by_day(visualization_df, criteria_columns)\n",
    "avg_hours_by_day['site_name'] = pyCLIF.helper[\"site_name\"]\n",
    "pd.DataFrame(avg_hours_by_day).to_csv(f'../output/final/avg_hours_by_day_{pyCLIF.helper[\"site_name\"]}_{datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}.csv',index=False)\n",
    "avg_hours_by_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_avg_hours_by_day_bar(avg_hours_by_day, criteria_columns):\n",
    "    # Melt the DataFrame for easier plotting with seaborn\n",
    "    melted_df = avg_hours_by_day.melt(id_vars='calendar_day', value_vars=criteria_columns, var_name='Criteria', value_name='Average Hours Met')\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Create a bar plot\n",
    "    sns.barplot(x='calendar_day', y='Average Hours Met', hue='Criteria', data=melted_df, palette='viridis')\n",
    "    \n",
    "    # Add custom x-axis labels for Day 1, Day 2, Day 3\n",
    "    plt.xticks(ticks=[0, 1, 2], labels=[\"Day 1\", \"Day 2\", \"Day 3\"])\n",
    "    \n",
    "    # Add labels and title\n",
    "    plt.title('Average Hours Criteria Met per Day')\n",
    "    plt.xlabel('Calendar Day')\n",
    "    plt.ylabel('Average Hours Criteria Met')\n",
    "    \n",
    "    # Move the legend to the bottom\n",
    "    plt.legend(title='Criteria', loc='upper center', bbox_to_anchor=(0.5, -0.1), ncol=3)\n",
    "    \n",
    "    # Show plot\n",
    "    plt.tight_layout()\n",
    "    # Save the plot\n",
    "    plt.savefig(f'../output/final/graphs/avg_hours_by_day_{pyCLIF.helper[\"site_name\"]}.png')\n",
    "    plt.show()\n",
    "\n",
    "# Plot the average hours by day using a bar plot\n",
    "plot_avg_hours_by_day_bar(avg_hours_by_day, criteria_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel categories plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Useful for EDA \n",
    "# Create a DataFrame for parallel categories plot\n",
    "parallel_df = business_hours_df[['patel_flag',  'any_yellow_or_green_no_red']].copy()\n",
    "parallel_df['patel_flag'] = parallel_df['patel_flag'].apply(lambda x: 1 if x else 0)\n",
    "parallel_df['any_yellow_or_green_no_red'] = parallel_df['any_yellow_or_green_no_red'].apply(lambda x: 1 if x else 0)\n",
    "\n",
    "# Create parallel categories plot\n",
    "fig = px.parallel_categories(parallel_df, dimensions=['patel_flag',  'any_yellow_or_green_no_red'],\n",
    "                             color=\"patel_flag\",\n",
    "                             labels={'patel_flag': 'Patel Met',  'any_yellow_or_green_no_red': 'Yellow Flag'},\n",
    "                             color_continuous_scale=px.colors.sequential.Inferno)\n",
    "\n",
    "fig.update_layout(title=\"Parallel Categories Plot: Comparison of Criteria Satisfaction\")\n",
    "fig.show()\n",
    "\n",
    "# Save the final figure\n",
    "fig.write_image(f'../output/final/graphs/parallel_categories_{pyCLIF.helper[\"site_name\"]}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Useful for EDA \n",
    "# Create a DataFrame for parallel categories plot\n",
    "parallel_df = business_hours_df[['patel_flag', 'team_flag', 'any_yellow_or_green_no_red']].copy()\n",
    "parallel_df['patel_flag'] = parallel_df['patel_flag'].apply(lambda x: 1 if x else 0)\n",
    "parallel_df['team_flag'] = parallel_df['team_flag'].apply(lambda x: 1 if x else 0)\n",
    "parallel_df['any_yellow_or_green_no_red'] = parallel_df['any_yellow_or_green_no_red'].apply(lambda x: 1 if x else 0)\n",
    "\n",
    "# Create parallel categories plot\n",
    "fig = px.parallel_categories(parallel_df, dimensions=['patel_flag', 'team_flag', 'any_yellow_or_green_no_red'],\n",
    "                             color=\"patel_flag\",\n",
    "                             labels={'patel_flag': 'Patel Met', 'team_flag': 'TEAM Met', 'any_yellow_or_green_no_red': 'Yellow Flag'},\n",
    "                             color_continuous_scale=px.colors.sequential.Inferno)\n",
    "\n",
    "fig.update_layout(title=\"Parallel Categories Plot: Comparison of Criteria Satisfaction\")\n",
    "fig.show()\n",
    "\n",
    "# Save the final figure\n",
    "fig.write_image(f'../output/final/graphs/parallel_categories_{pyCLIF.helper[\"site_name\"]}.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at encounters when Patel flag is not met but team flag is met\n",
    "## sanity check\n",
    "patel_fail_team_pass = business_hours_df[(business_hours_df['patel_flag'] == 0) & (business_hours_df['team_flag'] == 1)]\n",
    "# Verify the filter\n",
    "print(f\"\\nTotal number of hours where Patel failed and Team passed: {len(patel_fail_team_pass)}\\n\")\n",
    "\n",
    "if len(patel_fail_team_pass) > 0:\n",
    "    # Dictionary to store our failure counts\n",
    "    print(\"Primary cause of Patel Criteria non-compliance\")\n",
    "    failure_counts = {\n",
    "            'MAP': sum(patel_fail_team_pass['patel_map_flag'] == 0),\n",
    "            'SBP': sum(patel_fail_team_pass['patel_sbp_flag'] == 0),\n",
    "            'Pulse': sum(patel_fail_team_pass['patel_pulse_flag'] == 0),\n",
    "            'Respiratory Rate': sum(patel_fail_team_pass['patel_resp_rate_flag'] == 0),\n",
    "            'SpO2': sum(patel_fail_team_pass['patel_spo2_flag'] == 0)\n",
    "        }\n",
    "    failure_df = pd.DataFrame(list(failure_counts.items()),columns = ['Criteria','Count'])\n",
    "    failure_df.to_csv(f'../output/final/patel_fail_team_pass_subcomponents_{pyCLIF.helper[\"site_name\"]}_{datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}.csv',index=False)\n",
    "    print(failure_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".mobilization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
